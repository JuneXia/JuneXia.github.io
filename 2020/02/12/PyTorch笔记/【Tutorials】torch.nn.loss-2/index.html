<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://yoursite.com').hostname,
    root: '/',
    scheme: 'Gemini',
    version: '7.7.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="本节主要介绍PyTorch中的剩下的14种损失函数，加上前文的4种loss，一共是18种。">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch笔记&#x2F;【Tutorials】torch.nn.loss-2">
<meta property="og:url" content="http://yoursite.com/2020/02/12/PyTorch%E7%AC%94%E8%AE%B0/%E3%80%90Tutorials%E3%80%91torch.nn.loss-2/index.html">
<meta property="og:site_name" content="Paper搬运菌">
<meta property="og:description" content="本节主要介绍PyTorch中的剩下的14种损失函数，加上前文的4种loss，一共是18种。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://github.com/JuneXia/JuneXia.github.io/raw/hexo/source/images/ml/nn_loss4.jpg">
<meta property="og:image" content="https://github.com/JuneXia/JuneXia.github.io/raw/hexo/source/images/ml/nn_loss5.jpg">
<meta property="og:image" content="https://github.com/JuneXia/JuneXia.github.io/raw/hexo/source/images/ml/nn_loss6.jpg">
<meta property="article:published_time" content="2020-02-11T16:00:00.000Z">
<meta property="article:modified_time" content="2020-03-19T10:58:24.088Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/JuneXia/JuneXia.github.io/raw/hexo/source/images/ml/nn_loss4.jpg">

<link rel="canonical" href="http://yoursite.com/2020/02/12/PyTorch%E7%AC%94%E8%AE%B0/%E3%80%90Tutorials%E3%80%91torch.nn.loss-2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>PyTorch笔记/【Tutorials】torch.nn.loss-2 | Paper搬运菌</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Paper搬运菌</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/12/PyTorch%E7%AC%94%E8%AE%B0/%E3%80%90Tutorials%E3%80%91torch.nn.loss-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="其实，我是一个搬运工！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Paper搬运菌">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          PyTorch笔记/【Tutorials】torch.nn.loss-2
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-12 00:00:00" itemprop="dateCreated datePublished" datetime="2020-02-12T00:00:00+08:00">2020-02-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-19 18:58:24" itemprop="dateModified" datetime="2020-03-19T18:58:24+08:00">2020-03-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/PyTorch%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">PyTorch笔记</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本节主要介绍PyTorch中的剩下的14种损失函数，加上前文的4种loss，一共是18种。<br><a id="more"></a></p>
<h2 id="nn-L1Loss"><a href="#nn-L1Loss" class="headerlink" title="nn.L1Loss"></a>nn.L1Loss</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">L1Loss</span><span class="params">(_Loss)</span>:</span></span><br><span class="line">    <span class="string">r"""Creates a criterion that measures the mean absolute error (MAE) between each element in</span></span><br><span class="line"><span class="string">    the input :math:`x` and target :math:`y`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">        \ell(x, y) = L = \&#123;l_1,\dots,l_N\&#125;^\top, \quad</span></span><br><span class="line"><span class="string">        l_n = \left| x_n - y_n \right|,</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    where :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``</span></span><br><span class="line"><span class="string">    (default ``'mean'``), then:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">        \ell(x, y) =</span></span><br><span class="line"><span class="string">        \begin&#123;cases&#125;</span></span><br><span class="line"><span class="string">            \operatorname&#123;mean&#125;(L), &amp; \text&#123;if reduction&#125; = \text&#123;'mean';&#125;\\</span></span><br><span class="line"><span class="string">            \operatorname&#123;sum&#125;(L),  &amp; \text&#123;if reduction&#125; = \text&#123;'sum'.&#125;</span></span><br><span class="line"><span class="string">        \end&#123;cases&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :math:`x` and :math:`y` are tensors of arbitrary shapes with a total</span></span><br><span class="line"><span class="string">    of :math:`n` elements each.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The sum operation still operates over all the elements, and divides by :math:`n`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The division by :math:`n` can be avoided if one sets ``reduction = 'sum'``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span></span><br><span class="line"><span class="string">            the losses are averaged over each loss element in the batch. Note that for</span></span><br><span class="line"><span class="string">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span></span><br><span class="line"><span class="string">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span></span><br><span class="line"><span class="string">            when reduce is ``False``. Default: ``True``</span></span><br><span class="line"><span class="string">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span></span><br><span class="line"><span class="string">            losses are averaged or summed over observations for each minibatch depending</span></span><br><span class="line"><span class="string">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span></span><br><span class="line"><span class="string">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span></span><br><span class="line"><span class="string">        reduction (string, optional): Specifies the reduction to apply to the output:</span></span><br><span class="line"><span class="string">            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,</span></span><br><span class="line"><span class="string">            ``'mean'``: the sum of the output will be divided by the number of</span></span><br><span class="line"><span class="string">            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`</span></span><br><span class="line"><span class="string">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span></span><br><span class="line"><span class="string">            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Shape:</span></span><br><span class="line"><span class="string">        - Input: :math:`(N, *)` where :math:`*` means, any number of additional</span></span><br><span class="line"><span class="string">          dimensions</span></span><br><span class="line"><span class="string">        - Target: :math:`(N, *)`, same shape as the input</span></span><br><span class="line"><span class="string">        - Output: scalar. If :attr:`reduction` is ``'none'``, then</span></span><br><span class="line"><span class="string">          :math:`(N, *)`, same shape as the input</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; loss = nn.L1Loss()</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; target = torch.randn(3, 5)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; output = loss(input, target)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; output.backward()</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    __constants__ = [<span class="string">'reduction'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size_average=None, reduce=None, reduction=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line">        super(L1Loss, self).__init__(size_average, reduce, reduction)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, target)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.l1_loss(input, target, reduction=self.reduction)</span><br></pre></td></tr></table></figure>
<p><strong>功能</strong>：计算inputs与target之差的绝对值，在数据回归中常用。</p>
<p>计算公式：</p>
<script type="math/tex; mode=display">
l_n = \vert x_n - y_n \vert</script><h2 id="nn-MSELoss"><a href="#nn-MSELoss" class="headerlink" title="nn.MSELoss"></a>nn.MSELoss</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MSELoss</span><span class="params">(_Loss)</span>:</span></span><br><span class="line">    <span class="string">r"""Creates a criterion that measures the mean squared error (squared L2 norm) between</span></span><br><span class="line"><span class="string">    each element in the input :math:`x` and target :math:`y`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">        \ell(x, y) = L = \&#123;l_1,\dots,l_N\&#125;^\top, \quad</span></span><br><span class="line"><span class="string">        l_n = \left( x_n - y_n \right)^2,</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    where :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``</span></span><br><span class="line"><span class="string">    (default ``'mean'``), then:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">        \ell(x, y) =</span></span><br><span class="line"><span class="string">        \begin&#123;cases&#125;</span></span><br><span class="line"><span class="string">            \operatorname&#123;mean&#125;(L), &amp;  \text&#123;if reduction&#125; = \text&#123;'mean';&#125;\\</span></span><br><span class="line"><span class="string">            \operatorname&#123;sum&#125;(L),  &amp;  \text&#123;if reduction&#125; = \text&#123;'sum'.&#125;</span></span><br><span class="line"><span class="string">        \end&#123;cases&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :math:`x` and :math:`y` are tensors of arbitrary shapes with a total</span></span><br><span class="line"><span class="string">    of :math:`n` elements each.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The sum operation still operates over all the elements, and divides by :math:`n`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The division by :math:`n` can be avoided if one sets ``reduction = 'sum'``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span></span><br><span class="line"><span class="string">            the losses are averaged over each loss element in the batch. Note that for</span></span><br><span class="line"><span class="string">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span></span><br><span class="line"><span class="string">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span></span><br><span class="line"><span class="string">            when reduce is ``False``. Default: ``True``</span></span><br><span class="line"><span class="string">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span></span><br><span class="line"><span class="string">            losses are averaged or summed over observations for each minibatch depending</span></span><br><span class="line"><span class="string">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span></span><br><span class="line"><span class="string">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span></span><br><span class="line"><span class="string">        reduction (string, optional): Specifies the reduction to apply to the output:</span></span><br><span class="line"><span class="string">            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,</span></span><br><span class="line"><span class="string">            ``'mean'``: the sum of the output will be divided by the number of</span></span><br><span class="line"><span class="string">            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`</span></span><br><span class="line"><span class="string">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span></span><br><span class="line"><span class="string">            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Shape:</span></span><br><span class="line"><span class="string">        - Input: :math:`(N, *)` where :math:`*` means, any number of additional</span></span><br><span class="line"><span class="string">          dimensions</span></span><br><span class="line"><span class="string">        - Target: :math:`(N, *)`, same shape as the input</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; loss = nn.MSELoss()</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; target = torch.randn(3, 5)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; output = loss(input, target)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; output.backward()</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    __constants__ = [<span class="string">'reduction'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size_average=None, reduce=None, reduction=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line">        super(MSELoss, self).__init__(size_average, reduce, reduction)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, target)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.mse_loss(input, target, reduction=self.reduction)</span><br></pre></td></tr></table></figure>
<p><strong>功能</strong>：计算inputs与target之差的平方，在数据回归中常用。</p>
<p>计算公式：</p>
<script type="math/tex; mode=display">
l_n = (x_n - y_n)^2</script><p>主要参数：</p>
<ul>
<li><strong>reduction</strong>: 计算模式，可为 none/sum/mean \<br>&emsp; &emsp; &emsp; &emsp; none: 逐个元素(样本)计算 \<br>&emsp; &emsp; &emsp; &emsp; sum: 所有元素求和，返回标量 \<br>&emsp; &emsp; &emsp; &emsp; mean: 加权平均，返回标量 \</li>
</ul>
<p>L1Loss、MSELoss 代码示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"># @file name  : loss_function_2.py</span></span><br><span class="line"><span class="string"># @author     : TingsongYu https://github.com/TingsongYu</span></span><br><span class="line"><span class="string"># @date       : 2019-10-10 10:08:00</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tools.common_tools <span class="keyword">import</span> set_seed</span><br><span class="line"></span><br><span class="line">set_seed(<span class="number">1</span>)  <span class="comment"># 设置随机种子</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------------------------- 5 L1 loss ----------------------------------------------</span></span><br><span class="line"><span class="comment"># flag = 0</span></span><br><span class="line">flag = <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line"></span><br><span class="line">    inputs = torch.ones((<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    target = torch.ones((<span class="number">2</span>, <span class="number">2</span>)) * <span class="number">3</span></span><br><span class="line"></span><br><span class="line">    loss_f = nn.L1Loss(reduction=<span class="string">'none'</span>)</span><br><span class="line">    loss = loss_f(inputs, target)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"input:&#123;&#125;\ntarget:&#123;&#125;\nL1 loss:&#123;&#125;"</span>.format(inputs, target, loss))</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------------------------- 6 MSE loss ----------------------------------------------</span></span><br><span class="line"></span><br><span class="line">    loss_f_mse = nn.MSELoss(reduction=<span class="string">'none'</span>)</span><br><span class="line">    loss_mse = loss_f_mse(inputs, target)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"MSE loss:&#123;&#125;"</span>.format(loss_mse))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line">input:tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line">target:tensor([[<span class="number">3.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>]])</span><br><span class="line">L1 loss:tensor([[<span class="number">2.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">2.</span>]])</span><br><span class="line">MSE loss:tensor([[<span class="number">4.</span>, <span class="number">4.</span>],</span><br><span class="line">        [<span class="number">4.</span>, <span class="number">4.</span>]])</span><br></pre></td></tr></table></figure></p>
<h2 id="nn-SmoothL1Loss"><a href="#nn-SmoothL1Loss" class="headerlink" title="nn.SmoothL1Loss"></a>nn.SmoothL1Loss</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SmoothL1Loss</span><span class="params">(_Loss)</span>:</span></span><br><span class="line">    <span class="string">r"""Creates a criterion that uses a squared term if the absolute</span></span><br><span class="line"><span class="string">    element-wise error falls below 1 and an L1 term otherwise.</span></span><br><span class="line"><span class="string">    It is less sensitive to outliers than the `MSELoss` and in some cases</span></span><br><span class="line"><span class="string">    prevents exploding gradients (e.g. see `Fast R-CNN` paper by Ross Girshick).</span></span><br><span class="line"><span class="string">    Also known as the Huber loss:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">        \text&#123;loss&#125;(x, y) = \frac&#123;1&#125;&#123;n&#125; \sum_&#123;i&#125; z_&#123;i&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    where :math:`z_&#123;i&#125;` is given by:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">        z_&#123;i&#125; =</span></span><br><span class="line"><span class="string">        \begin&#123;cases&#125;</span></span><br><span class="line"><span class="string">        0.5 (x_i - y_i)^2, &amp; \text&#123;if &#125; |x_i - y_i| &lt; 1 \\</span></span><br><span class="line"><span class="string">        |x_i - y_i| - 0.5, &amp; \text&#123;otherwise &#125;</span></span><br><span class="line"><span class="string">        \end&#123;cases&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :math:`x` and :math:`y` arbitrary shapes with a total of :math:`n` elements each</span></span><br><span class="line"><span class="string">    the sum operation still operates over all the elements, and divides by :math:`n`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The division by :math:`n` can be avoided if sets ``reduction = 'sum'``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span></span><br><span class="line"><span class="string">            the losses are averaged over each loss element in the batch. Note that for</span></span><br><span class="line"><span class="string">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span></span><br><span class="line"><span class="string">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span></span><br><span class="line"><span class="string">            when reduce is ``False``. Default: ``True``</span></span><br><span class="line"><span class="string">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span></span><br><span class="line"><span class="string">            losses are averaged or summed over observations for each minibatch depending</span></span><br><span class="line"><span class="string">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span></span><br><span class="line"><span class="string">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span></span><br><span class="line"><span class="string">        reduction (string, optional): Specifies the reduction to apply to the output:</span></span><br><span class="line"><span class="string">            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,</span></span><br><span class="line"><span class="string">            ``'mean'``: the sum of the output will be divided by the number of</span></span><br><span class="line"><span class="string">            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`</span></span><br><span class="line"><span class="string">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span></span><br><span class="line"><span class="string">            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Shape:</span></span><br><span class="line"><span class="string">        - Input: :math:`(N, *)` where :math:`*` means, any number of additional</span></span><br><span class="line"><span class="string">          dimensions</span></span><br><span class="line"><span class="string">        - Target: :math:`(N, *)`, same shape as the input</span></span><br><span class="line"><span class="string">        - Output: scalar. If :attr:`reduction` is ``'none'``, then</span></span><br><span class="line"><span class="string">          :math:`(N, *)`, same shape as the input</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    __constants__ = [<span class="string">'reduction'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size_average=None, reduce=None, reduction=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line">        super(SmoothL1Loss, self).__init__(size_average, reduce, reduction)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, target)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.smooth_l1_loss(input, target, reduction=self.reduction)</span><br></pre></td></tr></table></figure>
<p><strong>功能</strong>：平滑的 L1Loss</p>
<p>主要参数：</p>
<ul>
<li><strong>reduction</strong>: 计算模式，可为 none/sum/mean，该参数和之前讲过的Loss类似，本文就不再赘述了。</li>
</ul>
<div align=center>
  <img src="https://github.com/JuneXia/JuneXia.github.io/raw/hexo/source/images/ml/nn_loss4.jpg" width = 80% height = 80% />
</div>

<p>计算公式：</p>
<script type="math/tex; mode=display">
loss(x, y) = \frac{1}{n} \sum_i z_i</script><script type="math/tex; mode=display">z_i = 
\begin{cases}
    0.5(x_i - y_i)^2, \qquad if \vert x_i - y_i \vert < 1 \\
    \vert x_i - y_i \vert - 0.5, \qquad otherwise
\end{cases}</script><p>采用这种平滑的Loss可以减轻离群点带来的影响。</p>
<p>下面在代码中画出 L1Loss 和 SmoothL1Loss 的曲线对比：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ------------------------------------------------- 7 Smooth L1 loss ----------------------------------------------</span></span><br><span class="line"><span class="comment"># flag = 0</span></span><br><span class="line">flag = <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line">    inputs = torch.linspace(<span class="number">-3</span>, <span class="number">3</span>, steps=<span class="number">500</span>)</span><br><span class="line">    target = torch.zeros_like(inputs)</span><br><span class="line"></span><br><span class="line">    loss_f = nn.SmoothL1Loss(reduction=<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">    loss_smooth = loss_f(inputs, target)</span><br><span class="line"></span><br><span class="line">    loss_l1 = np.abs(inputs.numpy())</span><br><span class="line"></span><br><span class="line">    plt.plot(inputs.numpy(), loss_smooth.numpy(), label=<span class="string">'Smooth L1 Loss'</span>)</span><br><span class="line">    plt.plot(inputs.numpy(), loss_l1, label=<span class="string">'L1 loss'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'x_i - y_i'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'loss value'</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><br>代码绘图结果就是上文贴出的图。</p>
<h2 id="nn-PoissonNLLLoss"><a href="#nn-PoissonNLLLoss" class="headerlink" title="nn.PoissonNLLLoss"></a>nn.PoissonNLLLoss</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PoissonNLLLoss</span><span class="params">(_Loss)</span>:</span></span><br><span class="line">    <span class="string">r"""Negative log likelihood loss with Poisson distribution of target.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The loss can be described as:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">        \text&#123;target&#125; \sim \mathrm&#123;Poisson&#125;(\text&#123;input&#125;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        \text&#123;loss&#125;(\text&#123;input&#125;, \text&#123;target&#125;) = \text&#123;input&#125; - \text&#123;target&#125; * \log(\text&#123;input&#125;)</span></span><br><span class="line"><span class="string">                                    + \log(\text&#123;target!&#125;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The last term can be omitted or approximated with Stirling formula. The</span></span><br><span class="line"><span class="string">    approximation is used for target values more than 1. For targets less or</span></span><br><span class="line"><span class="string">    equal to 1 zeros are added to the loss.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        log_input (bool, optional): if ``True`` the loss is computed as</span></span><br><span class="line"><span class="string">            :math:`\exp(\text&#123;input&#125;) - \text&#123;target&#125;*\text&#123;input&#125;`, if ``False`` the loss is</span></span><br><span class="line"><span class="string">            :math:`\text&#123;input&#125; - \text&#123;target&#125;*\log(\text&#123;input&#125;+\text&#123;eps&#125;)`.</span></span><br><span class="line"><span class="string">        full (bool, optional): whether to compute full loss, i. e. to add the</span></span><br><span class="line"><span class="string">            Stirling approximation term</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            .. math::</span></span><br><span class="line"><span class="string">                \text&#123;target&#125;*\log(\text&#123;target&#125;) - \text&#123;target&#125; + 0.5 * \log(2\pi\text&#123;target&#125;).</span></span><br><span class="line"><span class="string">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span></span><br><span class="line"><span class="string">            the losses are averaged over each loss element in the batch. Note that for</span></span><br><span class="line"><span class="string">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span></span><br><span class="line"><span class="string">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span></span><br><span class="line"><span class="string">            when reduce is ``False``. Default: ``True``</span></span><br><span class="line"><span class="string">        eps (float, optional): Small value to avoid evaluation of :math:`\log(0)` when</span></span><br><span class="line"><span class="string">            :attr:`log_input = False`. Default: 1e-8</span></span><br><span class="line"><span class="string">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span></span><br><span class="line"><span class="string">            losses are averaged or summed over observations for each minibatch depending</span></span><br><span class="line"><span class="string">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span></span><br><span class="line"><span class="string">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span></span><br><span class="line"><span class="string">        reduction (string, optional): Specifies the reduction to apply to the output:</span></span><br><span class="line"><span class="string">            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,</span></span><br><span class="line"><span class="string">            ``'mean'``: the sum of the output will be divided by the number of</span></span><br><span class="line"><span class="string">            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`</span></span><br><span class="line"><span class="string">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span></span><br><span class="line"><span class="string">            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; loss = nn.PoissonNLLLoss()</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; log_input = torch.randn(5, 2, requires_grad=True)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; target = torch.randn(5, 2)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; output = loss(log_input, target)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; output.backward()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Shape:</span></span><br><span class="line"><span class="string">        - Input: :math:`(N, *)` where :math:`*` means, any number of additional</span></span><br><span class="line"><span class="string">          dimensions</span></span><br><span class="line"><span class="string">        - Target: :math:`(N, *)`, same shape as the input</span></span><br><span class="line"><span class="string">        - Output: scalar by default. If :attr:`reduction` is ``'none'``, then :math:`(N, *)`,</span></span><br><span class="line"><span class="string">          the same shape as the input</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    __constants__ = [<span class="string">'log_input'</span>, <span class="string">'full'</span>, <span class="string">'eps'</span>, <span class="string">'reduction'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, log_input=True, full=False, size_average=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 eps=<span class="number">1e-8</span>, reduce=None, reduction=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line">        super(PoissonNLLLoss, self).__init__(size_average, reduce, reduction)</span><br><span class="line">        self.log_input = log_input</span><br><span class="line">        self.full = full</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, log_input, target)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.poisson_nll_loss(log_input, target, log_input=self.log_input, full=self.full,</span><br><span class="line">                                  eps=self.eps, reduction=self.reduction)</span><br></pre></td></tr></table></figure>
<p><strong>功能</strong>: 泊松分布的负对数似然损失函数，是输出分类服从泊松分布的时候使用的损失函数。</p>
<p>主要参数:</p>
<ul>
<li><strong>log_input</strong>: 输入是否为对数形式, 决定计算公式</li>
<li><strong>full</strong>: 计算所有loss, 默认为False</li>
<li><strong>eps</strong>: 修正项, 如果input是0或过小则容易导致nan，而eps的使用就是为了避免log(input)为nan</li>
<li><strong>reduction</strong>: </li>
</ul>
<p><strong>计算公式</strong>:<br>log_input = True</p>
<script type="math/tex; mode=display">
\text{loss(input, target) = exp(input) - target * input}  \tag{1}</script><p>log_input = False</p>
<script type="math/tex; mode=display">
\text{loss(input, target) = input - target * log(input + eps)}  \tag{2}</script><p>通过观察公式可知，(2)式相当于是对(1)式的对应项取对数：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \text{loss(input, target)} &= \text{log exp(input) - target * log(input + eps)} \\
    &= \text{input - target * log(input + eps)}
\end{aligned}</script><p>nn.PoissonNLLLoss 会逐个神经元计算loss</p>
<p>代码示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ------------------------------------------------- 8 Poisson NLL Loss ----------------------------------------------</span></span><br><span class="line"><span class="comment"># flag = 0</span></span><br><span class="line">flag = <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line"></span><br><span class="line">    inputs = torch.randn((<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    target = torch.randn((<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    loss_f = nn.PoissonNLLLoss(log_input=<span class="literal">True</span>, full=<span class="literal">False</span>, reduction=<span class="string">'none'</span>)</span><br><span class="line">    loss = loss_f(inputs, target)</span><br><span class="line">    print(<span class="string">"input:&#123;&#125;\ntarget:&#123;&#125;\nPoisson NLL loss:&#123;&#125;"</span>.format(inputs, target, loss))</span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------------------------- compute by hand</span></span><br><span class="line"><span class="comment"># flag = 0</span></span><br><span class="line">flag = <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line"></span><br><span class="line">    idx = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    loss_1 = torch.exp(inputs[idx, idx]) - target[idx, idx]*inputs[idx, idx]</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"第一个元素loss:"</span>, loss_1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line">input:</span><br><span class="line">tensor([[<span class="number">0.6614</span>, <span class="number">0.2669</span>],</span><br><span class="line">        [<span class="number">0.0617</span>, <span class="number">0.6213</span>]])</span><br><span class="line"></span><br><span class="line">target:</span><br><span class="line">tensor([[<span class="number">-0.4519</span>, <span class="number">-0.1661</span>],</span><br><span class="line">        [<span class="number">-1.5228</span>,  <span class="number">0.3817</span>]])</span><br><span class="line"></span><br><span class="line">Poisson NLL loss:</span><br><span class="line">tensor([[<span class="number">2.2363</span>, <span class="number">1.3503</span>],</span><br><span class="line">        [<span class="number">1.1575</span>, <span class="number">1.6242</span>]])</span><br><span class="line"></span><br><span class="line">第一个元素loss: </span><br><span class="line">tensor(<span class="number">2.2363</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="nn-KLDivLoss"><a href="#nn-KLDivLoss" class="headerlink" title="nn.KLDivLoss"></a>nn.KLDivLoss</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KLDivLoss</span><span class="params">(_Loss)</span>:</span></span><br><span class="line">    <span class="string">r"""The `Kullback-Leibler divergence`_ Loss</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    KL divergence is a useful distance measure for continuous distributions</span></span><br><span class="line"><span class="string">    and is often useful when performing direct regression over the space of</span></span><br><span class="line"><span class="string">    (discretely sampled) continuous output distributions.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    As with :class:`~torch.nn.NLLLoss`, the `input` given is expected to contain</span></span><br><span class="line"><span class="string">    *log-probabilities* and is not restricted to a 2D Tensor.</span></span><br><span class="line"><span class="string">    The targets are given as *probabilities* (i.e. without taking the logarithm).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This criterion expects a `target` `Tensor` of the same size as the</span></span><br><span class="line"><span class="string">    `input` `Tensor`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">        l(x,y) = L = \&#123; l_1,\dots,l_N \&#125;, \quad</span></span><br><span class="line"><span class="string">        l_n = y_n \cdot \left( \log y_n - x_n \right)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    where the index :math:`N` spans all dimensions of ``input`` and :math:`L` has the same</span></span><br><span class="line"><span class="string">    shape as ``input``. If :attr:`reduction` is not ``'none'`` (default ``'mean'``), then:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">        \ell(x, y) = \begin&#123;cases&#125;</span></span><br><span class="line"><span class="string">            \operatorname&#123;mean&#125;(L), &amp; \text&#123;if reduction&#125; = \text&#123;'mean';&#125; \\</span></span><br><span class="line"><span class="string">            \operatorname&#123;sum&#125;(L),  &amp; \text&#123;if reduction&#125; = \text&#123;'sum'.&#125;</span></span><br><span class="line"><span class="string">        \end&#123;cases&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    In default :attr:`reduction` mode ``'mean'``, the losses are averaged for each minibatch over observations</span></span><br><span class="line"><span class="string">    **as well as** over dimensions. ``'batchmean'`` mode gives the correct KL divergence where losses</span></span><br><span class="line"><span class="string">    are averaged over batch dimension only. ``'mean'`` mode's behavior will be changed to the same as</span></span><br><span class="line"><span class="string">    ``'batchmean'`` in the next major release.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. _Kullback-Leibler divergence:</span></span><br><span class="line"><span class="string">        https://en.wikipedia.org/wiki/Kullback-Leibler_divergence</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span></span><br><span class="line"><span class="string">            the losses are averaged over each loss element in the batch. Note that for</span></span><br><span class="line"><span class="string">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span></span><br><span class="line"><span class="string">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span></span><br><span class="line"><span class="string">            when reduce is ``False``. Default: ``True``</span></span><br><span class="line"><span class="string">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span></span><br><span class="line"><span class="string">            losses are averaged or summed over observations for each minibatch depending</span></span><br><span class="line"><span class="string">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span></span><br><span class="line"><span class="string">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span></span><br><span class="line"><span class="string">        reduction (string, optional): Specifies the reduction to apply to the output:</span></span><br><span class="line"><span class="string">            ``'none'`` | ``'batchmean'`` | ``'sum'`` | ``'mean'``.</span></span><br><span class="line"><span class="string">            ``'none'``: no reduction will be applied.</span></span><br><span class="line"><span class="string">            ``'batchmean'``: the sum of the output will be divided by batchsize.</span></span><br><span class="line"><span class="string">            ``'sum'``: the output will be summed.</span></span><br><span class="line"><span class="string">            ``'mean'``: the output will be divided by the number of elements in the output.</span></span><br><span class="line"><span class="string">            Default: ``'mean'``</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. note::</span></span><br><span class="line"><span class="string">        :attr:`size_average` and :attr:`reduce` are in the process of being deprecated,</span></span><br><span class="line"><span class="string">        and in the meantime, specifying either of those two args will override :attr:`reduction`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. note::</span></span><br><span class="line"><span class="string">        :attr:`reduction` = ``'mean'`` doesn't return the true kl divergence value, please use</span></span><br><span class="line"><span class="string">        :attr:`reduction` = ``'batchmean'`` which aligns with KL math definition.</span></span><br><span class="line"><span class="string">        In the next major release, ``'mean'`` will be changed to be the same as ``'batchmean'``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Shape:</span></span><br><span class="line"><span class="string">        - Input: :math:`(N, *)` where :math:`*` means, any number of additional</span></span><br><span class="line"><span class="string">          dimensions</span></span><br><span class="line"><span class="string">        - Target: :math:`(N, *)`, same shape as the input</span></span><br><span class="line"><span class="string">        - Output: scalar by default. If :attr:``reduction`` is ``'none'``, then :math:`(N, *)`,</span></span><br><span class="line"><span class="string">          the same shape as the input</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    __constants__ = [<span class="string">'reduction'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size_average=None, reduce=None, reduction=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line">        super(KLDivLoss, self).__init__(size_average, reduce, reduction)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, target)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.kl_div(input, target, reduction=self.reduction)</span><br></pre></td></tr></table></figure>
<p><strong>功能</strong>: 计算 KLD(divergence), KL 散度, 相对熵，用于衡量两个分布之间的相似性，也可以理解为计算两个分布的距离。<br><strong>注意事项</strong>: 需提前将输入计算 log-probabilities, 如通过nn.logsoftmax()</p>
<p>KL散度定义计算公式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    D_{KL}(P \Vert Q) = E_{x \sim p} \Bigg [ log \frac{P(x)}{Q(x)} \Bigg] &= E_{x \sim p} [log P(x) - log Q(x)] \\
    &= \sum^N_{i=1} P(X_i) (log P(X_i) - log Q(X_i))
\end{aligned}</script><p>其中 P 是真实分布，$P(X_i)$ 其实也就是标签，Q 是预测分布，$Q(X_i)$ 表示的模型输出的概率。</p>
<p>PyTorch中的计算公式：(以一个样本为例，所以这里忽略了求和符号 $\sum$)</p>
<script type="math/tex; mode=display">
l_n = y_n \cdot (log y_n - x_n)</script><p>&emsp; KL散度在PyTorch中的计算公式与KL散度定义的计算公式在外观上看略有不同，主要表现在定义的计算公式的最后一项是减 $log Q(X_i)$，而PyTorch中的计算公式的最后一项是减 $x_n$。但实际上它们是相同的，因为PyTorch 的 nn.KLDivLoss 要求先对输入计算一个 log-probabilities，然后再送入到 nn.KLDivLoss 计算，可以通过 nn.logsoftmax() 来计算 log-probabilities.</p>
<p>主要参数:</p>
<ul>
<li><strong>reduction</strong>: none/sum/mean/batchmean. 注意这里的 reduction 比前面其他 Loss 函数的 reduction 多了一个 batchmean。注意：mean的分母是 (batchsize * num_class)，而batchmean的分母是batchsize，参考下面的代码理解.</li>
<li><strong>batchmean</strong>: batchsize维度求平均值。</li>
</ul>
<p>代码示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ------------------------------------------------- 9 KL Divergence Loss ----------------------------------------------</span></span><br><span class="line"><span class="comment"># flag = 0</span></span><br><span class="line">flag = <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line"></span><br><span class="line">    inputs = torch.tensor([[<span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.2</span>], [<span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.5</span>]])</span><br><span class="line">    inputs_log = torch.log(inputs)</span><br><span class="line">    target = torch.tensor([[<span class="number">0.9</span>, <span class="number">0.05</span>, <span class="number">0.05</span>], [<span class="number">0.1</span>, <span class="number">0.7</span>, <span class="number">0.2</span>]], dtype=torch.float)</span><br><span class="line"></span><br><span class="line">    loss_f_none = nn.KLDivLoss(reduction=<span class="string">'none'</span>)</span><br><span class="line">    loss_f_mean = nn.KLDivLoss(reduction=<span class="string">'mean'</span>)</span><br><span class="line">    loss_f_bs_mean = nn.KLDivLoss(reduction=<span class="string">'batchmean'</span>)</span><br><span class="line"></span><br><span class="line">    loss_none = loss_f_none(inputs, target)</span><br><span class="line">    loss_mean = loss_f_mean(inputs, target)</span><br><span class="line">    loss_bs_mean = loss_f_bs_mean(inputs, target)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"loss_none:\n&#123;&#125;\nloss_mean:\n&#123;&#125;\nloss_bs_mean:\n&#123;&#125;"</span>.format(loss_none, loss_mean, loss_bs_mean))</span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------------------------- compute by hand</span></span><br><span class="line"><span class="comment"># flag = 0</span></span><br><span class="line">flag = <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line"></span><br><span class="line">    idx = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    loss_1 = target[idx, idx] * (torch.log(target[idx, idx]) - inputs[idx, idx])</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"第一个元素loss:"</span>, loss_1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line">loss_none:</span><br><span class="line">tensor([[<span class="number">-0.5448</span>, <span class="number">-0.1648</span>, <span class="number">-0.1598</span>],</span><br><span class="line">        [<span class="number">-0.2503</span>, <span class="number">-0.4597</span>, <span class="number">-0.4219</span>]])</span><br><span class="line"></span><br><span class="line">loss_mean:</span><br><span class="line"><span class="number">-0.3335360586643219</span></span><br><span class="line"></span><br><span class="line">loss_bs_mean:</span><br><span class="line"><span class="number">-1.000608205795288</span></span><br><span class="line"></span><br><span class="line">第一个元素loss: </span><br><span class="line">tensor(<span class="number">-0.5448</span>)</span><br></pre></td></tr></table></figure></p>
<p>上面代码中，inputs batchsize = 2, num_class = 3</p>
<script type="math/tex; mode=display">
nn.KLDivLoss(reduction='mean') = \frac{sum(loss\_none)}{batchsize * num\_class}</script><script type="math/tex; mode=display">
nn.KLDivLoss(reduction='batchmean') = \frac{sum(loss\_none)}{batchsize * num\_class}</script><h2 id="nn-MarginRankingLoss"><a href="#nn-MarginRankingLoss" class="headerlink" title="nn.MarginRankingLoss"></a>nn.MarginRankingLoss</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MarginRankingLoss</span><span class="params">(_Loss)</span>:</span></span><br><span class="line">    <span class="string">r"""Creates a criterion that measures the loss given</span></span><br><span class="line"><span class="string">    inputs :math:`x1`, :math:`x2`, two 1D mini-batch `Tensors`,</span></span><br><span class="line"><span class="string">    and a label 1D mini-batch tensor :math:`y` (containing 1 or -1).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    If :math:`y = 1` then it assumed the first input should be ranked higher</span></span><br><span class="line"><span class="string">    (have a larger value) than the second input, and vice-versa for :math:`y = -1`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The loss function for each sample in the mini-batch is:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">        \text&#123;loss&#125;(x, y) = \max(0, -y * (x1 - x2) + \text&#123;margin&#125;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        margin (float, optional): Has a default value of :math:`0`.</span></span><br><span class="line"><span class="string">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span></span><br><span class="line"><span class="string">            the losses are averaged over each loss element in the batch. Note that for</span></span><br><span class="line"><span class="string">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span></span><br><span class="line"><span class="string">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span></span><br><span class="line"><span class="string">            when reduce is ``False``. Default: ``True``</span></span><br><span class="line"><span class="string">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span></span><br><span class="line"><span class="string">            losses are averaged or summed over observations for each minibatch depending</span></span><br><span class="line"><span class="string">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span></span><br><span class="line"><span class="string">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span></span><br><span class="line"><span class="string">        reduction (string, optional): Specifies the reduction to apply to the output:</span></span><br><span class="line"><span class="string">            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,</span></span><br><span class="line"><span class="string">            ``'mean'``: the sum of the output will be divided by the number of</span></span><br><span class="line"><span class="string">            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`</span></span><br><span class="line"><span class="string">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span></span><br><span class="line"><span class="string">            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Shape:</span></span><br><span class="line"><span class="string">        - Input: :math:`(N, D)` where `N` is the batch size and `D` is the size of a sample.</span></span><br><span class="line"><span class="string">        - Target: :math:`(N)`</span></span><br><span class="line"><span class="string">        - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(N)`.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    __constants__ = [<span class="string">'margin'</span>, <span class="string">'reduction'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, margin=<span class="number">0.</span>, size_average=None, reduce=None, reduction=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line">        super(MarginRankingLoss, self).__init__(size_average, reduce, reduction)</span><br><span class="line">        self.margin = margin</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input1, input2, target)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.margin_ranking_loss(input1, input2, target, margin=self.margin, reduction=self.reduction)</span><br></pre></td></tr></table></figure>
<p><strong>功能</strong>: 计算两个向量之间的相似度 ,用于排序任务 \</p>
<blockquote>
<p><strong>特别说明</strong>: 该方法计算两组数据之间的差异, 返回一个 n<em>n 的 loss 矩阵。\<br>先计算第1组的第1个元素和第2组的n个元素的差异，得到n个loss值；\<br>再计算第1组的第2个元素和第2组的n个元素的差异，得到n个loss值；\<br>… \<br>再计算第1组的第n个元素和第2组的n个元素的差异，得到n个loss值；\<br>以上共得到n</em>n个loss值。</p>
</blockquote>
<p>计算公式：</p>
<script type="math/tex; mode=display">
loss(x, y) = max(0, -y * (x1 - x2) + margin), \qquad y \in \{-1, 1\}</script><p>主要参数:</p>
<ul>
<li><strong>margin</strong>: 边界值, x1与x2之间的差异值</li>
<li><strong>reduction</strong>: 计算模式, 可为none/sum/mean </li>
</ul>
<p>y = 1 时, 希望 x1 比 x2 大, 当 x1&gt;x2 时, 不产生 loss （此时loss = 0）\<br>y = -1 时, 希望 x2 比 x1 大, 当 x2&gt;x1 时, 不产生 loss</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------------------------------------- 10 Margin Ranking Loss --------------------------------------------</span></span><br><span class="line"><span class="comment"># flag = 0</span></span><br><span class="line">flag = <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line"></span><br><span class="line">    x1 = torch.tensor([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]], dtype=torch.float)</span><br><span class="line">    x2 = torch.tensor([[<span class="number">2</span>], [<span class="number">2</span>], [<span class="number">2</span>]], dtype=torch.float)</span><br><span class="line"></span><br><span class="line">    target = torch.tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>], dtype=torch.float)</span><br><span class="line"></span><br><span class="line">    loss_f_none = nn.MarginRankingLoss(margin=<span class="number">0</span>, reduction=<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">    loss = loss_f_none(x1, x2, target)</span><br><span class="line"></span><br><span class="line">    print(loss)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure>
<p>计算过程解析：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    output[0, 0] &= max \Big(0, -y[0] * (x1[0] - x2[0]) \Big) \\
                 &= max \Big(0, -1 * (1 - 2) \Big) = 1
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
    output[0, 1] &= max \Big(0, -y[1] * (x1[0] - x2[1]) \Big) \\
                 &= max \Big(0, -1 * (1 - 2) \Big) = 1
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
    output[0, 2] &= max \Big(0, -y[2] * (x1[0] - x2[1]) \Big) \\
                 &= max \Big(0, -(-1) * (1 - 2) \Big) = 0
\end{aligned}</script><h2 id="nn-MultiLabelMarginLoss"><a href="#nn-MultiLabelMarginLoss" class="headerlink" title="nn.MultiLabelMarginLoss"></a>nn.MultiLabelMarginLoss</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiLabelMarginLoss</span><span class="params">(_Loss)</span>:</span></span><br><span class="line">    <span class="string">r"""Creates a criterion that optimizes a multi-class multi-classification</span></span><br><span class="line"><span class="string">    hinge loss (margin-based loss) between input :math:`x` (a 2D mini-batch `Tensor`)</span></span><br><span class="line"><span class="string">    and output :math:`y` (which is a 2D `Tensor` of target class indices).</span></span><br><span class="line"><span class="string">    For each sample in the mini-batch:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">        \text&#123;loss&#125;(x, y) = \sum_&#123;ij&#125;\frac&#123;\max(0, 1 - (x[y[j]] - x[i]))&#125;&#123;\text&#123;x.size&#125;(0)&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    where :math:`x \in \left\&#123;0, \; \cdots , \; \text&#123;x.size&#125;(0) - 1\right\&#125;`, \</span></span><br><span class="line"><span class="string">    :math:`y \in \left\&#123;0, \; \cdots , \; \text&#123;y.size&#125;(0) - 1\right\&#125;`, \</span></span><br><span class="line"><span class="string">    :math:`0 \leq y[j] \leq \text&#123;x.size&#125;(0)-1`, \</span></span><br><span class="line"><span class="string">    and :math:`i \neq y[j]` for all :math:`i` and :math:`j`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :math:`y` and :math:`x` must have the same size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The criterion only considers a contiguous block of non-negative targets that</span></span><br><span class="line"><span class="string">    starts at the front.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This allows for different samples to have variable amounts of target classes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span></span><br><span class="line"><span class="string">            the losses are averaged over each loss element in the batch. Note that for</span></span><br><span class="line"><span class="string">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span></span><br><span class="line"><span class="string">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span></span><br><span class="line"><span class="string">            when reduce is ``False``. Default: ``True``</span></span><br><span class="line"><span class="string">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span></span><br><span class="line"><span class="string">            losses are averaged or summed over observations for each minibatch depending</span></span><br><span class="line"><span class="string">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span></span><br><span class="line"><span class="string">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span></span><br><span class="line"><span class="string">        reduction (string, optional): Specifies the reduction to apply to the output:</span></span><br><span class="line"><span class="string">            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,</span></span><br><span class="line"><span class="string">            ``'mean'``: the sum of the output will be divided by the number of</span></span><br><span class="line"><span class="string">            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`</span></span><br><span class="line"><span class="string">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span></span><br><span class="line"><span class="string">            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Shape:</span></span><br><span class="line"><span class="string">        - Input: :math:`(C)` or :math:`(N, C)` where `N` is the batch size and `C`</span></span><br><span class="line"><span class="string">          is the number of classes.</span></span><br><span class="line"><span class="string">        - Target: :math:`(C)` or :math:`(N, C)`, label targets padded by -1 ensuring same shape as the input.</span></span><br><span class="line"><span class="string">        - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(N)`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; loss = nn.MultiLabelMarginLoss()</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; x = torch.FloatTensor([[0.1, 0.2, 0.4, 0.8]])</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # for target y, only consider labels 3 and 0, not after label -1</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; y = torch.LongTensor([[3, 0, -1, 1]])</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; loss(x, y)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # 0.25 * ((1-(0.1-0.2)) + (1-(0.1-0.4)) + (1-(0.8-0.2)) + (1-(0.8-0.4)))</span></span><br><span class="line"><span class="string">        tensor(0.8500)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    __constants__ = [<span class="string">'reduction'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size_average=None, reduce=None, reduction=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line">        super(MultiLabelMarginLoss, self).__init__(size_average, reduce, reduction)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, target)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.multilabel_margin_loss(input, target, reduction=self.reduction)</span><br></pre></td></tr></table></figure>
<p><strong>功能</strong>: 多标签边界损失函数，不要与多分类任务混淆。</p>
<p>主要参数:</p>
<ul>
<li><strong>reduction</strong>: 计算模式, 可为none/sum/mean</li>
</ul>
<p><strong>什么是多标签任务</strong>：多标签任务是指某个样本可以是多个类别。例如图片类别分类（美食类，风景类，人像类，…），对于下面的图片，它既具有“云”的标签，也具有“树”的标签，还具有“草地”标签。</p>
<div align=center>
  <img src="https://github.com/JuneXia/JuneXia.github.io/raw/hexo/source/images/ml/nn_loss5.jpg" width = 60% height = 60% />
</div>

<p>计算公式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    loss(x, y) &= \sum_{ij} \frac{ \text{max} \Bigg( 0, 1 - \Big(x \Big[ y[j] \Big] - x[i] \Big) \Bigg) }{x.size(0)} \\
\text{where } i == 0 \text{ to } x.size(0), j &== 0 \text{ to } y.size(0), y[j] \geq 0, \text{ and } i \neq y[j] \text{ for all } i \text{ and } j.
\end{aligned}</script><p>式中 $y[j]$ 表示的标签，而$x\Big[ y[j] \Big]$ 的意思就是“标签所在神经元（即对应类别的预测值）”，又因为要求 $i \neq y[j]$，所以 $x[i]$ 表示的就是“非标签所在神经元（即非对应类别的预测值）”，当 $x \Big[ y[j] \Big] - x[i] &lt; 0$ 时，$\text{max} \Bigg( 0, 1 - \Big(x \Big[ y[j] \Big] - x[i] \Big) \Bigg)$ 定会 $&gt; 1$，而梯度下降是要朝着 loss 减小的方向前进的，所以为了让 loss 更小，则必然会希望 $x \Big[ y[j] \Big] - x[i] &gt; 0$，即希望 $x \Big[ y[j] \Big] - x[i]$ 朝着 $&gt; 0$ 的方向前进。说了那么多，其实用一句话总结就是 loss 希望非标签所在神经元要大于非标签所在神经元且要差值大于1.</p>
<p><strong>举个栗子</strong>: 四分类任务, 样本x属于0类和3类，此时样本 x 的标签是 [0, 3, -1, -1]（注意不是不是[1, 0, 0, 1]），详情先看看下面的代码吧：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------------------------------------- 11 Multi Label Margin Loss -----------------------------------------</span></span><br><span class="line"><span class="comment"># flag = 0</span></span><br><span class="line">flag = <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line"></span><br><span class="line">    x = torch.tensor([[<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.8</span>]])</span><br><span class="line">    y = torch.tensor([[<span class="number">0</span>, <span class="number">3</span>, <span class="number">-1</span>, <span class="number">-1</span>]], dtype=torch.long)  <span class="comment"># 这个Tensor数据类型一定要是long</span></span><br><span class="line"></span><br><span class="line">    loss_f = nn.MultiLabelMarginLoss(reduction=<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">    loss = loss_f(x, y)</span><br><span class="line"></span><br><span class="line">    print(loss)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line">tensor([<span class="number">0.8500</span>])</span><br></pre></td></tr></table></figure></p>
<p>下面来分析下计算过程.</p>
<p>对于下面这样的两个中张量:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.8</span>]])</span><br><span class="line">y = torch.tensor([[<span class="number">0</span>, <span class="number">3</span>, <span class="number">-1</span>, <span class="number">-1</span>]], dtype=torch.long)  <span class="comment"># 这个标签Tensor的数据类型一定要是long</span></span><br></pre></td></tr></table></figure><br>（上面的 x 和 y 是用二维tensor表示的，y.shape = x.shape = (1, 4)，这是因为第一个维度表示的 batch_size，然而下面为了叙述方便就不考虑第1个维度了）\<br>其中 x 对应预测值，y 对应对应标签，$y[j] \geq 0$ 的表示对应标签，$y[j] \lt 0$ 的表示非对应标签。\<br>为了叙述方便，这里规定用下标 $j$ 索引标签 $y$，用下标 $i$ 索引预测值 $x$. 按照上述计算公式的要求， \<br>因为 $\text{x.size(0) = 4, y.size(0) = 4}$，所以有 $i \in [0, 4]$，$j \in [0, 4]$， \<br>又因为 $y[j] \geq 0$，所以 $j$ 只能 $\in \{ 0, 1 \}$，因而 $y[j] \in \{ 0, 3 \}$， \<br>又因为 $i \neq y[j]$，所以 $i$ 只能 $\in \{ 1, 2 \}$，因而 $x[i] \in \{ 0.2, 0.4 \}$，而 $x[ y[j] ] \in \{ 0.1, 0.8 \}$</p>
<p>将上述数值带入计算公式，便有：\<br>当 $j=0$ 时，$y[j] = 0, x[y[j]] = x[0]$，带入公式得：(这里为了叙述方便，暂时不考虑分母)</p>
<script type="math/tex; mode=display">
\text{item}_1 = \sum_{i \in \{1, 2 \}, \; j=0} = \text{max(0, 1-(x[0] - x[1])) + max(0, 1-(x[0] - x[2]))}</script><script type="math/tex; mode=display">
\text{item}_2 = \sum_{i \in \{1, 2 \}, \; j=1} = \text{max(0, 1-(x[3] - x[1])) + max(0, 1-(x[3] - x[2]))}</script><p>将上面两式汇总，并带入分母，便有得到最后的 loss：</p>
<script type="math/tex; mode=display">
\text{loss}(x, y) = \sum_{ij} = \frac{\text{item}_1 + \text{item}_2}{4}</script><p>下面是手动计算的代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --------------------------------- compute by hand</span></span><br><span class="line"><span class="comment"># flag = 0</span></span><br><span class="line">flag = <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line">    x = x[<span class="number">0</span>]</span><br><span class="line">    item_1 = (<span class="number">1</span>-(x[<span class="number">0</span>] - x[<span class="number">1</span>])) + (<span class="number">1</span> - (x[<span class="number">0</span>] - x[<span class="number">2</span>]))    <span class="comment"># [0]</span></span><br><span class="line">    item_2 = (<span class="number">1</span>-(x[<span class="number">3</span>] - x[<span class="number">1</span>])) + (<span class="number">1</span> - (x[<span class="number">3</span>] - x[<span class="number">2</span>]))    <span class="comment"># [3]</span></span><br><span class="line"></span><br><span class="line">    loss_h = (item_1 + item_2) / x.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    print(loss_h)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line">tensor(<span class="number">0.8500</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="nn-SoftMarginLoss"><a href="#nn-SoftMarginLoss" class="headerlink" title="nn.SoftMarginLoss"></a>nn.SoftMarginLoss</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SoftMarginLoss</span><span class="params">(_Loss)</span>:</span></span><br><span class="line">    <span class="string">r"""Creates a criterion that optimizes a two-class classification</span></span><br><span class="line"><span class="string">    logistic loss between input tensor :math:`x` and target tensor :math:`y`</span></span><br><span class="line"><span class="string">    (containing 1 or -1).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">        \text&#123;loss&#125;(x, y) = \sum_i \frac&#123;\log(1 + \exp(-y[i]*x[i]))&#125;&#123;\text&#123;x.nelement&#125;()&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span></span><br><span class="line"><span class="string">            the losses are averaged over each loss element in the batch. Note that for</span></span><br><span class="line"><span class="string">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span></span><br><span class="line"><span class="string">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span></span><br><span class="line"><span class="string">            when reduce is ``False``. Default: ``True``</span></span><br><span class="line"><span class="string">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span></span><br><span class="line"><span class="string">            losses are averaged or summed over observations for each minibatch depending</span></span><br><span class="line"><span class="string">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span></span><br><span class="line"><span class="string">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span></span><br><span class="line"><span class="string">        reduction (string, optional): Specifies the reduction to apply to the output:</span></span><br><span class="line"><span class="string">            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,</span></span><br><span class="line"><span class="string">            ``'mean'``: the sum of the output will be divided by the number of</span></span><br><span class="line"><span class="string">            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`</span></span><br><span class="line"><span class="string">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span></span><br><span class="line"><span class="string">            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Shape:</span></span><br><span class="line"><span class="string">        - Input: :math:`(*)` where :math:`*` means, any number of additional</span></span><br><span class="line"><span class="string">          dimensions</span></span><br><span class="line"><span class="string">        - Target: :math:`(*)`, same shape as the input</span></span><br><span class="line"><span class="string">        - Output: scalar. If :attr:`reduction` is ``'none'``, then same shape as the input</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    __constants__ = [<span class="string">'reduction'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size_average=None, reduce=None, reduction=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line">        super(SoftMarginLoss, self).__init__(size_average, reduce, reduction)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, target)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.soft_margin_loss(input, target, reduction=self.reduction)</span><br></pre></td></tr></table></figure>
<p><strong>功能</strong>: 计算二分类的 logistic 损失。<br>主要参数:</p>
<ul>
<li><strong>reduction</strong>: 计算模式, 可为none/sum/mean</li>
</ul>
<p><strong>计算公式</strong>：</p>
<script type="math/tex; mode=display">
\text{loss}(x, y) = \sum_i \frac{log(1 + exp(-y[i] * x[i]))}{x.nelement()}</script><p>上式中，标签 $y[i]$ 只能取 $\{ -1, 1 \}$ 中的元素（注意不是 $\{ 0, 1 \}$）</p>
<p>nn.SoftMarginLoss 是逐元素计算 loss</p>
<p>代码示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------------------------------------- 12 SoftMargin Loss -----------------------------------------</span></span><br><span class="line"><span class="comment"># flag = 0</span></span><br><span class="line">flag = <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line"></span><br><span class="line">    inputs = torch.tensor([[<span class="number">0.3</span>, <span class="number">0.7</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>]])</span><br><span class="line">    target = torch.tensor([[<span class="number">-1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">-1</span>]], dtype=torch.float)</span><br><span class="line"></span><br><span class="line">    loss_f = nn.SoftMarginLoss(reduction=<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">    loss = loss_f(inputs, target)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"SoftMargin: "</span>, loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------------------------- compute by hand</span></span><br><span class="line"><span class="comment"># flag = 0</span></span><br><span class="line">flag = <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line"></span><br><span class="line">    idx = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    inputs_i = inputs[idx, idx]</span><br><span class="line">    target_i = target[idx, idx]</span><br><span class="line"></span><br><span class="line">    loss_h = np.log(<span class="number">1</span> + np.exp(-target_i * inputs_i))</span><br><span class="line"></span><br><span class="line">    print(loss_h)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line">SoftMargin:  </span><br><span class="line">tensor([[<span class="number">0.8544</span>, <span class="number">0.4032</span>],</span><br><span class="line">        [<span class="number">0.4741</span>, <span class="number">0.9741</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动计算结果：</span></span><br><span class="line">tensor(<span class="number">0.8544</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="nn-MultiLabelSoftMarginLoss"><a href="#nn-MultiLabelSoftMarginLoss" class="headerlink" title="nn.MultiLabelSoftMarginLoss"></a>nn.MultiLabelSoftMarginLoss</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiLabelSoftMarginLoss</span><span class="params">(_WeightedLoss)</span>:</span></span><br><span class="line">    <span class="string">r"""Creates a criterion that optimizes a multi-label one-versus-all</span></span><br><span class="line"><span class="string">    loss based on max-entropy, between input :math:`x` and target :math:`y` of size</span></span><br><span class="line"><span class="string">    :math:`(N, C)`.</span></span><br><span class="line"><span class="string">    For each sample in the minibatch:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">        loss(x, y) = - \frac&#123;1&#125;&#123;C&#125; * \sum_i y[i] * \log((1 + \exp(-x[i]))^&#123;-1&#125;)</span></span><br><span class="line"><span class="string">                         + (1-y[i]) * \log\left(\frac&#123;\exp(-x[i])&#125;&#123;(1 + \exp(-x[i]))&#125;\right)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    where :math:`i \in \left\&#123;0, \; \cdots , \; \text&#123;x.nElement&#125;() - 1\right\&#125;`,</span></span><br><span class="line"><span class="string">    :math:`y[i] \in \left\&#123;0, \; 1\right\&#125;`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        weight (Tensor, optional): a manual rescaling weight given to each</span></span><br><span class="line"><span class="string">            class. If given, it has to be a Tensor of size `C`. Otherwise, it is</span></span><br><span class="line"><span class="string">            treated as if having all ones.</span></span><br><span class="line"><span class="string">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span></span><br><span class="line"><span class="string">            the losses are averaged over each loss element in the batch. Note that for</span></span><br><span class="line"><span class="string">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span></span><br><span class="line"><span class="string">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span></span><br><span class="line"><span class="string">            when reduce is ``False``. Default: ``True``</span></span><br><span class="line"><span class="string">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span></span><br><span class="line"><span class="string">            losses are averaged or summed over observations for each minibatch depending</span></span><br><span class="line"><span class="string">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span></span><br><span class="line"><span class="string">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span></span><br><span class="line"><span class="string">        reduction (string, optional): Specifies the reduction to apply to the output:</span></span><br><span class="line"><span class="string">            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,</span></span><br><span class="line"><span class="string">            ``'mean'``: the sum of the output will be divided by the number of</span></span><br><span class="line"><span class="string">            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`</span></span><br><span class="line"><span class="string">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span></span><br><span class="line"><span class="string">            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Shape:</span></span><br><span class="line"><span class="string">        - Input: :math:`(N, C)` where `N` is the batch size and `C` is the number of classes.</span></span><br><span class="line"><span class="string">        - Target: :math:`(N, C)`, label targets padded by -1 ensuring same shape as the input.</span></span><br><span class="line"><span class="string">        - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(N)`.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    __constants__ = [<span class="string">'weight'</span>, <span class="string">'reduction'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, weight=None, size_average=None, reduce=None, reduction=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line">        super(MultiLabelSoftMarginLoss, self).__init__(weight, size_average, reduce, reduction)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, target)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.multilabel_soft_margin_loss(input, target, weight=self.weight, reduction=self.reduction)</span><br></pre></td></tr></table></figure>
<p><strong>功能</strong>: 是 nn.SoftMarginLoss 的多标签版本</p>
<p>主要参数:</p>
<ul>
<li><strong>weight</strong>: 各类别的loss设置权值</li>
<li><strong>reduction</strong>: 计算模式, 可为none/sum/mean</li>
</ul>
<p><strong>计算公式</strong>:</p>
<script type="math/tex; mode=display">
loss(x, y) = -\frac{1}{C} * \sum_i y[i] * log \Big(\frac{1}{1 + exp(-x[i])} \Big) + (1 - y[i]) * log \Big(\frac{exp(-x[i])}{1 + exp(-x[i])} \Big)</script><p>式中，$C$ 表示类别数量，$y$ 表示真实标签，$x$ 表示预测值. 这里 $y \in {0, 1}$.</p>
<p>举个例子:<br>对于一个具有3个标签的分类任务来说，样本 x 属于这3个标签中的其中2个标签，则样本 x 的标签为 [0, 1, 1].</p>
<p>代码示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------------------------------------- 13 MultiLabel SoftMargin Loss -----------------------------------------</span></span><br><span class="line"><span class="comment"># flag = 0</span></span><br><span class="line">flag = <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line"></span><br><span class="line">    inputs = torch.tensor([[<span class="number">0.3</span>, <span class="number">0.7</span>, <span class="number">0.8</span>]])</span><br><span class="line">    target = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]], dtype=torch.float)</span><br><span class="line"></span><br><span class="line">    loss_f = nn.MultiLabelSoftMarginLoss(reduction=<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">    loss = loss_f(inputs, target)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"MultiLabel SoftMargin: "</span>, loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------------------------- compute by hand</span></span><br><span class="line"><span class="comment"># flag = 0</span></span><br><span class="line">flag = <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line"></span><br><span class="line">    i_0 = torch.log(torch.exp(-inputs[<span class="number">0</span>, <span class="number">0</span>]) / (<span class="number">1</span> + torch.exp(-inputs[<span class="number">0</span>, <span class="number">0</span>])))</span><br><span class="line"></span><br><span class="line">    i_1 = torch.log(<span class="number">1</span> / (<span class="number">1</span> + torch.exp(-inputs[<span class="number">0</span>, <span class="number">1</span>])))</span><br><span class="line">    i_2 = torch.log(<span class="number">1</span> / (<span class="number">1</span> + torch.exp(-inputs[<span class="number">0</span>, <span class="number">2</span>])))</span><br><span class="line"></span><br><span class="line">    loss_h = (i_0 + i_1 + i_2) / <span class="number">-3</span></span><br><span class="line"></span><br><span class="line">    print(loss_h)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line">MultiLabel SoftMargin:  </span><br><span class="line">tensor([<span class="number">0.5429</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动计算结果：</span></span><br><span class="line">tensor(<span class="number">0.5429</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="nn-MultiMarginLoss"><a href="#nn-MultiMarginLoss" class="headerlink" title="nn.MultiMarginLoss"></a>nn.MultiMarginLoss</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiMarginLoss</span><span class="params">(_WeightedLoss)</span>:</span></span><br><span class="line">    <span class="string">r"""Creates a criterion that optimizes a multi-class classification hinge</span></span><br><span class="line"><span class="string">    loss (margin-based loss) between input :math:`x` (a 2D mini-batch `Tensor`) and</span></span><br><span class="line"><span class="string">    output :math:`y` (which is a 1D tensor of target class indices,</span></span><br><span class="line"><span class="string">    :math:`0 \leq y \leq \text&#123;x.size&#125;(1)-1`):</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For each mini-batch sample, the loss in terms of the 1D input :math:`x` and scalar</span></span><br><span class="line"><span class="string">    output :math:`y` is:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">        \text&#123;loss&#125;(x, y) = \frac&#123;\sum_i \max(0, \text&#123;margin&#125; - x[y] + x[i]))^p&#125;&#123;\text&#123;x.size&#125;(0)&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    where :math:`x \in \left\&#123;0, \; \cdots , \; \text&#123;x.size&#125;(0) - 1\right\&#125;`</span></span><br><span class="line"><span class="string">    and :math:`i \neq y`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Optionally, you can give non-equal weighting on the classes by passing</span></span><br><span class="line"><span class="string">    a 1D :attr:`weight` tensor into the constructor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The loss function then becomes:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">        \text&#123;loss&#125;(x, y) = \frac&#123;\sum_i \max(0, w[y] * (\text&#123;margin&#125; - x[y] + x[i]))^p)&#125;&#123;\text&#123;x.size&#125;(0)&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        p (int, optional): Has a default value of :math:`1`. :math:`1` and :math:`2`</span></span><br><span class="line"><span class="string">            are the only supported values.</span></span><br><span class="line"><span class="string">        margin (float, optional): Has a default value of :math:`1`.</span></span><br><span class="line"><span class="string">        weight (Tensor, optional): a manual rescaling weight given to each</span></span><br><span class="line"><span class="string">            class. If given, it has to be a Tensor of size `C`. Otherwise, it is</span></span><br><span class="line"><span class="string">            treated as if having all ones.</span></span><br><span class="line"><span class="string">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span></span><br><span class="line"><span class="string">            the losses are averaged over each loss element in the batch. Note that for</span></span><br><span class="line"><span class="string">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span></span><br><span class="line"><span class="string">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span></span><br><span class="line"><span class="string">            when reduce is ``False``. Default: ``True``</span></span><br><span class="line"><span class="string">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span></span><br><span class="line"><span class="string">            losses are averaged or summed over observations for each minibatch depending</span></span><br><span class="line"><span class="string">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span></span><br><span class="line"><span class="string">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span></span><br><span class="line"><span class="string">        reduction (string, optional): Specifies the reduction to apply to the output:</span></span><br><span class="line"><span class="string">            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,</span></span><br><span class="line"><span class="string">            ``'mean'``: the sum of the output will be divided by the number of</span></span><br><span class="line"><span class="string">            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`</span></span><br><span class="line"><span class="string">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span></span><br><span class="line"><span class="string">            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    __constants__ = [<span class="string">'p'</span>, <span class="string">'margin'</span>, <span class="string">'weight'</span>, <span class="string">'reduction'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, p=<span class="number">1</span>, margin=<span class="number">1.</span>, weight=None, size_average=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 reduce=None, reduction=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line">        super(MultiMarginLoss, self).__init__(weight, size_average, reduce, reduction)</span><br><span class="line">        <span class="keyword">if</span> p != <span class="number">1</span> <span class="keyword">and</span> p != <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"only p == 1 and p == 2 supported"</span>)</span><br><span class="line">        <span class="keyword">assert</span> weight <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> weight.dim() == <span class="number">1</span></span><br><span class="line">        self.p = p</span><br><span class="line">        self.margin = margin</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, target)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.multi_margin_loss(input, target, p=self.p, margin=self.margin,</span><br><span class="line">                                   weight=self.weight, reduction=self.reduction)</span><br></pre></td></tr></table></figure>
<p><strong>功能</strong>: 计算多分类的折页损失</p>
<p>主要参数:</p>
<ul>
<li><strong>p</strong>: 可选1或2</li>
<li><strong>weight</strong>: 各类别的loss设置权值</li>
<li><strong>margin</strong>: 边界值</li>
<li><strong>reduction</strong>: 计算模式, 可为none/sum/mean</li>
</ul>
<p>计算公式：</p>
<script type="math/tex; mode=display">
\text{loss}(x, y) = \frac{\sum_i \max(0, \text{margin} - x[y] + x[i])^p}{\text{x.size}(0)}</script><script type="math/tex; mode=display">
\begin{aligned}
    where & \quad x \in \left\{0, \; \cdots , \; \text{x.size}(0) - 1\right\}, 
y \in \left\{0, \; \cdots , \; \text{y.size}(0) - 1\right\}, 
0 \leq y[j] \leq \text{x.size}[0] - 1, \\
and & \quad i \neq y[j] \quad \text{for} \; \text{all} \; i \; \text{and} \; j.
\end{aligned}</script><p>y 表示标签所在的神经元，x[y] 表示标签所在神经元的输出值，x[i] 表示非标签所在神经元的输出值.（这一点和 nn.MultiLabelMarginLoss 有些类似.）</p>
<p>代码示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------------------------------------- 14 Multi Margin Loss -----------------------------------------</span></span><br><span class="line"><span class="comment"># flag = 0</span></span><br><span class="line">flag = <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line"></span><br><span class="line">    x = torch.tensor([[<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.7</span>], [<span class="number">0.2</span>, <span class="number">0.5</span>, <span class="number">0.3</span>]])</span><br><span class="line">    y = torch.tensor([<span class="number">1</span>, <span class="number">2</span>], dtype=torch.long)</span><br><span class="line"></span><br><span class="line">    loss_f = nn.MultiMarginLoss(reduction=<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">    loss = loss_f(x, y)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Multi Margin Loss: "</span>, loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------------------------- compute by hand</span></span><br><span class="line"><span class="comment"># flag = 0</span></span><br><span class="line">flag = <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line"></span><br><span class="line">    x = x[<span class="number">0</span>]</span><br><span class="line">    margin = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    i_0 = margin - (x[<span class="number">1</span>] - x[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># i_1 = margin - (x[1] - x[1])</span></span><br><span class="line">    i_2 = margin - (x[<span class="number">1</span>] - x[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    loss_h = (i_0 + i_2) / x.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    print(loss_h)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line">Multi Margin Loss:  </span><br><span class="line">tensor([<span class="number">0.8000</span>, <span class="number">0.7000</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动计算结果：</span></span><br><span class="line">tensor(<span class="number">0.8000</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="nn-TripletMarginLoss"><a href="#nn-TripletMarginLoss" class="headerlink" title="nn.TripletMarginLoss"></a>nn.TripletMarginLoss</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TripletMarginLoss</span><span class="params">(_Loss)</span>:</span></span><br><span class="line">    <span class="string">r"""Creates a criterion that measures the triplet loss given an input</span></span><br><span class="line"><span class="string">    tensors :math:`x1`, :math:`x2`, :math:`x3` and a margin with a value greater than :math:`0`.</span></span><br><span class="line"><span class="string">    This is used for measuring a relative similarity between samples. A triplet</span></span><br><span class="line"><span class="string">    is composed by `a`, `p` and `n` (i.e., `anchor`, `positive examples` and `negative</span></span><br><span class="line"><span class="string">    examples` respectively). The shapes of all input tensors should be</span></span><br><span class="line"><span class="string">    :math:`(N, D)`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The distance swap is described in detail in the paper `Learning shallow</span></span><br><span class="line"><span class="string">    convolutional feature descriptors with triplet losses`_ by</span></span><br><span class="line"><span class="string">    V. Balntas, E. Riba et al.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The loss function for each sample in the mini-batch is:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">        L(a, p, n) = \max \&#123;d(a_i, p_i) - d(a_i, n_i) + &#123;\rm margin&#125;, 0\&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    where</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">        d(x_i, y_i) = \left\lVert &#123;\bf x&#125;_i - &#123;\bf y&#125;_i \right\rVert_p</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        margin (float, optional): Default: :math:`1`.</span></span><br><span class="line"><span class="string">        p (int, optional): The norm degree for pairwise distance. Default: :math:`2`.</span></span><br><span class="line"><span class="string">        swap (bool, optional): The distance swap is described in detail in the paper</span></span><br><span class="line"><span class="string">            `Learning shallow convolutional feature descriptors with triplet losses` by</span></span><br><span class="line"><span class="string">            V. Balntas, E. Riba et al. Default: ``False``.</span></span><br><span class="line"><span class="string">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span></span><br><span class="line"><span class="string">            the losses are averaged over each loss element in the batch. Note that for</span></span><br><span class="line"><span class="string">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span></span><br><span class="line"><span class="string">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span></span><br><span class="line"><span class="string">            when reduce is ``False``. Default: ``True``</span></span><br><span class="line"><span class="string">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span></span><br><span class="line"><span class="string">            losses are averaged or summed over observations for each minibatch depending</span></span><br><span class="line"><span class="string">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span></span><br><span class="line"><span class="string">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span></span><br><span class="line"><span class="string">        reduction (string, optional): Specifies the reduction to apply to the output:</span></span><br><span class="line"><span class="string">            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,</span></span><br><span class="line"><span class="string">            ``'mean'``: the sum of the output will be divided by the number of</span></span><br><span class="line"><span class="string">            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`</span></span><br><span class="line"><span class="string">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span></span><br><span class="line"><span class="string">            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Shape:</span></span><br><span class="line"><span class="string">        - Input: :math:`(N, D)` where :math:`D` is the vector dimension.</span></span><br><span class="line"><span class="string">        - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(N)`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; anchor = torch.randn(100, 128, requires_grad=True)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; positive = torch.randn(100, 128, requires_grad=True)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; negative = torch.randn(100, 128, requires_grad=True)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; output = triplet_loss(anchor, positive, negative)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; output.backward()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. _Learning shallow convolutional feature descriptors with triplet losses:</span></span><br><span class="line"><span class="string">        http://www.bmva.org/bmvc/2016/papers/paper119/index.html</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    __constants__ = [<span class="string">'margin'</span>, <span class="string">'p'</span>, <span class="string">'eps'</span>, <span class="string">'swap'</span>, <span class="string">'reduction'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, margin=<span class="number">1.0</span>, p=<span class="number">2.</span>, eps=<span class="number">1e-6</span>, swap=False, size_average=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 reduce=None, reduction=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line">        super(TripletMarginLoss, self).__init__(size_average, reduce, reduction)</span><br><span class="line">        self.margin = margin</span><br><span class="line">        self.p = p</span><br><span class="line">        self.eps = eps</span><br><span class="line">        self.swap = swap</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, anchor, positive, negative)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.triplet_margin_loss(anchor, positive, negative, margin=self.margin, p=self.p,</span><br><span class="line">                                     eps=self.eps, swap=self.swap, reduction=self.reduction)</span><br></pre></td></tr></table></figure>
<p><strong>功能</strong>: 计算三元组损失, 人脸验证中常用</p>
<p>主要参数:</p>
<ul>
<li><strong>p</strong>:范数的阶, 默认为2</li>
<li><strong>margin</strong>: 边界值</li>
<li><strong>reduction</strong>: 计算模式, 可为none/sum/mean</li>
</ul>
<div align=center>
  <img src="https://github.com/JuneXia/JuneXia.github.io/raw/hexo/source/images/ml/nn_loss6.jpg" width = 80% height = 80% />
</div>

<p>计算公式：</p>
<script type="math/tex; mode=display">
L(a, p, n) = \text{max} {d(a_i, p_i) - d(a_i, n_i) + margin, 0}</script><script type="math/tex; mode=display">
d(x_i, y_i) = \Vert x_i - y_i \Vert _p</script><p>代码示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------------------------------------- 15 Triplet Margin Loss -----------------------------------------</span></span><br><span class="line"><span class="comment"># flag = 0</span></span><br><span class="line">flag = <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line"></span><br><span class="line">    anchor = torch.tensor([[<span class="number">1.</span>]])</span><br><span class="line">    pos = torch.tensor([[<span class="number">2.</span>]])</span><br><span class="line">    neg = torch.tensor([[<span class="number">0.5</span>]])</span><br><span class="line"></span><br><span class="line">    loss_f = nn.TripletMarginLoss(margin=<span class="number">1.0</span>, p=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    loss = loss_f(anchor, pos, neg)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Triplet Margin Loss"</span>, loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------------------------- compute by hand</span></span><br><span class="line"><span class="comment"># flag = 0</span></span><br><span class="line">flag = <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line"></span><br><span class="line">    margin = <span class="number">1</span></span><br><span class="line">    a, p, n = anchor[<span class="number">0</span>], pos[<span class="number">0</span>], neg[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    d_ap = torch.abs(a-p)</span><br><span class="line">    d_an = torch.abs(a-n)</span><br><span class="line"></span><br><span class="line">    loss = d_ap - d_an + margin</span><br><span class="line"></span><br><span class="line">    print(loss)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line">Triplet Margin Loss </span><br><span class="line">tensor(<span class="number">1.5000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动计算结果</span></span><br><span class="line">tensor([<span class="number">1.5000</span>])</span><br></pre></td></tr></table></figure></p>
<h2 id="nn-HingeEmbeddingLoss"><a href="#nn-HingeEmbeddingLoss" class="headerlink" title="nn.HingeEmbeddingLoss"></a>nn.HingeEmbeddingLoss</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HingeEmbeddingLoss</span><span class="params">(_Loss)</span>:</span></span><br><span class="line">    <span class="string">r"""Measures the loss given an input tensor :math:`x` and a labels tensor :math:`y`</span></span><br><span class="line"><span class="string">    (containing 1 or -1).</span></span><br><span class="line"><span class="string">    This is usually used for measuring whether two inputs are similar or</span></span><br><span class="line"><span class="string">    dissimilar, e.g. using the L1 pairwise distance as :math:`x`, and is typically</span></span><br><span class="line"><span class="string">    used for learning nonlinear embeddings or semi-supervised learning.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The loss function for :math:`n`-th sample in the mini-batch is</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">        l_n = \begin&#123;cases&#125;</span></span><br><span class="line"><span class="string">            x_n, &amp; \text&#123;if&#125;\; y_n = 1,\\</span></span><br><span class="line"><span class="string">            \max \&#123;0, \Delta - x_n\&#125;, &amp; \text&#123;if&#125;\; y_n = -1,</span></span><br><span class="line"><span class="string">        \end&#123;cases&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    and the total loss functions is</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">        \ell(x, y) = \begin&#123;cases&#125;</span></span><br><span class="line"><span class="string">            \operatorname&#123;mean&#125;(L), &amp; \text&#123;if reduction&#125; = \text&#123;'mean';&#125;\\</span></span><br><span class="line"><span class="string">            \operatorname&#123;sum&#125;(L),  &amp; \text&#123;if reduction&#125; = \text&#123;'sum'.&#125;</span></span><br><span class="line"><span class="string">        \end&#123;cases&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    where :math:`L = \&#123;l_1,\dots,l_N\&#125;^\top`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        margin (float, optional): Has a default value of `1`.</span></span><br><span class="line"><span class="string">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span></span><br><span class="line"><span class="string">            the losses are averaged over each loss element in the batch. Note that for</span></span><br><span class="line"><span class="string">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span></span><br><span class="line"><span class="string">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span></span><br><span class="line"><span class="string">            when reduce is ``False``. Default: ``True``</span></span><br><span class="line"><span class="string">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span></span><br><span class="line"><span class="string">            losses are averaged or summed over observations for each minibatch depending</span></span><br><span class="line"><span class="string">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span></span><br><span class="line"><span class="string">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span></span><br><span class="line"><span class="string">        reduction (string, optional): Specifies the reduction to apply to the output:</span></span><br><span class="line"><span class="string">            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,</span></span><br><span class="line"><span class="string">            ``'mean'``: the sum of the output will be divided by the number of</span></span><br><span class="line"><span class="string">            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`</span></span><br><span class="line"><span class="string">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span></span><br><span class="line"><span class="string">            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Shape:</span></span><br><span class="line"><span class="string">        - Input: :math:`(*)` where :math:`*` means, any number of dimensions. The sum operation</span></span><br><span class="line"><span class="string">          operates over all the elements.</span></span><br><span class="line"><span class="string">        - Target: :math:`(*)`, same shape as the input</span></span><br><span class="line"><span class="string">        - Output: scalar. If :attr:`reduction` is ``'none'``, then same shape as the input</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    __constants__ = [<span class="string">'margin'</span>, <span class="string">'reduction'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, margin=<span class="number">1.0</span>, size_average=None, reduce=None, reduction=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line">        super(HingeEmbeddingLoss, self).__init__(size_average, reduce, reduction)</span><br><span class="line">        self.margin = margin</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, target)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.hinge_embedding_loss(input, target, margin=self.margin, reduction=self.reduction)</span><br></pre></td></tr></table></figure>
<p><strong>功能</strong>: 计算两个输入的相似性,常用于非线性 embedding 和半监督学习</p>
<p>主要参数:</p>
<ul>
<li><strong>margin</strong>: 边界值</li>
<li><strong>reduction</strong>: 计算模式, 可为none/sum/mean</li>
</ul>
<p>计算公式：</p>
<script type="math/tex; mode=display">l_n = 
\begin{cases}
    x_n, \qquad & \text{if} \; y_n = 1, \\
    max{0, \Delta - x_n}, \qquad & \text{if} \; y_n = -1,
\end{cases}</script><p>这里的 $\Delta$ 也相当于是 margin，默认是1. \<br><strong>特别注意</strong>: 输入x应为两个输入之差的绝对值</p>
<p>代码示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------------------------------------- 16 Hinge Embedding Loss -----------------------------------------</span></span><br><span class="line"><span class="comment"># flag = 0</span></span><br><span class="line">flag = <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line"></span><br><span class="line">    inputs = torch.tensor([[<span class="number">1.</span>, <span class="number">0.8</span>, <span class="number">0.5</span>]])</span><br><span class="line">    target = torch.tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>]])</span><br><span class="line"></span><br><span class="line">    loss_f = nn.HingeEmbeddingLoss(margin=<span class="number">1</span>, reduction=<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">    loss = loss_f(inputs, target)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Hinge Embedding Loss"</span>, loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------------------------- compute by hand</span></span><br><span class="line"><span class="comment"># flag = 0</span></span><br><span class="line">flag = <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line">    margin = <span class="number">1.</span></span><br><span class="line">    loss = max(<span class="number">0</span>, margin - inputs.numpy()[<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    print(loss)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line">Hinge Embedding Loss </span><br><span class="line">tensor([[<span class="number">1.0000</span>, <span class="number">0.8000</span>, <span class="number">0.5000</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动计算结果：</span></span><br><span class="line"><span class="number">0.5</span></span><br></pre></td></tr></table></figure></p>
<h2 id="nn-CosineEmbeddingLoss"><a href="#nn-CosineEmbeddingLoss" class="headerlink" title="nn.CosineEmbeddingLoss"></a>nn.CosineEmbeddingLoss</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CosineEmbeddingLoss</span><span class="params">(_Loss)</span>:</span></span><br><span class="line">    <span class="string">r"""Creates a criterion that measures the loss given input tensors</span></span><br><span class="line"><span class="string">    :math:`x_1`, :math:`x_2` and a `Tensor` label :math:`y` with values 1 or -1.</span></span><br><span class="line"><span class="string">    This is used for measuring whether two inputs are similar or dissimilar,</span></span><br><span class="line"><span class="string">    using the cosine distance, and is typically used for learning nonlinear</span></span><br><span class="line"><span class="string">    embeddings or semi-supervised learning.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The loss function for each sample is:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">        \text&#123;loss&#125;(x, y) =</span></span><br><span class="line"><span class="string">        \begin&#123;cases&#125;</span></span><br><span class="line"><span class="string">        1 - \cos(x_1, x_2), &amp; \text&#123;if &#125; y = 1 \\</span></span><br><span class="line"><span class="string">        \max(0, \cos(x_1, x_2) - \text&#123;margin&#125;), &amp; \text&#123;if &#125; y = -1</span></span><br><span class="line"><span class="string">        \end&#123;cases&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        margin (float, optional): Should be a number from :math:`-1` to :math:`1`,</span></span><br><span class="line"><span class="string">            :math:`0` to :math:`0.5` is suggested. If :attr:`margin` is missing, the</span></span><br><span class="line"><span class="string">            default value is :math:`0`.</span></span><br><span class="line"><span class="string">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span></span><br><span class="line"><span class="string">            the losses are averaged over each loss element in the batch. Note that for</span></span><br><span class="line"><span class="string">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span></span><br><span class="line"><span class="string">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span></span><br><span class="line"><span class="string">            when reduce is ``False``. Default: ``True``</span></span><br><span class="line"><span class="string">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span></span><br><span class="line"><span class="string">            losses are averaged or summed over observations for each minibatch depending</span></span><br><span class="line"><span class="string">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span></span><br><span class="line"><span class="string">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span></span><br><span class="line"><span class="string">        reduction (string, optional): Specifies the reduction to apply to the output:</span></span><br><span class="line"><span class="string">            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,</span></span><br><span class="line"><span class="string">            ``'mean'``: the sum of the output will be divided by the number of</span></span><br><span class="line"><span class="string">            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`</span></span><br><span class="line"><span class="string">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span></span><br><span class="line"><span class="string">            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    __constants__ = [<span class="string">'margin'</span>, <span class="string">'reduction'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, margin=<span class="number">0.</span>, size_average=None, reduce=None, reduction=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line">        super(CosineEmbeddingLoss, self).__init__(size_average, reduce, reduction)</span><br><span class="line">        self.margin = margin</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input1, input2, target)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.cosine_embedding_loss(input1, input2, target, margin=self.margin, reduction=self.reduction)</span><br></pre></td></tr></table></figure>
<p><strong>功能</strong>: 采用余弦相似度计算两个输入的相似性，也是常用于非线性 embedding 和半监督学习。</p>
<p>计算公式：</p>
<script type="math/tex; mode=display">
\text{loss}(x, y) =
        \begin{cases}
        1 - \cos(x_1, x_2), & \text{if } y = 1 \\
        \max(0, \cos(x_1, x_2) - \text{margin}), & \text{if } y = -1
        \end{cases}</script><script type="math/tex; mode=display">
cos(\theta) = \frac{A \cdot B}{\Vert A \Vert \Vert B \Vert} = \frac{\sum^n_{i=1} A_i \times B_i}{\sqrt{\sum^n_{i=1}(A_i)^2} \times \sqrt{\sum^n_{i=1}(B_i)^2}}.</script><p>主要参数:</p>
<ul>
<li><strong>margin</strong>: 可取值[-1, 1]（因为cos的值域就是[-1,1]）, 推荐为[0, 0.5]</li>
<li><strong>reduction</strong>: 计算模式, 可为none/sum/mean</li>
</ul>
<h2 id="nn-CTCLoss"><a href="#nn-CTCLoss" class="headerlink" title="nn.CTCLoss"></a>nn.CTCLoss</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CTCLoss</span><span class="params">(_Loss)</span>:</span></span><br><span class="line">    <span class="string">r"""The Connectionist Temporal Classification loss.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Calculates loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the</span></span><br><span class="line"><span class="string">    probability of possible alignments of input to target, producing a loss value which is differentiable</span></span><br><span class="line"><span class="string">    with respect to each input node. The alignment of input to target is assumed to be "many-to-one", which</span></span><br><span class="line"><span class="string">    limits the length of the target sequence such that it must be :math:`\leq` the input length.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        blank (int, optional): blank label. Default :math:`0`.</span></span><br><span class="line"><span class="string">        reduction (string, optional): Specifies the reduction to apply to the output:</span></span><br><span class="line"><span class="string">            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,</span></span><br><span class="line"><span class="string">            ``'mean'``: the output losses will be divided by the target lengths and</span></span><br><span class="line"><span class="string">            then the mean over the batch is taken. Default: ``'mean'``</span></span><br><span class="line"><span class="string">        zero_infinity (bool, optional):</span></span><br><span class="line"><span class="string">            Whether to zero infinite losses and the associated gradients.</span></span><br><span class="line"><span class="string">            Default: ``False``</span></span><br><span class="line"><span class="string">            Infinite losses mainly occur when the inputs are too short</span></span><br><span class="line"><span class="string">            to be aligned to the targets.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Shape:</span></span><br><span class="line"><span class="string">        - Log_probs: Tensor of size :math:`(T, N, C)`,</span></span><br><span class="line"><span class="string">          where :math:`T = \text&#123;input length&#125;`,</span></span><br><span class="line"><span class="string">          :math:`N = \text&#123;batch size&#125;`, and</span></span><br><span class="line"><span class="string">          :math:`C = \text&#123;number of classes (including blank)&#125;`.</span></span><br><span class="line"><span class="string">          The logarithmized probabilities of the outputs (e.g. obtained with</span></span><br><span class="line"><span class="string">          :func:`torch.nn.functional.log_softmax`).</span></span><br><span class="line"><span class="string">        - Targets: Tensor of size :math:`(N, S)` or</span></span><br><span class="line"><span class="string">          :math:`(\operatorname&#123;sum&#125;(\text&#123;target\_lengths&#125;))`,</span></span><br><span class="line"><span class="string">          where :math:`N = \text&#123;batch size&#125;` and</span></span><br><span class="line"><span class="string">          :math:`S = \text&#123;max target length, if shape is &#125; (N, S)`.</span></span><br><span class="line"><span class="string">          It represent the target sequences. Each element in the target</span></span><br><span class="line"><span class="string">          sequence is a class index. And the target index cannot be blank (default=0).</span></span><br><span class="line"><span class="string">          In the :math:`(N, S)` form, targets are padded to the</span></span><br><span class="line"><span class="string">          length of the longest sequence, and stacked.</span></span><br><span class="line"><span class="string">          In the :math:`(\operatorname&#123;sum&#125;(\text&#123;target\_lengths&#125;))` form,</span></span><br><span class="line"><span class="string">          the targets are assumed to be un-padded and</span></span><br><span class="line"><span class="string">          concatenated within 1 dimension.</span></span><br><span class="line"><span class="string">        - Input_lengths: Tuple or tensor of size :math:`(N)`,</span></span><br><span class="line"><span class="string">          where :math:`N = \text&#123;batch size&#125;`. It represent the lengths of the</span></span><br><span class="line"><span class="string">          inputs (must each be :math:`\leq T`). And the lengths are specified</span></span><br><span class="line"><span class="string">          for each sequence to achieve masking under the assumption that sequences</span></span><br><span class="line"><span class="string">          are padded to equal lengths.</span></span><br><span class="line"><span class="string">        - Target_lengths: Tuple or tensor of size :math:`(N)`,</span></span><br><span class="line"><span class="string">          where :math:`N = \text&#123;batch size&#125;`. It represent lengths of the targets.</span></span><br><span class="line"><span class="string">          Lengths are specified for each sequence to achieve masking under the</span></span><br><span class="line"><span class="string">          assumption that sequences are padded to equal lengths. If target shape is</span></span><br><span class="line"><span class="string">          :math:`(N,S)`, target_lengths are effectively the stop index</span></span><br><span class="line"><span class="string">          :math:`s_n` for each target sequence, such that ``target_n = targets[n,0:s_n]`` for</span></span><br><span class="line"><span class="string">          each target in a batch. Lengths must each be :math:`\leq S`</span></span><br><span class="line"><span class="string">          If the targets are given as a 1d tensor that is the concatenation of individual</span></span><br><span class="line"><span class="string">          targets, the target_lengths must add up to the total length of the tensor.</span></span><br><span class="line"><span class="string">        - Output: scalar. If :attr:`reduction` is ``'none'``, then</span></span><br><span class="line"><span class="string">          :math:`(N)`, where :math:`N = \text&#123;batch size&#125;`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Example::</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; T = 50      # Input sequence length</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; C = 20      # Number of classes (including blank)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; N = 16      # Batch size</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; S = 30      # Target sequence length of longest target in batch</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; S_min = 10  # Minimum target length, for demonstration purposes</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt;</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # Initialize random batch of input vectors, for *size = (T,N,C)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt;</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; # Initialize random batch of targets (0 = blank, 1:C = classes)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt;</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; ctc_loss = nn.CTCLoss()</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; loss = ctc_loss(input, target, input_lengths, target_lengths)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; loss.backward()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Reference:</span></span><br><span class="line"><span class="string">        A. Graves et al.: Connectionist Temporal Classification:</span></span><br><span class="line"><span class="string">        Labelling Unsegmented Sequence Data with Recurrent Neural Networks:</span></span><br><span class="line"><span class="string">        https://www.cs.toronto.edu/~graves/icml_2006.pdf</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. Note::</span></span><br><span class="line"><span class="string">        In order to use CuDNN, the following must be satisfied: :attr:`targets` must be</span></span><br><span class="line"><span class="string">        in concatenated format, all :attr:`input_lengths` must be `T`.  :math:`blank=0`,</span></span><br><span class="line"><span class="string">        :attr:`target_lengths` :math:`\leq 256`, the integer arguments must be of</span></span><br><span class="line"><span class="string">        dtype :attr:`torch.int32`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The regular implementation uses the (more common in PyTorch) `torch.long` dtype.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. include:: cudnn_deterministic.rst</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    __constants__ = [<span class="string">'blank'</span>, <span class="string">'reduction'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, blank=<span class="number">0</span>, reduction=<span class="string">'mean'</span>, zero_infinity=False)</span>:</span></span><br><span class="line">        super(CTCLoss, self).__init__(reduction=reduction)</span><br><span class="line">        self.blank = blank</span><br><span class="line">        self.zero_infinity = zero_infinity</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, log_probs, targets, input_lengths, target_lengths)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.ctc_loss(log_probs, targets, input_lengths, target_lengths, self.blank, self.reduction,</span><br><span class="line">                          self.zero_infinity)</span><br></pre></td></tr></table></figure>
<p><strong>功能</strong>: 计算 CTC 损失, 解决时序类数据的分类。比如OCR中输入和输出长度不确定的情况下我们怎么去计算他的分类。</p>
<p>英文全称：Connectionist Temporal Classification</p>
<p>参考文献：<br>《A. Graves et al.: Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks》</p>
<p>主要参数:</p>
<ul>
<li><strong>blank</strong>: blank label</li>
<li><strong>zero_infinity</strong>: 无穷大的值或梯度置0</li>
<li><strong>reduction</strong>: 计算模式, 可为none/sum/mean</li>
</ul>
<p>代码示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ---------------------------------------------- 18 CTC Loss -----------------------------------------</span></span><br><span class="line"><span class="comment"># flag = 0</span></span><br><span class="line">flag = <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> flag:</span><br><span class="line">    T = <span class="number">50</span>      <span class="comment"># Input sequence length</span></span><br><span class="line">    C = <span class="number">20</span>      <span class="comment"># Number of classes (including blank)</span></span><br><span class="line">    N = <span class="number">16</span>      <span class="comment"># Batch size</span></span><br><span class="line">    S = <span class="number">30</span>      <span class="comment"># Target sequence length of longest target in batch</span></span><br><span class="line">    S_min = <span class="number">10</span>  <span class="comment"># Minimum target length, for demonstration purposes</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize random batch of input vectors, for *size = (T,N,C)</span></span><br><span class="line">    inputs = torch.randn(T, N, C).log_softmax(<span class="number">2</span>).detach().requires_grad_()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize random batch of targets (0 = blank, 1:C = classes)</span></span><br><span class="line">    target = torch.randint(low=<span class="number">1</span>, high=C, size=(N, S), dtype=torch.long)</span><br><span class="line"></span><br><span class="line">    input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)</span><br><span class="line">    target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)</span><br><span class="line"></span><br><span class="line">    ctc_loss = nn.CTCLoss()</span><br><span class="line">    loss = ctc_loss(inputs, target, input_lengths, target_lengths)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"CTC loss: "</span>, loss)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line">CTC loss:  </span><br><span class="line">tensor(<span class="number">7.5385</span>, grad_fn=&lt;MeanBackward0&gt;)</span><br></pre></td></tr></table></figure></p>
<h1 id="总结：PyTorch-中的18中损失函数"><a href="#总结：PyTorch-中的18中损失函数" class="headerlink" title="总结：PyTorch 中的18中损失函数"></a>总结：PyTorch 中的18中损失函数</h1><ol>
<li>nn.CrossEntropyLoss</li>
<li>nn.NLLLoss</li>
<li>nn.BCELoss</li>
<li>nn.BCEWithLogitsLoss</li>
<li>nn.L1Loss</li>
<li>nn.MSELoss</li>
<li>nn.SmoothL1Loss</li>
<li>nn.PoissonNLLLoss</li>
<li>nn.KLDivLoss</li>
<li>nn.MarginRankingLoss</li>
<li>nn.MultiLabelMarginLoss</li>
<li>nn.SoftMarginLoss</li>
<li>nn.MultiLabelSoftMarginLoss</li>
<li>nn.MultiMarginLoss</li>
<li>nn.TripletMarginLoss</li>
<li>nn.HingeEmbeddingLoss</li>
<li>nn.CosineEmbeddingLoss</li>
<li>nn.CTCLoss</li>
</ol>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] DeepShare.net &gt; PyTorch框架</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/02/12/PyTorch%E7%AC%94%E8%AE%B0/%E3%80%90Tutorials%E3%80%91torch.nn.loss-1/" rel="prev" title="PyTorch笔记/【Tutorials】torch.nn.loss-1">
      <i class="fa fa-chevron-left"></i> PyTorch笔记/【Tutorials】torch.nn.loss-1
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/02/12/%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E4%B8%8E%E5%B7%A5%E5%85%B7/%E3%80%90Markdown%E3%80%91%E5%85%B3%E4%BA%8E%E6%8F%92%E4%BB%B6%E7%9A%84%E4%BD%BF%E7%94%A8/" rel="next" title="开发环境与工具/【Markdown】关于插件的使用">
      开发环境与工具/【Markdown】关于插件的使用 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#nn-L1Loss"><span class="nav-number">1.</span> <span class="nav-text">nn.L1Loss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nn-MSELoss"><span class="nav-number">2.</span> <span class="nav-text">nn.MSELoss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nn-SmoothL1Loss"><span class="nav-number">3.</span> <span class="nav-text">nn.SmoothL1Loss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nn-PoissonNLLLoss"><span class="nav-number">4.</span> <span class="nav-text">nn.PoissonNLLLoss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nn-KLDivLoss"><span class="nav-number">5.</span> <span class="nav-text">nn.KLDivLoss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nn-MarginRankingLoss"><span class="nav-number">6.</span> <span class="nav-text">nn.MarginRankingLoss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nn-MultiLabelMarginLoss"><span class="nav-number">7.</span> <span class="nav-text">nn.MultiLabelMarginLoss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nn-SoftMarginLoss"><span class="nav-number">8.</span> <span class="nav-text">nn.SoftMarginLoss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nn-MultiLabelSoftMarginLoss"><span class="nav-number">9.</span> <span class="nav-text">nn.MultiLabelSoftMarginLoss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nn-MultiMarginLoss"><span class="nav-number">10.</span> <span class="nav-text">nn.MultiMarginLoss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nn-TripletMarginLoss"><span class="nav-number">11.</span> <span class="nav-text">nn.TripletMarginLoss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nn-HingeEmbeddingLoss"><span class="nav-number">12.</span> <span class="nav-text">nn.HingeEmbeddingLoss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nn-CosineEmbeddingLoss"><span class="nav-number">13.</span> <span class="nav-text">nn.CosineEmbeddingLoss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nn-CTCLoss"><span class="nav-number">14.</span> <span class="nav-text">nn.CTCLoss</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#总结：PyTorch-中的18中损失函数"><span class="nav-number"></span> <span class="nav-text">总结：PyTorch 中的18中损失函数</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考文献"><span class="nav-number"></span> <span class="nav-text">参考文献</span></a></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name"></p>
  <div class="site-description" itemprop="description">其实，我是一个搬运工！</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">125</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder"></span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.7.0
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  













<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme: 'default',
      logLevel: 3,
      flowchart: { curve: 'linear' },
      gantt: { axisFormat: '%m/%d/%Y' },
      sequence: { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
      
<script type="text/x-mathjax-config">
    MathJax.Ajax.config.path['mhchem'] = '//cdn.jsdelivr.net/npm/mathjax-mhchem@3';

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        extensions: ['[mhchem]/mhchem.js'],
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

</body>
</html>
