<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://yoursite.com').hostname,
    root: '/',
    scheme: 'Gemini',
    version: '7.7.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="论文：Faster R-CNN: Towards Real-Time Object Detection with Region Proposal NetworksShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun, NIPS 2015">
<meta property="og:type" content="article">
<meta property="og:title" content="dl&#x2F;【Paper笔记 Detection】Faster R-CNN">
<meta property="og:url" content="http://yoursite.com/2018/09/14/dl/%E3%80%90Paper%E7%AC%94%E8%AE%B0%20Detection%E3%80%91Faster%20R-CNN/index.html">
<meta property="og:site_name" content="Paper搬运菌">
<meta property="og:description" content="论文：Faster R-CNN: Towards Real-Time Object Detection with Region Proposal NetworksShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun, NIPS 2015">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://github.com/JuneXia/JuneXia.github.io/raw/hexo/source/images/ml/FasterRCNN-loss1.jpg">
<meta property="og:image" content="https://github.com/JuneXia/JuneXia.github.io/raw/hexo/source/images/ml/FasterRCNN-loss2.jpg">
<meta property="article:published_time" content="2018-09-13T16:00:00.000Z">
<meta property="article:modified_time" content="2020-05-27T02:47:41.557Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/JuneXia/JuneXia.github.io/raw/hexo/source/images/ml/FasterRCNN-loss1.jpg">

<link rel="canonical" href="http://yoursite.com/2018/09/14/dl/%E3%80%90Paper%E7%AC%94%E8%AE%B0%20Detection%E3%80%91Faster%20R-CNN/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>dl/【Paper笔记 Detection】Faster R-CNN | Paper搬运菌</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Paper搬运菌</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/14/dl/%E3%80%90Paper%E7%AC%94%E8%AE%B0%20Detection%E3%80%91Faster%20R-CNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="其实，我是一个搬运工！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Paper搬运菌">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          dl/【Paper笔记 Detection】Faster R-CNN
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-09-14 00:00:00" itemprop="dateCreated datePublished" datetime="2018-09-14T00:00:00+08:00">2018-09-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-05-27 10:47:41" itemprop="dateModified" datetime="2020-05-27T10:47:41+08:00">2020-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习笔记</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>论文：<a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a><br>Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun, NIPS 2015</p>
<a id="more"></a>
<p>SSD 是 2016年发表，SSD没有全连接层，FasterRCNN有全连接层。</p>
<p>SSD和FasterRCNN在Feature map区域建议采样方式的区别：SSD是在6个feature map上用了4~6个anchor框去做采样（sliding widow依然是3x3），而FasterRCNN是在最后一个feature map上用9个anchor框去采样。所以SSD的采样框比FasterRCNN多，进而精度也比FasterRCNN高，但其抛弃了ROI pooling，所以其稳定性没有FasterRCNN高。<br>这里的精度和稳定性是这样理解的：检测某种物体很准确，但对检测另一种物体不准确；这段时间检测准确性很好，过一段时间后检测准确性又不行了。</p>
<p><strong>Abstract</strong><br>&emsp; State-of-the-art object detection networks depend on region proposal algorithms to hypothesize(假设,假定) object locations.  Advances(发展,前进,提出) like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing(expose,遗弃,陈列,揭露) region  proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image  convolutional features with the detection network, thus enabling nearly(几乎) cost-free region proposals. An RPN is a fully convolutional  network that simultaneously(同时地) predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to  generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN  into a single network by sharing their convolutional features using the recently popular terminology(专业术语,用辞) of neural networks with “attention” mechanisms(注意力机制), the RPN component tells the unified(unify:统一,使一致) network where to look. For the very deep VGG-16 model [3],  our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection  accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO  2015 competitions, Faster R-CNN and RPN are the foundations(基础,房基) of the 1st-place winning entries(entry:进入,条目,记录) in several tracks. Code has been  made publicly available.</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>&emsp; Recent advances(advance:n.发展,前进;v.提出,使…前进) in object detection are driven by the success of region proposal methods (e.g., [4]) and region-based convolutional neural networks (RCNNs) [5]. <code>Although region-based CNNs were computationally expensive as originally developed in [5], their cost has been drastically(彻底地,激烈地,大幅度地) reduced thanks to sharing convolutions across proposals [1], [2]. (虽然基于区域的CNNs的计算成本与最初在[5]中开发时一样高，但是由于proposals之间共享卷积，它们的成本已经大大降低了).</code> The latest incarnation(化身,典型), Fast R-CNN [2], achieves near real-time rates using very deep networks [3], when ignoring the time spent on region proposals. Now, proposals are the test-time computational bottleneck in state-of-the-art detection systems.</p>
<p>&emsp; Region proposal methods typically(典型地,代表性地) rely on <code>inexpensive features and economical inference schemes(廉价的特征和经济的推理方案)</code>. Selective Search [4], one of the most popular methods, <code>greedily(贪婪地) merges superpixels(超像素) based on engineered low-level features.(它贪婪地合并基于工程底层特征的超像素).</code> Yet when compared to efficient detection networks [2], <code>Selective Search is an order of magnitude slower, at 2 seconds per image in a CPU implementation. (选择性搜索要慢了一个数量级，在CPU实现中，每幅图像要慢2秒).</code> EdgeBoxes [6] currently provides the best tradeoff(权衡,折中,(公平)交易) between proposal quality and speed, at 0.2 seconds per image. Nevertheless, the region proposal step still consumes as much running time as the detection network.</p>
<p>&emsp; <code>One may note that(人们可能会注意到)</code> fast region-based CNNs take advantage of GPUs, <code>while(在…期间;与…同时;(比对两件事物)然而;虽然,尽管;直到…为止;adv.在…时候) the region proposal methods used in research are implemented on the CPU(而研究中使用的区域建议方法是在CPU上实现的)</code>, making such runtime comparisons inequitable(不公平的). An obvious(明显的,显著的) way to accelerate proposal computation is to reimplement it for the GPU. This may be an effective engineering solution, but re-implementation ignores the down-stream(下游) detection network and therefore misses important opportunities for sharing computation.</p>
<h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><h1 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h1><p>&emsp; Our object detection system, called Faster R-CNN, is  composed of two modules. The first module is a deep  fully convolutional network that proposes regions,  and the second module is the Fast R-CNN detector [2]  that uses the proposed regions. The entire system is a single, unified network for object detection (Figure 2). Using the recently popular terminology(专业术语,用辞) of neural networks with attention [31] mechanisms, the RPN module tells the Fast R-CNN module where to look. In Section 3.1 we introduce the designs and properties of the network for region proposal. In Section 3.2 we develop algorithms for training both modules with features shared. </p>
<h2 id="Region-Proposal-Networks"><a href="#Region-Proposal-Networks" class="headerlink" title="Region Proposal Networks"></a>Region Proposal Networks</h2><p>&emsp; A Region Proposal Network (RPN) takes an image  (of any size) as input and outputs a set of rectangular  object proposals, each with an objectness score. <code>We  model(模型,典范,模拟) this process with a fully convolutional network  [7], which we describe in this section. (我们用一个全卷积网络来模拟这个过程，我们将在本节中描述它).</code> Because our ultimate goal is to share computation with a Fast R-CNN  object detection network [2], we assume that both nets  share a common set of convolutional layers. In our experiments, we investigate(调查,研究) the Zeiler and Fergus model  [32] (ZF), which has 5 shareable convolutional layers  and the Simonyan and Zisserman model [3] (VGG-16),  which has 13 shareable convolutional layers.</p>
<p>&emsp; To generate region proposals, <code>we slide a small network over the convolutional feature map output by the last shared convolutional layer. (我们在最后一个共享卷积层的卷积特征图输出上滑动一个小网络)</code> This small network <code>takes as input(take…as input,以…为输入) an $n \times n$ spatial window of the input convolutional feature map. (以输入卷积特征图的一个n×n空间窗口作为输入).</code> Each sliding window is mapped(map: n.地图,示意图;v.绘制地图,映射) to a lower-dimensional feature (256-d for ZF and 512-d for VGG, with ReLU [33] following). This feature is fed into two sibling(兄弟姐妹,同级的) fullyconnected layers a box-regression layer (reg) and a box-classification layer (cls). We use n = 3 in this paper, noting that the effective receptive field on the input image is large (171 and 228 pixels for ZF and VGG, respectively). <code>This mini-network is illustrated at a single position in Figure 3 (left). (图3(左)显示了这个迷你网络的一个位置).</code> <code>Note that because the mini-network operates in a sliding-window fashion, the fully-connected layers are shared across(从…的一边到另一边,穿过,越过;在对面,横过) all spatial locations. (请注意，由于微型网络以滑动窗口的方式运行，因此所有空间位置都共享完全连接的层).</code> <code>This architecture is naturally implemented with an n n convolutional layer followed by two sibling 1 1 convolutional layers (for reg and cls, respectively). (这个架构很自然地通过一个n×n卷积层和两个同级的1×1卷积层(分别用于reg和cls)来实现).</code></p>
<h3 id="Anchors"><a href="#Anchors" class="headerlink" title="Anchors"></a>Anchors</h3><p>&emsp; At each sliding-window location, we simultaneously(同时地) predict multiple region proposals, where the number of maximum possible proposals for each location is denoted as $k$. So the reg layer has $4k$ outputs encoding the coordinates of $k$ boxes, and the $cls$ layer outputs $2k$ scores that estimate probability of object or not object for each proposal. <code>The k proposals are parameterized relative to k reference boxes, which we call anchors. (k个proposal是相对于k个参考框参数化的，我们称之为锚).</code> <code>An anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio (Figure 3, left). (锚位于所讨论的滑动窗口的中心，并与比例和高宽比相关联(图3,左))</code> By default we use 3 scales and 3 aspect ratios, yielding $k = 9$ anchors at each sliding position. For a convolutional feature map of a size $W \times H$ (typically ~2,400), there are $W \times H \times k$ anchors in total.</p>
<p><strong>Translation-Invariant Anchors</strong>(平移不变锚)<br>&emsp; An important property of our approach is that it  is translation invariant, <code>both in terms of the anchors  and the functions that compute proposals relative to(相对于,涉及) the anchors. (无论是在锚点方面，还是在计算相对于锚点的proposal functon方面)</code> <code>If one translates an object in an image,  the proposal should translate and the same function should be able to predict the proposal in either location. (如果某个时候移动了图像中的一个目标，那么他的proposal也应该被平移，同样的function应该能够预测任何位置的proposal)</code> This translation-invariant property is guaranteed by our method5. As a comparison, the MultiBox  method [27] uses k-means to generate 800 anchors,  which are not translation invariant. So MultiBox does  not guarantee that the same proposal is generated if  an object is translated.</p>
<p>&emsp; The translation-invariant property also reduces the  model size. MultiBox has a $(4 + 1) \times 800$-dimensional  fully-connected output layer, whereas our method has  a $(4 + 2) \times 9$-dimensional convolutional output layer  in the case of k = 9 anchors. As a result, our output  layer has $2.8 \times 10^4$ parameters $(512 \times (4 + 2) \times 9$ for VGG-16), <code>two orders of magnitude fewer than  MultiBox&#39;s output layer that has $6.1 \times 10^6$ parameters $(1536 \times (4 + 1) \times 800$ for GoogleNet [34] in MultiBox  [27]). (比MultiBox的输出层少两个数量级，后者有$$6.1 \times 10^6$$个参数).</code> If considering the feature projection(投影,规划) layers, our  proposal layers still have an order of magnitude fewer  parameters than MultiBox<sup><a href="#fn_1" id="reffn_1">1</a></sup>. We expect our method  to have less risk of overfitting on small datasets, like  PASCAL VOC.</p>
<blockquote>
<blockquote id="fn_1">
<sup>1</sup>. Considering the feature projection layers, our proposal layers’ parameter count is 3 × 3 × 512 × 512 + 512 × 6 × 9 = 2:4 × 106; MultiBox’s proposal layers’ parameter count is 7 × 7 × (64 + 96 + 64 + 64) × 1536 + 1536 × 5 × 800 = 27 × 106.<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a>
</blockquote>
</blockquote>
<p><strong>Multi-Scale Anchors as Regression References</strong><br>&emsp; <code>Our design of anchors presents(我们的anchors设计提出了)</code> a novel scheme  for addressing multiple scales (and aspect ratios). As  shown in Figure 1, there have been two popular ways  for multi-scale predictions. The first way is based on  image/feature pyramids, e.g., in DPM [8] and CNNbased methods [9], [1], [2]. The images are resized at  multiple scales, and feature maps (HOG [8] or deep  convolutional features [9], [1], [2]) are computed for  each scale (Figure 1(a)). This way is often useful but  is time-consuming. The second way is to use sliding  windows of multiple scales (and/or aspect ratios) on  the feature maps. For example, in DPM [8], models  of different aspect ratios are trained separately(分别地;分离地;个别地) using  different filter sizes (such as $5 \times 7$ and $7 \times 5$). If this way  is used to address multiple scales, <code>it can be thought(think:n.思考;想法;关心;v.想,思考;认为) of as a  pyramid of filters(它可以被认为是一个“过滤器的金字塔”)</code> (Figure 1(b)). The second  way is usually adopted jointly with the first way [8].</p>
<p>&emsp; As a comparison, our anchor-based method is built on a pyramid of anchors, which is more cost-efficient(经济有效的). Our method classifies and regresses bounding boxes with reference to anchor boxes of multiple scales and aspect ratios. It only relies on images and feature maps of a single scale, and uses filters (sliding windows on the feature map) of a single size. We show by experiments the effects of this scheme for addressing multiple scales and sizes (Table 8).</p>
<p>&emsp; Because of this multi-scale design based on anchors, we can simply use the convolutional features computed on a single-scale image, as is also done by the Fast R-CNN detector [2]. <code>The design of multiscale anchors is a key component for sharing features without extra cost for addressing scales. (多尺度anchors的设计是实现特征共享的关键环节，而不需要额外的处理尺度成本).</code></p>
<h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>For training RPNs, we assign a binary class label (of being an object or not) to each anchor. We assign a positive label to two kinds of anchors: (i) the anchor/anchors with the highest Intersection-overUnion (IoU) overlap with a ground-truth box, or (ii) an anchor that has an IoU overlap higher than 0.7 with any ground-truth box. Note that a single ground-truth box may assign positive labels to multiple anchors. Usually the second condition is sufficient to determine the positive samples; but we still adopt the first condition for the reason that in some rare cases the second condition may find no positive sample. We assign a negative label to a non-positive anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes. Anchors that are neither positive nor negative do not contribute to the training objective. </p>
<p>&emsp; With these definitions, we minimize an objective function following the multi-task loss in Fast R-CNN [2]. Our loss function for an image is defined as:</p>
<div align=center>
  <img src="https://github.com/JuneXia/JuneXia.github.io/raw/hexo/source/images/ml/FasterRCNN-loss1.jpg" width = 80% height = 80% />
</div>

<p>Here, $i$ is the index of an anchor in a mini-batch and $p_i$ is the predicted probability of anchor $i$ being an object. The ground-truth label $p_i^<em>$ is 1 if the anchor is positive, and is 0 if the anchor is negative. $t_i$ is a vector representing the 4 parameterized coordinates of the predicted bounding box, and $t_i^</em>$ is that of the ground-truth box associated with a positive anchor. The classification loss $L_{cls}$ is log loss over two classes (object $vs$. not object). For the regression loss, we use $L_{reg}(t_i, t_i^<em>) = R(t_i - t_i^</em>)$ where $R$ is the robust loss function (smooth L1) defined in [2]. The term $p_i^<em> L_{reg}$ means the regression loss is activated only for positive anchors ($p_i^</em> = 1)$ and is disabled otherwise ($p_i^* = 0$). The outputs of the $cls$ and $reg$ layers consist of ${p_i}$ and ${t_i}$ respectively.</p>
<p>&emsp; The two terms are normalized by $N_{cls}$ and $N_{reg}$ and weighted by a balancing parameter $λ$. In our current implementation (as in the released code), the $cls$ term in Eqn.(1) is normalized by the mini-batch size (i.e., $N_{cls} = 256$) and the $reg$ term is normalized by the number of anchor locations (i.e., $N_{reg} \sim 2400$). By default we set $λ = 10$, and thus both $cls$ and $reg$ terms are roughly equally weighted. We show by experiments that the results are insensitive to the values of $λ$ in a wide range (Table 9). We also note that the normalization as above is not required and could be simplified.</p>
<p>&emsp; For bounding box regression, we adopt the parameterizations of the 4 coordinates following [5]:</p>
<div align=center>
  <img src="https://github.com/JuneXia/JuneXia.github.io/raw/hexo/source/images/ml/FasterRCNN-loss2.jpg" width = 80% height = 80% />
</div>

<p>where $x, y, w$, and $h$ denote the box’s center coordinates and its width and height. Variables $x, x_a$, and $x$ are for the predicted box, anchor box, and groundtruth box respectively (likewise for $y, w, h$). This can be thought of as bounding-box regression from an anchor box to a nearby ground-truth box.</p>
<blockquote>
<p>关于公示(2)的一些理解可以参考文献 [1]</p>
</blockquote>
<p>&emsp; Nevertheless, our method achieves bounding-box regression by a different manner(n.方式;习惯;种类;规矩;风俗) from previous RoIbased (Region of Interest) methods [1], [2]. In [1], [2], bounding-box regression is performed on features pooled from <em>arbitrarily</em>(adv.任意地;反复无常地;专横地) sized RoIs, and the regression weights are <em>shared</em> by all region sizes. In our formulation, the features used for regression are of the same spatial size (3 × 3) on the feature maps. To account for varying sizes, a set of k bounding-box regressors(n.[数]回归量) are learned. Each regressor is responsible for one scale and one aspect ratio, and the k regressors do not share weights. As such, it is still possible to predict boxes of various sizes even though the features are of a fixed size/scale, thanks to the design of anchors.</p>
<h3 id="Training-RPNs"><a href="#Training-RPNs" class="headerlink" title="Training RPNs"></a>Training RPNs</h3><p>The RPN can be trained end-to-end by backpropagation and stochastic gradient descent (SGD) [35]. We follow the image-centric sampling strategy from [2] to train this network. <strong>Each mini-batch <code>arises from(来自)</code> a single image that contains many positive and negative example anchors.</strong> It is possible to optimize for the loss functions of all anchors, but this will bias towards negative samples as they are dominate. Instead, we randomly sample 256 anchors in an image to compute the loss function of a mini-batch, where the sampled positive and negative anchors have a ratio of up to 1:1. If there are fewer than 128 positive samples in an image, we pad the mini-batch with negative ones.</p>
<p>&emsp; We randomly initialize all new layers by drawing weights from a zero-mean Gaussian distribution with standard deviation 0.01. All other layers (i.e., the shared convolutional layers) are initialized by pretraining a model for ImageNet classification [36], as is standard practice [5]. We tune all layers of the ZF net, and conv3 1 and up for the VGG net to conserve memory [2]. We use a learning rate of 0.001 for 60k mini-batches, and 0.0001 for the next 20k mini-batches on the PASCAL VOC dataset. We use a momentum of 0.9 and a weight decay of 0.0005 [37]. Our implementation uses Caffe [38].</p>
<h2 id="Sharing-Features-for-RPN-and-Fast-R-CNN"><a href="#Sharing-Features-for-RPN-and-Fast-R-CNN" class="headerlink" title="Sharing Features for RPN and Fast R-CNN"></a>Sharing Features for RPN and Fast R-CNN</h2><p><code>Thus far(迄今为止)</code> we have described how to train a network for region proposal generation, without considering the region-based object detection CNN that will utilize these proposals. For the detection network, we adopt Fast R-CNN [2]. Next we describe algorithms that learn a unified network composed of RPN and Fast R-CNN with shared convolutional layers (Figure 2).</p>
<p>&emsp; Both RPN and Fast R-CNN, trained independently, will modify their convolutional layers in different ways. We therefore need to develop a technique that <strong>allows for sharing convolutional layers between the two networks, rather than learning two separate networks</strong>. We discuss three ways for training networks with features shared:</p>
<p>(i) <em>Alternating(adj.交替的;交互的) training</em>. In this solution, we first train RPN, and use the proposals to train Fast R-CNN. The network tuned by Fast R-CNN is then used to initialize RPN, and this process is iterated(iterate vt.迭代;重复). This is the solution that is used in all experiments in this paper.</p>
<p>(ii) <em>Approximate(n/adj.近似(的),大概(的)) joint training</em>. In this solution, the RPN and Fast R-CNN networks are merged into one network during training as in Figure 2. In each SGD iteration, the forward pass generates region proposals which are treated just like fixed, pre-computed proposals when training a Fast R-CNN detector. The backward propagation <code>takes place as usual(一切如常,照常进行)</code>, where for the shared layers the backward propagated signals from both the RPN loss and the Fast R-CNN loss are combined. This solution is easy to implement. But this solution ignores the derivative(n.派生物;金融衍生产品;派生词;(化学)衍生物;导数) w.r.t. the proposal boxes coordinates that are also network responses, so is approximate. In our experiments, we have empirically(adv.以经验为主地) found this solver produces close(接近的) results, yet reduces the training time by about 25-50% comparing with alternating training. This solver is included in our released Python code.</p>
<p>未完待续。。。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] <a href="https://blog.csdn.net/Blateyang/article/details/84800007" target="_blank" rel="noopener">关于RPN中proposal的坐标回归参数的一点理解及Faster R-CNN的学习资料</a></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2018/08/25/%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E4%B8%8E%E5%B7%A5%E5%85%B7/%E3%80%90%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E4%B8%8E%E5%B7%A5%E5%85%B7%E3%80%91Ubuntu16.04+RTX750+CUDA10.0/" rel="prev" title="【开发环境与工具】Ubuntu16.04+RTX750+CUDA10.0">
      <i class="fa fa-chevron-left"></i> 【开发环境与工具】Ubuntu16.04+RTX750+CUDA10.0
    </a></div>
      <div class="post-nav-item">
    <a href="/2018/09/16/dl/%E3%80%90Paper%E7%AC%94%E8%AE%B0%20Detection%E3%80%91FPN/" rel="next" title="dl/【Paper笔记 Detection】FPN">
      dl/【Paper笔记 Detection】FPN <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Related-Work"><span class="nav-number">2.</span> <span class="nav-text">Related Work</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Faster-R-CNN"><span class="nav-number">3.</span> <span class="nav-text">Faster R-CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Region-Proposal-Networks"><span class="nav-number">3.1.</span> <span class="nav-text">Region Proposal Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Anchors"><span class="nav-number">3.1.1.</span> <span class="nav-text">Anchors</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Loss-Function"><span class="nav-number">3.1.2.</span> <span class="nav-text">Loss Function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Training-RPNs"><span class="nav-number">3.1.3.</span> <span class="nav-text">Training RPNs</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sharing-Features-for-RPN-and-Fast-R-CNN"><span class="nav-number">3.2.</span> <span class="nav-text">Sharing Features for RPN and Fast R-CNN</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考文献"><span class="nav-number">4.</span> <span class="nav-text">参考文献</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name"></p>
  <div class="site-description" itemprop="description">其实，我是一个搬运工！</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">141</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder"></span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.7.0
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  













<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme: 'default',
      logLevel: 3,
      flowchart: { curve: 'linear' },
      gantt: { axisFormat: '%m/%d/%Y' },
      sequence: { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
      
<script type="text/x-mathjax-config">
    MathJax.Ajax.config.path['mhchem'] = '//cdn.jsdelivr.net/npm/mathjax-mhchem@3';

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        extensions: ['[mhchem]/mhchem.js'],
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

</body>
</html>
