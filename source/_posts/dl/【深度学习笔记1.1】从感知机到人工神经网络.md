---
title: 【深度学习笔记1.1】从感知机到人工神经网络
date: 2017-08-02 17:28:05
tags:
categories: ["深度学习笔记"]
mathjax: true
---

## 线性阈值单元
线性阈值单元（LTU）：输入和输出是数字（而不是二进制开/关值），并且每个输入连接都与权重相连。LTU计算其输入的加权和（$z = w_1 x_1 + w_2 x_2 + ... + w_n x_n = \boldsymbol{w}^T·\boldsymbol{x}$），然后将阶跃函数应用于该和，并输出结果：$H_W(x) = STEP(Z) = STEP(\boldsymbol{w}^T·\boldsymbol{x})$ [1]。<!-- more -->
单一的 LTU 可被用作简单线性二元分类[2]。\
![enter image description here](https://lh3.googleusercontent.com/-CuEuksDFuNg/W8WSCCecggI/AAAAAAAAAFs/i9jXEV-p4holbZpK7D-B23LCk5kE2UWKwCLcBGAs/s0/perceptron1.png "perceptron1.png")

### 代码示例1
sklearn 提供了一个感知器类，它实现了一个 LTU 网络 [2]。
```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.linear_model import Perceptron
iris = load_iris()
X = iris.data[:, (2, 3)]  # 花瓣长度，宽度
y = (iris.target == 0).astype(np.int)
per_clf = Perceptron(random_state=42)
per_clf.fit(X, y)
y_pred = per_clf.predict([[2, 0.5]])
print(y_pred)
```

## 感知器
感知器(Perceptron)，也可翻译为感知机，是 Frank Rosenblatt 在1957年就职于Cornell航空实验室(Cornell Aeronautical Laboratory)时所发明的一种人工神经网络[1]。感知器简单地由一层 LTU 组成，每个神经元连接到所有输入[2]。

![enter image description here](https://lh3.googleusercontent.com/-4GQY-k1m-04/W8WTNXDTjCI/AAAAAAAAAF8/i6_ZHYxsHqEz3c0c9vePph0Sq1B_Pug4wCLcBGAs/s0/perceptron2.png "perceptron2.png")


&emsp; 阶跃函数和符号函数在 $z = 0$ 处是不连续的,其他位置导数为 0,无法利用梯度下降算法进行参数优化。为了能够让感知机模型能够从数据中间自动学习,Frank Rosenblatt 提出了感知机的学习算法。

>**感知机训练算法** [3]\
初始化参数 $\boldsymbol{w} = 0, b = 0$ \
**repeat** \
&emsp; 从训练集随机采样一个样本($x_i , y_i$) \
&emsp; 计算感知机的输出 $a = sign(\boldsymbol{w} w^T x_i + b)$ \
&emsp; 如果 $a ≠ y_i$ : \
&emsp; &emsp; $w′ ← w + η ∙ y_i ∙ x_i$ \
&emsp; &emsp; $b′ ← b + η ∙ y_i$ \
**until** 训练次数达到要求 \
输出: 分类网络参数$\boldsymbol{w}$和$b$ \

其中η为学习率。虽然感知机提出之处被寄予了良好的发展潜力,但是 Marvin Lee Minsky 和 Seymour Papert 于 1969 年在《Perceptrons》书中证明了以感知机为代表的线性模型不能解决异或(XOR)等线性不可分问题,这直接导致了当时新兴的神经网络的研究进入了低谷期。尽管感知机模型不能解决线性不可分问题,但书中也提到通过嵌套多层神经网络可以解决。


## 多层感知器
事实证明，感知器的一些局限性可以通过堆叠多个感知器来消除。由此产生的人工神经网络被称为多层感知器（MLP）[2]。

![enter image description here](https://lh3.googleusercontent.com/-BWgLPEfUJhs/W8WVCsijs4I/AAAAAAAAAGM/15Y9xMZ-OBQkJ6mIFzk97ObbgLsdWZ__ACLcBGAs/s0/perceptron3.png "perceptron3.png")

MLP 由一个（通过）输入层、一个或多个称为隐藏层的 LTU 组成，一个最终层 LTU 称为输出层（见图 10-7）。除了输出层之外的每一层包括偏置神经元，并且全连接到下一层。当人工神经网络有两个或多个隐含层时，称为深度神经网络（DNN）。

### 代码示例2
```python
import tensorflow as tf
from sklearn.datasets import load_iris
from sklearn.metrics import accuracy_score

iris = load_iris()
X_train = iris.data
y_train = iris.target
feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(X_train)
dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[300, 100], n_classes=3, feature_columns=feature_columns)
dnn_clf.fit(x=X_train, y=y_train, batch_size=50, steps=40)

y_predicted = list(dnn_clf.predict(X_train))
print(accuracy_score(y_train, y_predicted))
print(dnn_clf.evaluate(X_train, y_train))

print('end')
```


## 参考文献
[1] [百度百科：感知器（神经网络模型）](https://baike.baidu.com/item/%E6%84%9F%E7%9F%A5%E5%99%A8/16525448?fr=aladdin) \
[2] [hands_on_Ml_with_Sklearn_and_TF.第9章.人工神经网络简介](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/docs/10.%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D.md) \
[3] TensorFlow深度学习.龙良曲>6.1感知机


