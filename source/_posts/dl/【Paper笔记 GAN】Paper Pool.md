---
title: 
date: 2020-7-25
tags:
categories: ["æ·±åº¦å­¦ä¹ ç¬”è®°"]
mathjax: true
---

<!--more-->

Assessing Eye Aesthetics for Automatic Multi-Reference Eye In-Painting

Bo Yanâˆ—, Qing Lin, Weimin Tan, Shili Zhou
Shanghai Key Laboratory of Intelligent Information Processing,
School of Computer Science, Fudan University
{byan, 18210240028, wmtan14, 15307130270}@fudan.edu.cn

CVPR 2020ï¼Œå°†ç¾å­¦ä¿¡æ¯å¼•å…¥åˆ°å›¾åƒä¿®å¤ä¸­


![](../../images/ml/AesGAN-fig3.jpg) \
Figure 3. The labeled eye aesthetics assessment dataset according to manual scoring. The dataset has a total of 1,040 eye images divided into two categories. The first line shows low-quality eye images, and the second line shows high-quality ones.
> åˆ¶ä½œäº†ä¸€ä¸ªäººçœ¼ç¾å­¦æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒ **Eye Aesthetic Assessment Network** . (åº”è¯¥æ˜¯äº‹å…ˆè®­ç»ƒå¥½ï¼Œåœ¨åé¢è®­ç»ƒAesGANçš„æ—¶å€™æ˜¯ç›´æ¥æ‹¿æ¥ç”¨çš„ã€‚)


![](../../images/ml/AesGAN-fig4.jpg) \
Figure 4. The architecture of our eye aesthetic assessment network. We first introduce the reconstruction branch into the image quality assessment task to maintain the uniqueness of eye aesthetic. Only the eye aesthetic feature extraction module and the eye scoring module are needed during testing.
> ä¸Šå›¾å³æ˜¯ **Eye Aesthetic Assessment Network**


![](../../images/ml/AesGAN-fig5.jpg) \
Figure 5. The architecture of our eye in-painting network (AesGAN) based on eye aesthetic and face semantic, containing a generator, two discriminators, an eye aesthetic assessment network and a parsing network. The function f(O,R) is the eye aesthetic feature extraction module in Figure 4.
> Step1: äº‹å…ˆè®­ç»ƒå¥½ Eye Aesthetic Assessment Network
> 
> Step2: äº‹å…ˆè®­ç»ƒå¥½ Face Semantic Network
> 
> Step3: è®­ç»ƒ AesGAN \
> Step3.1 ä» Multi-Reference ä¸­ æŒ‘é€‰ Reference(R) \
> &emsp; äº‹å…ˆå‡†å¤‡å¥½ä¸€ç»„ Multi-Referenceï¼Œè¿™ç»„ Reference åº”è¯¥éƒ½æ˜¯å¾ˆç¾çš„ğŸ¶ï¼Œä»–ä»¬è™½ç„¶æ‹¥æœ‰ä¸åŒçš„è„¸å‹ï¼Œä½†éƒ½å…·æœ‰ç¾ä¸½çš„çœ¼ç›ã€‚\
> æˆ‘ä»¬å°†è¿™ç»„ Multi-Reference è¾“å…¥åˆ°äº‹å…ˆè®­ç»ƒå¥½çš„ Eye Aesthetic Assessment Network è¿›è¡Œæ‰“åˆ†ï¼Œç„¶åå†ä½¿ç”¨ SSIM ä»ä¸­æŒ‘å‡ºè„¸å‹å’Œè¾“å…¥çš„ Incomplete-Image æœ€ç›¸ä¼¼çš„ä½œä¸ºæœ¬æ¬¡çš„ Reference(R).
> 
> Step3.2: Generator å‰å‘è®¡ç®— \
> &emsp; Incomplete-Image å’Œ Reference(R) ä¸€èµ·è¢«è¾“å…¥åˆ° Generatorï¼Œç”Ÿæˆä¸€å¼  Inpainted-Image(O)ï¼ˆä¹Ÿå°±æ˜¯fake-imageï¼‰
> 
> Step3.3: Discriminate å’Œ loss è®¡ç®— \
> loss_GAN(O, GT) \
> loss_Recon(?, ?)   # TODO: è¿™ä¸ªlosså¯å‚è€ƒ ExGAN \
> loss_Aesth(O, R) \
> loss_Parsing(O, I)  # I è¡¨ç¤ºåŸå›¾ï¼Œå³ Incomplete-Image \
> æ€»losså°±æ˜¯ä¸Šè¿°lossåŠ æƒæ±‚å’Œã€‚


----------------------------------------

[Soft-Gated Warping-GAN for Pose-Guided Person Image Synthesis](https://papers.nips.cc/paper/7329-soft-gated-warping-gan-for-pose-guided-person-image-synthesis.pdf)


è™šæ‹Ÿæ¢è£…: [VITON: An Image-based Virtual Try-on Network](https://arxiv.org/pdf/1711.08447.pdf)


[Object-driven Text-to-Image Synthesis via Adversarial Training](https://arxiv.org/abs/1902.10740)


[Deep Neural Network Augmentation: Generating Faces for Affect Analysis](https://link.springer.com/article/10.1007/s11263-020-01304-3)


[ClothFlow: A Flow-Based Model for Clothed Person Generation](https://openaccess.thecvf.com/content_ICCV_2019/papers/Han_ClothFlow_A_Flow-Based_Model_for_Clothed_Person_Generation_ICCV_2019_paper.pdf)


StyleFlow





