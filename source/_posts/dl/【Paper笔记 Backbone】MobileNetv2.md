---
title: 
date: 2020-04-1
tags:
categories: ["æ·±åº¦å­¦ä¹ ç¬”è®°"]
mathjax: true
---
[MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) \
Mark Sandler Andrew Howard Menglong Zhu Andrey Zhmoginov Liang-Chieh Chen \
Google Inc. \
{sandler, howarda, menglong, azhmogin, lcchen}@google.com

ä»£ç å·²è´´æ³¨é‡Šï¼Œè®ºæ–‡æ•´ç†æœªå®Œå¾…ç»­ã€‚
<!-- more -->

**Abstract**
&emsp; In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. 
&emsp; is based on an **inverted residual structure**(å€’æ®‹å·®ç»“æ„) where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. 
&emsp; Finally, our approach allows decoupling(decoupleå»è€¦åˆ;ä½¿åˆ†ç¦») of the input/output domains from the expressiveness(n. å–„äºè¡¨ç°;è¡¨æƒ…ä¸°å¯Œ;è¡¨ç°) of the transformation, which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classification, COCO object detection [2], VOC image segmentation [3]. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.

# Introduction
&emsp; Neural networks have revolutionized many areas of machine intelligence, enabling superhuman accuracy for challenging image recognition tasks. However, the drive to improve accuracy often comes at a cost: modern state of the art networks require high computational resources beyond the capabilities of many mobile and embedded applications.

&emsp; This paper introduces a new neural network architecture that is specifically tailored(tailor v.ä¸“é—¨åˆ¶ä½œ,å®šåˆ¶;(è£ç¼)åº¦èº«ç¼åˆ¶(è¡£æœ);ä½¿é€‚åº”,è¿åˆ) for mobile and resource constrained environments. Our network pushes the state of the art for mobile tailored computer vision models, by significantly decreasing the number of operations and memory needed while retaining the same accuracy. 

&emsp; Our main contribution is a novel layer module: the **inverted residual with linear bottleneck**. 
This module takes as an input a low-dimensional compressed representation which is first expanded to high dimension and filtered with a lightweight depthwise convolution. 
è¯¥æ¨¡å—é‡‡ç”¨ä½ç»´å‹ç¼©representationä½œä¸ºè¾“å…¥ï¼Œè¯¥representationé¦–å…ˆä¼šè¢«æ‰©å±•åˆ°é«˜ç»´ï¼Œç„¶åç”¨è½»é‡çº§çš„depthwise convolutionè¿›è¡Œæ»¤æ³¢ã€‚
Features are subsequently projected back to a low-dimensional representation with a linear convolution. The official implementation is available as part of TensorFlow-Slim model library in [4]. 

&emsp; This module can be efficiently implemented using standard operations in any modern framework and allows our models to beat(n.æ‹å­;æ•²å‡»;vt.æ‰“è´¥;æ…æ‹Œ;adj.ç­‹ç–²åŠ›å°½çš„;ç–²æƒ«ä¸å ªçš„) state of the art along multiple performance points using standard benchmarks. 
è¿™ä¸ªæ¨¡å—å¯ä»¥åœ¨ä»»ä½•ç°ä»£æ¡†æ¶ä¸­ä½¿ç”¨æ ‡å‡†æ“ä½œæ¥æœ‰æ•ˆåœ°å®ç°ï¼Œå¹¶ä¸”ä½¿ç”¨æ ‡å‡†benchmarksï¼Œå¯ä»¥è®©æˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªæ€§èƒ½ç‚¹ä¸Šè¶…è¶Šå½“å‰æŠ€æœ¯æ°´å¹³ã€‚
Furthermore, this convolutional module is particularly suitable for mobile designs, because it allows to significantly reduce the memory footprint needed during inference by `never fully materializing large intermediate tensors. (ä¸å®Œå…¨å®ç°å¤§å‹ä¸­é—´å¼ é‡)`. 
This reduces the need for `main memory(ä¸»å­˜)` access in many embedded hardware designs, that provide small amounts of very fast software controlled cache memory.

# Related Work
&emsp; Tuning(n. è°ƒéŸ³;éŸ³è°ƒ;(ç”µå­æˆ–æ”¶éŸ³æœº)è°ƒè°;åè°ƒä¸€è‡´) deep neural architectures to strike(v.æ’å‡»;æ‰“;è¡Œè¿›;è¾¾åˆ°(å¹³è¡¡)) an optimal balance between accuracy and performance `has been an area of active research(ç§¯æç ”ç©¶çš„ä¸€ä¸ªé¢†åŸŸ)` for the last several years. 
è°ƒæ•´æ·±åº¦ç¥ç»ç»“æ„ä»¥åœ¨ç²¾åº¦å’Œæ€§èƒ½ä¹‹é—´å–å¾—æœ€ä½³å¹³è¡¡ï¼Œæ˜¯è¿‡å»å‡ å¹´ç§¯æç ”ç©¶çš„ä¸€ä¸ªé¢†åŸŸã€‚
Both manual architecture search and improvements in training algorithms, `carried out(å®æ–½,è´¯å½»)` by numerous teams has lead to dramatic improvements over early designs such as AlexNet [5], VGGNet [6], GoogLeNet [7], and ResNet [8]. 
æ‰‹å·¥æ¶æ„æœç´¢ä»¥åŠå¤§é‡å›¢é˜Ÿå¯¹ä¸€äº›è®­ç»ƒç®—æ³•çš„æ”¹è¿›ï¼Œå¯¼è‡´äº†å¯¹æ—©æœŸç®—æ³•çš„å·¨å¤§æ”¹è¿›ï¼Œ
Recently there has been lots of progress in algorithmic architecture exploration included hyperparameter optimization [9, 10, 11] as well as various methods of network pruning(prune n.ä¿®å‰ª;å‰ªæ) [12, 13, 14, 15, 16, 17] and connectivity learning [18, 19]. \
A substantial(n. æœ¬è´¨;é‡è¦ææ–™;adj.å¤§é‡çš„;å®è´¨çš„) amount of work has also been dedicated(dedicate v. è‡´åŠ›;çŒ®èº«;é¢˜çŒ®;æŠŠâ€¦ç”¨äº) to changing the connectivity structure of the internal convolutional blocks such as in ShuffleNet [20] or introducing sparsity [21] and others [22].
å¤§é‡å·¥ä½œè¿˜è‡´åŠ›äºæ”¹å˜å†…éƒ¨å·ç§¯å—çš„connectivityç»“æ„ï¼Œå¦‚ShuffleNetæˆ–å¼•å…¥ç¨€ç–æ€§å’Œå…¶ä»–ã€‚

&emsp; Recently, [23, 24, 25, 26], opened up a new direction of bringing optimization methods including genetic algorithms and reinforcement learning to architectural search. 
è¿‘å¹´æ¥ï¼Œ[23,24,25,26]å¼€è¾Ÿäº†ä¸€ä¸ªæ–°æ–¹å‘ï¼ŒåŒ…æ‹¬å°†é—ä¼ ç®—æ³•ã€å¼ºåŒ–å­¦ä¹ ç­‰ä¼˜åŒ–æ–¹æ³•å¼•å…¥åˆ°æ¶æ„æœç´¢ã€‚
However one drawback is that the resulting networks end up very complex. In this paper, we pursue the goal of developing better intuition(n.ç›´è§‰;ç›´è§‰åŠ›;ç›´è§‰çš„çŸ¥è¯†) about how neural networks operate and use that to guide the simplest possible network design. Our approach should be seen as complimentary(adj.è¡¥å……;èµ é€çš„;ç§°èµçš„;é—®å€™çš„) to the one described in [23] and related work. 
æˆ‘ä»¬çš„æ–¹æ³•åº”è¯¥è¢«çœ‹ä½œæ˜¯å¯¹[23]å’Œç›¸å…³å·¥ä½œä¸­æ‰€æè¿°çš„æ–¹æ³•çš„è¡¥å……ã€‚
`In this vein(åœ¨è¿™æ–¹é¢)` our approach is similar to those taken by [20, 22] and allows to further improve the performance, while providing a glimpse(n.ä¸€ç¥,ä¸€çœ‹; vt.ç¥è§) on its internal operation. 
åœ¨è¿™æ–¹é¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç±»ä¼¼äº[20,22]æ‰€é‡‡å–çš„æ–¹æ³•ï¼Œå¹¶å…è®¸è¿›ä¸€æ­¥æ”¹è¿›æ€§èƒ½ï¼ŒåŒæ—¶æä¾›å¯¹å…¶å†…éƒ¨æ“ä½œçš„ä¸€ç¥ã€‚
Our network design is based on MobileNetV1 [27]. It retains its simplicity and does not require any special operators while significantly improves its accuracy, achieving state of the art on multiple image classification and detection tasks for mobile applications.

# Preliminaries, discussion and intuition
## Depthwise Separable Convolutions
&emsp; Depthwise Separable Convolutions are a key building block for many efficient neural network architectures [27, 28, 20] and we use them in the present work as well. The basic idea is to replace a full convolutional operator with a factorized(factorize vt.å› å¼åˆ†è§£) version that splits convolution into two separate layers. The first layer is called a **depthwise convolution**, it performs lightweight filtering by applying a single convolutional filter per input channel. The second layer is a $1 \times 1$ convolution, called a **pointwise convolution**, which is responsible for building new features through computing linear combinations of the input channels.
&emsp; Standard convolution takes an $h_i Ã— w_i Ã— d_i$ input tensor $L_i$, and applies convolutional kernel $K \in R^{kÃ—kÃ—d_iÃ—d_j}$ to produce an $h_i Ã— w_i Ã— d_j$ output tensor $L_j$. Standard convolutional layers have the computational cost of $h_i Â· w_i Â· d_i Â· d_j Â· k Â· k$.

&emsp; Depthwise separable convolutions are a drop-in replacement for standard convolutional layers. Empirically they work almost as well as regular convolutions but only cost:
$$
h_i \cdot w_i \cdot d_i (k^2 + d_j)  \tag{1}
$$
è¿™ä¸ªå¼å­çš„ç”±æ¥ï¼ˆæˆ‘çš„ç†è§£ï¼‰ï¼š
$h_i \cdot w_i \cdot d_i \cdot k \cdot k + h_i \cdot w_i \cdot d_i \cdot 1 \cdot 1 \cdot d_j$ï¼Œå‰é¢æ˜¯depthwiseå·ç§¯çš„è®¡ç®—é‡ï¼Œåé¢æ˜¯pointwiseå·ç§¯çš„è®¡ç®—é‡ã€‚

&emsp; which is the sum of the depthwise and $1 Ã— 1$ pointwise convolutions. Effectively depthwise separable convolution reduces computation compared to traditional layers by almost a factor of $k^2$ (more precisely(adv.ç²¾ç¡®åœ°;æ°æ°), by a factor $k^2d_j/(k^2 + d_j)$). 

è¿™ä¸ªç»“è®ºçš„ç®€å•æ¨ç†å¦‚ä¸‹ï¼š
$$
\begin{aligned}
    \frac{h_i \cdot w_i \cdot d_i (k^2 + d_j)} {h_i Â· w_i Â· d_i Â· d_j Â· k Â· k} = \frac{k^2 \cdot d_j} {k^2 + d_j}
\end{aligned}
$$
å‡è®¾å·ç§¯æ ¸ $k$ è®¾ç½®ä¸º3ï¼Œ$d_j = 100$ï¼Œåˆ™ä¸Šå¼ç­‰äºï¼š$\frac{9 \cdot 100} {9 + 100} \approx \frac{900}{100} = 9 = 3^2$ \

MobileNetV2 uses $k = 3$ ($3 \times 3$ depthwise separable convolutions) so the computational cost is 8 to 9 times smaller than that of standard convolutions at only a small reduction in accuracy [27].

## Linear Bottlenecks
&emsp; Consider a deep neural network consisting of $n$ layers $L_i$ each of which has an activation tensor of dimensions $h_i \times w_i \times d_i$. Throughout this section we will be discussing the basic properties of these activation tensors, which we will treat as containers of $h_i \times w_i$ pixels with $d_i$ dimensions. Informally(adv. éæ­£å¼åœ°;ä¸æ‹˜ç¤¼èŠ‚åœ°;é€šä¿—åœ°), for an input set of real images, we say that the set of layer activations (for any layer $L_i$) forms a â€œmanifold of interestâ€. 
> manifold \
> adv. éå¸¸å¤š
> n. ï¼ˆæ±½è½¦å¼•æ“ç”¨ä»¥è¿›æ°”å’Œæ’æ°”ï¼‰æ­§ç®¡ï¼Œå¤šæ”¯ç®¡ï¼›æœ‰å¤šç§å½¢å¼ä¹‹ç‰©ï¼›æµå½¢
> adj. å¤šç§å¤šæ ·çš„ï¼Œè®¸å¤šç§ç±»çš„
> v. å¤å°ï¼›ä½¿â€¦â€¦å¤šæ ·åŒ–

It has been long assumed that manifolds of interest in neural networks could be embedded in low-dimensional subspaces. 
é•¿æœŸä»¥æ¥ï¼Œäººä»¬ä¸€ç›´è®¤ä¸ºå¯¹ç¥ç»ç½‘ç»œæ„Ÿå…´è¶£çš„æµå½¢å¯ä»¥åµŒå…¥åˆ°ä½ç»´å­ç©ºé—´ä¸­ã€‚
In other words, when we look at all individual d-channel pixels of a deep convolutional layer, `the information encoded in those values actually lie in some manifold(è¿™äº›å€¼ä¸­ç¼–ç çš„ä¿¡æ¯å®é™…ä¸Šæ˜¯ä¸€äº›æµå½¢)`, which in turn is embeddable into a low-dimensional subspace.
> Note that dimensionality of the manifold differs from the dimensionality of a subspace that could be embedded via a linear transformation.

&emsp; At a first glance(v.ç¥é—ª,ç¥è§,æ‰«è§†,åŒ†åŒ†ä¸€çœ‹;æµè§ˆ), such a fact could then be captured and exploited(exploit vt. å¼€å‘,å¼€æ‹“;å‰¥å‰Š;å¼€é‡‡;n.å‹‹ç»©;åŠŸç»©) by simply reducing the dimensionality of a layer thus reducing the dimensionality of the operating space. 
ä¹ä¸€çœ‹ï¼Œè¿™æ ·ä¸€ä¸ªäº‹å®å¯ä»¥é€šè¿‡ç®€å•åœ°å‡å°‘ä¸€ä¸ªå±‚çš„ç»´åº¦æ¥æ•è·å’Œåˆ©ç”¨ï¼Œä»è€Œå‡å°‘æ“ä½œç©ºé—´çš„ç»´åº¦ã€‚

`This has been successfully exploited by MobileNetV1`[27] to effectively trade off between computation and accuracy via a width multiplier parameter, and has been incorporated into efficient model designs of other networks as well [20]. 
`MobileNetV1 å·²ç»æˆåŠŸåœ°åˆ©ç”¨è¿™ä¸€ç‚¹`ï¼Œé€šè¿‡å®½åº¦ä¹˜å­å‚æ•°æœ‰æ•ˆåœ°åœ¨computationå’Œaccuracyä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œå¹¶ä¸”è¿™ä¸€æ–¹æ³•å·²è¢«çº³å…¥åˆ°å…¶ä»–ç½‘ç»œä»¥åŠ[20]çš„é«˜æ•ˆæ¨¡å‹è®¾è®¡ä¸­ã€‚

Following that intuition(n.ç›´è§‰), the width multiplier approach allows one to reduce the dimensionality of the activation space until the manifold of interest spans this entire(adj.å…¨éƒ¨çš„;å…¨ä½“çš„) space. 
æ ¹æ®è¿™ç§ç›´è§‰ï¼Œå®½åº¦ä¹˜å­æ–¹æ³•å…è®¸æˆ‘ä»¬é™ä½æ¿€æ´»ç©ºé—´çš„ç»´åº¦ï¼Œç›´åˆ°interest manifoldè·¨è¶Šæ•´ä¸ªç©ºé—´ã€‚

However, this intuition breaks down when we recall that deep convolutional neural networks actually have non-linear per coordinate transformations, such as ReLU. 
ç„¶è€Œï¼Œå½“æˆ‘ä»¬å›æƒ³èµ·æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œå®é™…ä¸Šå…·æœ‰æ¯ä¸ªåæ ‡çš„éçº¿æ€§è½¬æ¢(å¦‚ReLU)æ—¶ï¼Œè¿™ç§ç›´è§‰å°±å¤±æ•ˆäº†ã€‚

For example, ReLU applied to a line in 1D space produces a 'ray', where as in $R^n$ space, it generally results in a `piece-wise(adj.[æ•°]åˆ†æ®µçš„;adv.åˆ†æ®µåœ°)` linear curve with $n$-joints.
ä¾‹å¦‚ï¼ŒReLUåº”ç”¨åˆ°ä¸€ç»´ç©ºé—´ä¸­çš„ä¸€æ¡ç›´çº¿ä¸Šä¼šäº§ç”Ÿä¸€æ¡å°„çº¿ï¼Œè€Œåœ¨$R^n$ç©ºé—´ä¸­ï¼Œå®ƒé€šå¸¸ä¼šäº§ç”Ÿå…·æœ‰$n$ä¸ªå…³èŠ‚çš„åˆ†æ®µçº¿æ€§æ›²çº¿ã€‚

&emsp; It is easy to see that in general if a result of a layer transformation ReLU($Bx$) has a non-zero volume $S$, the points mapped to interior $S$ are obtained via a linear transformation $B$ of the input, thus indicating that the part of the input space corresponding to the full dimensional output, is limited to a linear transformation. 
é€šå¸¸å¾ˆå®¹æ˜“çœ‹åˆ°ï¼Œä¸€ä¸ªlayerå˜æ¢ReLU($Bx$)çš„ç»“æœæ˜¯æœ‰ä¸€ä¸ªé 0 volume $S$ï¼Œé‚£ä¹ˆæ˜ å°„åˆ°å†…éƒ¨$S$çš„ç‚¹æ˜¯é€šè¿‡è¾“å…¥çš„ä¸€ä¸ªçº¿æ€§å˜æ¢$B$å¾—åˆ°çš„ï¼Œä»è€Œè¡¨æ˜ï¼Œä¸full dimensionalè¾“å‡ºç›¸å¯¹åº”çš„è¾“å…¥ç©ºé—´éƒ¨åˆ†ï¼Œè¢«é™åˆ¶ä¸ºä¸€ä¸ªçº¿æ€§å˜æ¢ã€‚

In other words, deep networks only have the power of a linear classifier on the non-zero volume part of the output domain. We refer to supplemental material for a more formal statement.
æ¢å¥è¯è¯´ï¼Œæ·±åº¦ç½‘ç»œåªåœ¨è¾“å‡ºåŸŸçš„éé›¶volumeéƒ¨åˆ†å…·æœ‰çº¿æ€§åˆ†ç±»å™¨çš„èƒ½åŠ›ã€‚

ğŸ‘†ï¼Ÿ

<div align=center>
  <img src="https://github.com/JuneXia/JuneXia.github.io/raw/hexo/source/images/ml/mobilenetv2-1.jpg" width = 80% height = 80% />
</div>
Figure 1: &emsp; Examples of ReLU transformations of low-dimensional manifolds embedded in higher-dimensional spaces. In these examples the initial spiral(n.èºæ—‹;æ—‹æ¶¡;adj.èºæ—‹å½¢çš„;ç›˜æ—‹çš„) is embedded into an $n$-dimensional space using random matrix $T$ followed by ReLU, and then projected back to the 2D space using $T^{-1}$. In examples above n = 2,3 result in information loss where certain points of the manifold collapse into each other, while for n = 15 to 30 the transformation is highly `non-convex(éå‡¸)`.

> ä¸Šé¢è¿™æ®µè¯çš„æ„æ€å®é™…å°±æ˜¯è¯´ï¼šä½œè€…åšäº†ä¸€äº›å®éªŒæ¡ˆä¾‹ï¼Œå³ä½¿ç”¨éšæœºçŸ©é˜µ $T$ å’Œ ReLU å°†åˆå§‹åˆå§‹çš„2ç»´èºæ—‹å½¢åµŒå…¥åˆ°ä¸€ä¸ªnç»´ç©ºé—´ä¸­ï¼Œç„¶åä½¿ç”¨ $T^{-1}$ å°†å…¶æŠ•å½±å›2Dç©ºé—´ã€‚è€Œå½“n=2æˆ–3æ—¶ï¼Œå³èºæ—‹å½¢å…ˆè¢«åµŒå…¥åˆ°2æˆ–3ç»´ç©ºé—´ä¸­ç„¶åå†è¢«æŠ•å½±å›2Dç©ºé—´ï¼Œè¿™ä¼šå¯¼è‡´æŸäº›ç‚¹å¡Œé™·(å˜å½¢)ï¼Œè€Œå½“n=15åˆ°30æ—¶ï¼Œè¿™ç§å¡Œé™·(å˜å½¢)ä¼šæœ‰æ‰€ç¼“è§£ã€‚
> 
> è€Œè¿™å®é™…ä¸Šå°±æ˜¯è¯´ï¼Œå¯¹ä½ç»´ç‰¹å¾ä½¿ç”¨ReLUè¿™ç§éçº¿æ€§æ¿€æ´»å‡½æ•°ä¼šå¯¼è‡´ä¸¥é‡çš„ä¿¡æ¯æŸå¤±ã€‚
> 
> ä»è€Œé“å‡ºäº†ä½œè€…åœ¨MobileNetv2ä¸­ä½¿ç”¨çº¿æ€§æ¿€æ´»å‡½æ•°æ›¿ä»£éçº¿æ€§æ¿€æ´»å‡½æ•°çš„åŸå› ï¼š
> ç”±äºå€’æ®‹å·®ç»“æ„çš„è¾“å…¥ã€è¾“å‡ºéƒ½æ˜¯ä½ç»´ç‰¹å¾ï¼Œè€Œç”¨ReLUè¿™ç§éçº¿æ€§æ¿€æ´»å‡½æ•°ä¼šå¯¼è‡´ä¿¡æ¯æŸå¤±æ¯”è¾ƒä¸¥é‡ï¼Œæ‰€ä»¥ä½œè€…åœ¨MobileNetv2ä¸­ç”¨çº¿æ€§æ¿€æ´»å‡½æ•°æ›¿ä»£äº†éçº¿æ€§æ¿€æ´»å‡½æ•°ã€‚

<div align=center>
  <img src="https://github.com/JuneXia/JuneXia.github.io/raw/hexo/source/images/ml/mobilenetv2-2.jpg" width = 80% height = 80% />
</div>
Figure 2: Evolution(n.æ¼”å˜;è¿›åŒ–è®º;è¿›å±•) of separable convolution blocks. The diagonally(adv.å¯¹è§’åœ°;æ–œå¯¹åœ°) hatched(hatch n.v.å­µåŒ–;ç­–åˆ’; hatched adj.é˜´å½±çº¿çš„) texture indicates layers that do not contain non-linearities. The last (lightly colored) layer indicates the beginning of the next block. Note: 2d and 2c are equivalent(ç›¸ç­‰çš„;ç­‰ä»·çš„) blocks when stacked(stack n.v.å †å ). Best viewed in color.

<div align=center>
  <img src="https://github.com/JuneXia/JuneXia.github.io/raw/hexo/source/images/ml/mobilenetv2-3.jpg" width = 80% height = 80% />
</div>
Figure 3: The difference between residual block [8, 30] and inverted residual. Diagonally hatched layers do not use non-linearities. We use thickness(n.åšåº¦;å±‚;æµ“åº¦;å«æ··ä¸æ¸…) of each block to indicate its relative(n.ç›¸å…³ç‰©;adj.ç›¸å¯¹çš„) number of channels. Note how classical(adj.ç»å…¸çš„;ä¼ ç»Ÿçš„) residuals connects the layers with high number of channels, whereas the inverted residuals connect the bottlenecks. Best viewed in color.

&emsp; On the other hand, when ReLU collapses the channel, it inevitably loses information in *that channel*. However if we have lots of channels, and there is a a structure in the activation manifold that information might still be preserved in the other channels. In supplemental(adj.è¡¥å……çš„(ç­‰äºsupplementary);è¿½åŠ çš„) materials, we show that if the input manifold can be embedded into a significantly(adv.æ˜¾è‘—åœ°;ç›¸å½“æ•°é‡åœ°) lower-dimensional subspace of the activation space then the ReLU transformation preserves the information while introducing the needed complexity into the set of expressible functions.

&emsp; To summarize, we have highlighted two properties that are indicative(adj.è±¡å¾çš„;æŒ‡ç¤ºçš„;è¡¨ç¤ºâ€¦çš„) of the requirement that the manifold of interest should lie in a low-dimensional subspace of the higher-dimensional activation space:
ç»¼ä¸Šæ‰€è¿°ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†ä¸¤ä¸ªæ€§è´¨ï¼Œè¿™ä¸¤ä¸ªæ€§è´¨è¡¨æ˜äº†æˆ‘ä»¬æ‰€å…³å¿ƒçš„æµå½¢åº”è¯¥ä½äºé«˜ç»´æ¿€æ´»ç©ºé—´çš„ä½ç»´å­ç©ºé—´ä¸­ï¼š

1. If the manifold of interest remains non-zero volume after ReLU transformation, it corresponds to a linear transformation.
2. ReLU is capable of preserving complete information about the input manifold, but only if the input  manifold lies in a low-dimensional subspace of the input space. \
ReLUèƒ½å¤Ÿä¿å­˜è¾“å…¥æµå½¢çš„å®Œæ•´ä¿¡æ¯ï¼Œä½†å‰ææ˜¯è¾“å…¥æµå½¢ä½äºè¾“å…¥ç©ºé—´çš„ä½ç»´å­ç©ºé—´ä¸­ã€‚

&emsp; These two insights provide us with an `empirical hint(ç»éªŒæç¤º)` for optimizing existing neural architectures: assuming the manifold of interest is low-dimensional we can capture this by inserting *linear bottleneck* layers into the convolutional blocks. Experimental evidence suggests that using linear layers is crucial as it prevents(prevent v.é˜²æ­¢;é˜»æ­¢;é¢„é˜²) nonlinearities from destroying(destroy vt.ç ´å;æ¶ˆç­;æ¯å) too much information. In Section 6, we show empirically that using non-linear layers in bottlenecks indeed hurts the performance by several percent, `further validating our hypothesis.(ä»è€Œè¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„å‡è®¾)`. 
> We note that in the presence of shortcuts the information loss is actually less strong. \
> æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œåœ¨å­˜åœ¨shortcutsçš„æƒ…å†µä¸‹ï¼Œä¿¡æ¯æŸå¤±å®é™…ä¸Šä¸é‚£ä¹ˆä¸¥é‡ã€‚

We note that similar reports where non-linearity was helped were reported in [29] where non-linearity was removed from the input of the traditional residual block and that lead to improved performance on CIFAR dataset.
> è¹©è„šç›´è¯‘ï¼š\
> æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œåœ¨[29]ä¸­ä¹Ÿæœ‰å…³äºéçº¿æ€§å±‚æ˜¯æœ‰å¸®åŠ©çš„è¿™æ ·ç±»ä¼¼çš„æŠ¥é“ï¼Œè€Œnon-linearityè¢«ä»ä¼ ç»Ÿæ®‹å·®å—ä¸­çš„è¾“å…¥ä¸­ç§»é™¤ï¼Œä»è€Œæé«˜äº†CIFARæ•°æ®é›†çš„æ€§èƒ½ã€‚
> 
> ç†è§£ç¿»è¯‘ï¼š\
> æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œä¹Ÿæœ‰ç±»ä¼¼çš„æŠ¥é“ï¼Œåœ¨[29]ä¸­æŠ¥å‘Šè¯´éçº¿æ€§æ˜¯æœ‰å¸®åŠ©çš„ï¼Œç„¶è€Œå´å°†ä¼ ç»Ÿæ®‹å·®å—è¾“å…¥ä¸­çš„éçº¿æ€§ç§»é™¤ï¼Œä»è€Œæé«˜äº†CIFARæ•°æ®é›†çš„æ€§èƒ½ã€‚(è¨€å¤–ä¹‹æ„å°±æ˜¯è¯´ï¼šéçº¿æ€§ç¡®å®æœ‰å¸®åŠ©ï¼Œä½†åœ¨[29]ä¸­è¿˜æ˜¯å°†å…¶ç§»é™¤äº†ï¼Œè€Œç§»é™¤äº†æ•ˆæœä¼šæ›´å¥½)

&emsp; For the remainder(n.[æ•°]ä½™æ•°,æ®‹ä½™;å‰©ä½™ç‰©;å…¶ä½™çš„äºº; adj.å‰©ä½™çš„; v.å»‰ä»·å‡ºå”®) of this paper we will be utilizing bottleneck convolutions. 
We will refer to the ratio between the size of the input bottleneck and the inner size as the *expansion ratio*.
æˆ‘ä»¬å°†è¾“å…¥ç“¶é¢ˆçš„å¤§å°ä¸å†…éƒ¨å¤§å°çš„æ¯”å€¼ç§°ä¸ºæ‰©å±•æ¯”ã€‚

## Inverted residuals
&emsp; The bottleneck blocks appear similar to residual block where each block contains an input followed by several bottlenecks then followed by expansion [8]. However, inspired by the intuition that the bottlenecks actually contain all the necessary information, while an expansion layer acts merely as an implementation(n.[è®¡]å®ç°;å±¥è¡Œ;å®æ–½) detail that accompanies a non-linear transformation of the tensor, we use shortcuts directly between the bottlenecks.

**Running time and parameter count for bottleneck convolution** &emsp; The basic implementation structure is illustrated in Table 1. For a block of size $h \times w$, expansion factor $t$ and kernel size $k$ with $d'$ input channels and $d''$ output channels, the total number of multiply add required is $h Â· w Â· d' Â· t(d' + k^2 + d'')$. Compared with (1)(æŒ‡å…¬å¼(1)) this expression has an extra term, as indeed we have an extra $1 \times 1$ convolution, however the nature of our networks allows us to utilize much smaller input and output dimensions. In Table 3 we compare the needed sizes for each resolution between MobileNetV1, MobileNetV2 and ShuffleNet.

## Information flow interpretation
&emsp; One interesting property of our architecture is that it provides a natural separation between the input/output domains of the building blocks (bottleneck layers), and the layer transformation that is a non-linear function that converts input to the output. The former(å‰è€…) can be seen as the capacity of the network at each layer, whereas the latter(åè€…) as the expressiveness(n. è¡¨è¾¾èƒ½åŠ›;å–„äºè¡¨ç°;è¡¨æƒ…ä¸°å¯Œ). 

This is in contrast with traditional convolutional blocks, both regular and separable, where both expressiveness and capacity are tangled(tangle v.(ä½¿)ç¼ ç»“åœ¨ä¸€èµ·;(ä½¿)ä¹±æˆä¸€å›¢;äº‰åµ;æ‰“æ¶;n.ç¼ ç»“;æ··ä¹±,çº·ä¹±;äº‰åµ;æ‰“æ¶) together and are functions of the output layer depth.
è¿™ä¸ä¼ ç»Ÿçš„å·ç§¯å—å½¢æˆå¯¹æ¯”ï¼Œæ— è®ºæ˜¯è§„åˆ™å—è¿˜æ˜¯å¯åˆ†å—ï¼Œåœ¨ä¼ ç»Ÿå·ç§¯å—ä¸­ï¼Œè¡¨è¾¾æ€§å’Œå®¹é‡éƒ½æ˜¯çº ç¼ åœ¨ä¸€èµ·çš„ï¼Œæ˜¯è¾“å‡ºå±‚æ·±åº¦çš„å‡½æ•°ã€‚

&emsp; In particular, in our case, when inner layer depth is 0 the underlying convolution is the identity function thanks to the shortcut connection. 
ç‰¹åˆ«åœ°ï¼Œåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå½“å†…å±‚æ·±åº¦ä¸º 0 æ—¶ï¼Œç”±äºå¿«æ·è¿æ¥ï¼Œåº•å±‚å·ç§¯æ˜¯ identity function. \
When the expansion ratio is smaller than 1, this is a classical residual convolutional block [8, 30]. However, for our purposes we show that expansion ratio greater than 1 is the most useful.

&emsp; This interpretation allows us to study the expressiveness of the network separately from its capacity and we believe that further exploration of this separation is warranted(warrant n.æ ¹æ®;è¯æ˜;æ­£å½“ç†ç”±;å§”ä»»çŠ¶;vt.ä¿è¯;æ‹…ä¿;æ‰¹å‡†;è¾©è§£) to provide a better understanding of the network properties.
è¿™ä¸€è§£é‡Šä½¿æˆ‘ä»¬èƒ½å¤Ÿç‹¬ç«‹äºç½‘ç»œçš„å®¹é‡æ¥ç ”ç©¶ç½‘ç»œçš„è¡¨è¾¾æ€§ï¼Œæˆ‘ä»¬ç›¸ä¿¡ï¼Œå¯¹è¿™ç§åˆ†ç¦»çš„è¿›ä¸€æ­¥æ¢ç´¢æœ‰åŠ©äºæ›´å¥½åœ°ç†è§£ç½‘ç»œçš„ç‰¹æ€§ã€‚


# Model Architecture


<div align=center>
  <img src="https://github.com/JuneXia/JuneXia.github.io/raw/hexo/source/images/ml/mobilenetv2-4.jpg" width = 80% height = 80% />
</div>





PyTorch å®˜æ–¹å®ç°ä»£ç è§£æ

torchvison.models.mobilenet.py
```python
from torch import nn
from .utils import load_state_dict_from_url


__all__ = ['MobileNetV2', 'mobilenet_v2']


model_urls = {
    'mobilenet_v2': 'https://download.pytorch.org/models/mobilenet_v2-b0353104.pth',
}


def _make_divisible(v, divisor, min_value=None):
    """
    This function is taken from the original tf repo.
    It ensures that all layers have a channel number that is divisible by 8
    It can be seen here:
    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py
    :param v:
    :param divisor:
    :param min_value:
    :return:
    """
    if min_value is None:
        min_value = divisor
    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
    # Make sure that round down does not go down by more than 10%.
    # ç¡®ä¿å‘ä¸‹å–æ•´çš„æ—¶å€™ä¸ä¼šè¶…è¿‡10%.
    if new_v < 0.9 * v:
        new_v += divisor
    return new_v


class ConvBNReLU(nn.Sequential):
    """
    :param in_planes: the number of input channel.
    :param out_planes: the number of output channel.
    :param kernel_size:
    :param stride:
    :param groups: it's denote general converlution while groups=1, depth-wise converlution while groups=in_planes.
    """
    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):
        padding = (kernel_size - 1) // 2
        super(ConvBNReLU, self).__init__(
            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),
            nn.BatchNorm2d(out_planes),
            nn.ReLU6(inplace=True)
        )


class InvertedResidual(nn.Module):
    def __init__(self, inp, oup, stride, expand_ratio):
        super(InvertedResidual, self).__init__()
        self.stride = stride
        assert stride in [1, 2]

        hidden_dim = int(round(inp * expand_ratio))
        # hidden_dim is matched to $tk$ of paper.

        self.use_res_connect = self.stride == 1 and inp == oup
        # set using residul connect or not.

        layers = []
        if expand_ratio != 1:
            # pw
            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))
            # In practicaly, layers çš„ append or extend æ˜¯ç­‰æ•ˆçš„ï¼Œåªä¸è¿‡appendæ˜¯ä¸€æ¬¡è¿½åŠ ä¸€ä¸ªå…ƒç´ ï¼Œè€Œextendæ˜¯ä¸€æ¬¡è¿½åŠ å¤šä¸ªå…ƒç´ 
            layers.extend([
            # dw
            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),
            # pw-linear
            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),
            nn.BatchNorm2d(oup),
        ])
        self.conv = nn.Sequential(*layers)

    def forward(self, x):
        if self.use_res_connect:
            return x + self.conv(x)
        else:
            return self.conv(x)


class MobileNetV2(nn.Module):
    def __init__(self, num_classes=1000, width_mult=1.0, inverted_residual_setting=None, round_nearest=8):
        """
        MobileNet V2 main class

        Args:
            num_classes (int): Number of classes
            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount
            inverted_residual_setting: Network structure
            round_nearest (int): Round the number of channels in each layer to be a multiple of this number
            Set to 1 to turn off rounding
        """
        super(MobileNetV2, self).__init__()
        block = InvertedResidual
        input_channel = 32
        last_channel = 1280

        if inverted_residual_setting is None:
            inverted_residual_setting = [
                # t, c, n, s
                # t: æ‰©å±•å› å­ï¼Œç”¨äºåœ¨pwå·ç§¯ä¸­ç”¨äºå°†kä¸ªè¾“å…¥channelæ•°é‡å˜æ¢åˆ°tkä¸ªè¾“å‡ºchannelã€‚
                # c: è¾“å‡ºchannel
                # n: InvertedResidualéœ€è¦é‡å¤çš„æ¬¡æ•°
                # s: å·åŸºå±‚stride
                [1, 16, 1, 1],
                [6, 24, 2, 2],
                [6, 32, 3, 2],
                [6, 64, 4, 2],
                [6, 96, 3, 1],
                [6, 160, 3, 2],
                [6, 320, 1, 1],
            ]

        # only check the first element, assuming user knows t,c,n,s are required
        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:
            raise ValueError("inverted_residual_setting should be non-empty "
                             "or a 4-element list, got {}".format(inverted_residual_setting))

        # building first layer
        input_channel = _make_divisible(input_channel * width_mult, round_nearest)
        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)

        features = [ConvBNReLU(3, input_channel, stride=2)]
        # building inverted residual blocks
        for t, c, n, s in inverted_residual_setting:
            output_channel = _make_divisible(c * width_mult, round_nearest)
            for i in range(n):
                stride = s if i == 0 else 1  # åœ¨æ¯ä¸ªinverted residual blocksä¸­ï¼Œç¬¬ä¸€ä¸ªå·ç§¯çš„strideç”¨sï¼Œç¬¬äºŒä¸ªå·ç§¯çš„strideéƒ½ç»Ÿä¸€çš„ç”¨1

                features.append(block(input_channel, output_channel, stride, expand_ratio=t))
                input_channel = output_channel
        # building last several layers
        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))
        # make it nn.Sequential
        self.features = nn.Sequential(*features)

        # building classifier
        self.classifier = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(self.last_channel, num_classes),
        )

        # weight initialization
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        x = self.features(x)
        x = x.mean([2, 3])  # avg-pooling
        x = self.classifier(x)
        return x


def mobilenet_v2(pretrained=False, progress=True, **kwargs):
    """
    Constructs a MobileNetV2 architecture from
    `"MobileNetV2: Inverted Residuals and Linear Bottlenecks" <https://arxiv.org/abs/1801.04381>`_.

    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    model = MobileNetV2(**kwargs)
    if pretrained:
        state_dict = load_state_dict_from_url(model_urls['mobilenet_v2'],
                                              progress=progress)
        model.load_state_dict(state_dict)
    return model
```

