---
title: 【深度学习笔记3.2 正则化】Dropout
date: 2019-03-11 17:28:05
tags:
categories: ["深度学习笔记"]
mathjax: true
---
<!-- more -->

关于dropout的理解与总结：

 - dropout是什么？参考文献[1]
 - dropout会让train变差，让test变好。一般的如果在train-set上表现好，在test-set上表现差，用dropout才有效果。使用dropout是为了避免过拟合。(来自网友)
 - 下图来自文献[3]
![enter image description here](https://lh3.googleusercontent.com/-HPmIrBCprd8/XFOjxz5a3YI/AAAAAAAAAL0/IoAB49sKhHUdL1ea25P9KJkwRmDyL0CfACLcBGAs/s0/dropout1.png "dropout1.png")
&emsp; 上图中的思想就是说：Dropout是一种正则化技术，是防止过拟合最有效的方法，然而在以下几种情况下使用dropout会损害性能。
1). 放在最后一层之前(即softmax之前)。这通常是一个不适合使用dropout的地方，因为网络没有能力在分类之前“纠正”drop引起的错误；
2). 当网络较小时，通常不需要正则化。如果模型capacity已经很低时，那么增加正则化会进一步降低模型capacity以至于损害模型性能；
3). 使用dropout的网络收敛速度比较慢，但它最终收敛时的误差会比较低，所以如果你没有那么多时间训练网络直至收敛的话，你可能不应该使用dropout.
 - 当数据量不大的时候，通常不考虑使用dropout.
 - 目前dropout已经很少使用，已经被一种称为Batch Normalization的技术有取代，当然这并不是说dropout就不是一个有效的工具[3]。


## 参考文献
[1] [理解dropout](https://blog.csdn.net/stdcoutzyx/article/details/49022443)
[2] [深度学习中Dropout原理解析](https://zhuanlan.zhihu.com/p/38200980)
[3] [Dropout makes performance worse](https://stats.stackexchange.com/questions/299292/dropout-makes-performance-worse)
