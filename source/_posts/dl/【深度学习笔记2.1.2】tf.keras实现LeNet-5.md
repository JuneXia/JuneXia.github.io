---
title: 【深度学习笔记2.1.2】tf.keras实现LeNet-5
date: 2019-07-16 17:28:05
tags:
categories: ["深度学习笔记"]
mathjax: true
---
<!-- more -->

# 构建简单的模型
tf.keras简洁高效，本实验尝试使用tf.keras.Sequential来实现LeNet。
```python
import numpy as np  
import tensorflow as tf  
from tensorflow.keras import layers  
from tensorflow.keras.utils import to_categorical  
  
  
if __name__ == '__main__':  
    (mnist_images, mnist_labels), a = tf.keras.datasets.mnist.load_data()  
    mnist_labels = to_categorical(mnist_labels, 10)  
    mnist_images = np.expand_dims(mnist_images, axis=3) / 255  
  mnist_labels = mnist_labels.astype(np.float)  
  
    model = tf.keras.Sequential([  
        layers.Conv2D(6, (5, 5), activation='relu', padding='same', name='conv1'),  
  layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2), name='pool1'),  
  layers.Conv2D(16, (5, 5), activation='relu', padding='valid', name='conv2'),  
  layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool2'),  
  layers.Conv2D(120, [5, 5], activation='relu', padding='valid', name='conv3'),  
  layers.Flatten(name='flatten'),  
  layers.Dense(units=84, activation='relu', name='fc2'),  
  
  layers.Dense(10, activation='softmax')  
    ])  
  
    model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),  
  loss='categorical_crossentropy',  
  metrics=['accuracy'])  
  
    model.fit(mnist_images, mnist_labels, batch_size=32, epochs=5)
```

# 函数式API
[`tf.keras.Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential)  模型是层的简单堆叠，无法表示任意模型。使用  [Keras 函数式 API](https://keras.io/getting-started/functional-api-guide/)  可以构建复杂的模型拓扑[1]，例如：

-   多输入模型，
-   多输出模型，
-   具有共享层的模型（同一层被调用多次），
-   具有非序列数据流的模型（例如，剩余连接）。

使用函数式 API 构建的模型具有以下特征：

1.  层实例可调用并返回张量。
2.  输入张量和输出张量用于定义  [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model)  实例。
3.  此模型的训练方式和  `Sequential`  模型一样。

以下示例使用函数式 API 构建的LeNet：
```python
import numpy as np  
import tensorflow as tf  
from tensorflow import keras  
from tensorflow.keras import layers  
from tensorflow.keras.utils import to_categorical  
  
  
class LeNet(keras.Model):  
    def __init__(self, input_shape=(28, 28, 1), num_classes=10):  
        # super(LeNet, self).__init__(name="LeNet")  
  self.num_classes = num_classes  
  
        img_input = layers.Input(shape=input_shape)  
  
        x = layers.Conv2D(6, (5, 5), activation='relu', padding='same', name='conv1')(img_input)  
        x = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2), name='pool1')(x)  
        x = layers.Conv2D(16, (5, 5), activation='relu', padding='valid', name='conv2')(x)  
        x = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool2')(x)  
        x = layers.Conv2D(120, [5, 5], activation='relu', padding='valid', name='conv3')(x)  
        x = layers.Flatten(name='flatten')(x)  
        x = layers.Dense(units=84, activation='relu', name='fc2')(x)  
  
        x = layers.Dense(units=num_classes, activation='softmax', name='prediction')(x)  
  
        # 调用Model类的Model(input, output, name="***")构造方法  
  super(LeNet, self).__init__(img_input, x, name='LeNet')  
        # 本次实验是使用tf.keras的函数式API编程，上面这种调用父类构造的方法和下面的直接创建父类对象效果是一样的。  
  # tf.keras.Model(inputs=img_input, outputs=x, name='LeNet')  
 # 如果要用tf.keras.Model创建父类对象的话，那么子类LeNet也没有必要继承keras.Model了。  
  
  
  def __call__(self, *args, **kwargs):  
        '''  
  TODO: 实实测该函数并不会被调用  
 '''  print('debug')  
  
    def call(self, inputs):  
        '''  
  TODO: 实测该函数并不会被调用  
 '''  # 前向传播计算  
  # 使用在__init__方法中定义的层  
  return self.output(inputs)  
  
  
if __name__ == '__main__':  
    (mnist_images, mnist_labels), a = tf.keras.datasets.mnist.load_data()  
    mnist_labels = to_categorical(mnist_labels, 10)  
    mnist_images = np.expand_dims(mnist_images, axis=3) / 255  
  mnist_labels = mnist_labels.astype(np.float)  
  
    model = LeNet()  
  
    model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),  
  loss='categorical_crossentropy',  
  metrics=['accuracy'])  
  
    model.fit(mnist_images, mnist_labels, batch_size=32, epochs=5)
```

上述LeNet代码的输出是分类标签，然而有的时候我们希望得到一个embedding特征向量，这时候可以将上述LeNet中的分类层拿出来，此时LeNet得到的输出是一个embedding，我们再用tf.keras.Sequential对其进行一层封装即可：
```python
...重复代码省略...

# Change Mark: 相对于lenet2.py，本次实验必须使用eager模式，否则会报错  
tf.enable_eager_execution()  
print('is eager executing: ', tf.executing_eagerly())  
  
  
class LeNet(keras.Model):  
    def __init__(self, input_shape=(28, 28, 1), num_classes=10):  
        ...略
        # 从__init__函数中去掉layers.Dense(units=num_classes, ...)层，其他不变。
        ...
  
if __name__ == '__main__':  
    
    ...
    
    # Change Mark: 相对于lenet2.py，本次实验中的LeNet只是整个网络中的一部分，使用tf.keras.Sequential对网络进行拼接。  
  model = tf.keras.Sequential([  
        LeNet(),  
  
    # 这种先用sigmoid再用softmax的方法也可以，但是实测准确率没有直接用softmax高  
    # ****************************************  
    # layers.Dense(10, activation='sigmoid') 
    # layers.Activation('softmax') 
    # ****************************************  
    
    # OK, GOOD 
    # ****************************************  
    layers.Dense(10, activation='softmax')  
    # ****************************************  
  ])  
  
  ...
  
```


# 模型子类化
通过对  [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model)  进行子类化并定义您自己的前向传播来构建完全可自定义的模型。在  `__init__`  方法中创建层并将它们设置为类实例的属性。在  `call`  方法中定义前向传播[1]。

在启用  [Eager Execution](https://www.tensorflow.org/guide/eager)  时，模型子类化特别有用，因为可以命令式地编写前向传播。

> **要点**：针对作业使用正确的 API。虽然模型子类化较为灵活，但代价是复杂性更高且用户出错率更高。如果可能，请首选函数式 API。

以下示例展示了使用自定义前向传播进行子类化的  [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model)：

```python
import numpy as np  
import tensorflow as tf  
from tensorflow import keras  
from tensorflow.keras import layers  
from tensorflow.keras.utils import to_categorical  
  
  
# Change Mark: 相对于lenet2.py，本次实验使用eager模式时，在第一次调用call函数时，  
# 传入的inputs是整个训练集（亲测第二次及以后都是每次只传入batch_size个训练集），这将导致内存溢出。  
# 但本次实验不使用eager模式是ok的。如果非要想使用eager模式，可以试试将训练用tf.data封装后再传入model.fit。  
# tf.enable_eager_execution()  
print('is eager executing: ', tf.executing_eagerly())  
  
  
class LeNet(tf.keras.Model):  
    def __init__(self, num_classes=10):  
        super(LeNet, self).__init__()  
        self.num_classes = num_classes  
  
        self.conv1_fn = layers.Conv2D(6, (5, 5), activation='relu', padding='same', name='conv1')  
        self.pool1_fn = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2), name='pool1')  
        self.conv2_fn = layers.Conv2D(16, (5, 5), activation='relu', padding='valid', name='conv2')  
        self.pool2_fn = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool2')  
        self.conv3_fn = layers.Conv2D(120, [5, 5], activation='relu', padding='valid', name='conv3')  
        self.flat_fn = layers.Flatten(name='flatten')  
        self.dense1_fn = layers.Dense(units=84, activation='relu', name='fc2')  
  
    def call(self, inputs):  
        x = self.conv1_fn(inputs)  
        x = self.pool1_fn(x)  
        x = self.conv2_fn(x)  
        x = self.pool2_fn(x)  
        x = self.conv3_fn(x)  
        x = self.flat_fn(x)  
        x = self.dense1_fn(x)  
  
        return x  
  
    def compute_output_shape(self, input_shape):  
        # You need to override this function if you want to use the subclassed model  
 # as part of a functional-style model. # Otherwise, this method is optional.  shape = tf.TensorShape(input_shape).as_list()  
        shape = [shape[0], 84]  
        return tf.TensorShape(shape)  
  
  
if __name__ == '__main__':  
    (mnist_images, mnist_labels), a = tf.keras.datasets.mnist.load_data()  
    mnist_labels = to_categorical(mnist_labels, 10)  
    mnist_images = np.expand_dims(mnist_images, axis=3) / 255  
  mnist_labels = mnist_labels.astype(np.float)  
  
    model = tf.keras.Sequential([  
        LeNet(),  
  layers.Dense(10, activation='softmax')  
    ])  
  
    model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),  
  loss='categorical_crossentropy',  
  metrics=['accuracy'])  
  
    model.fit(mnist_images, mnist_labels, batch_size=32, epochs=5)
```


# 参考文献
[1] [TensorFlow学习指南>高阶API>Keras](https://www.tensorflow.org/guide/keras#model_subclassing)
