---
title: 【机器学习基础】范数与正则化
date: 2018-06-25
tags:
categories: ["机器学习笔记"]
mathjax: true
---

本篇主要介绍范数以及使用范数的正则化策略。
<!-- more -->

## tf.nn.l2_loss
参考[3,6]
```python
if __name__ == '__main__3':
    A = tf.constant([[1, 1], [2, 2], [3, 3]], dtype=tf.float32)
    B = tf.constant([1, 2, 3], dtype=tf.float32)

    with tf.Session() as sess:
        a = tf.nn.l2_loss(A).eval()
        b = tf.nn.l2_loss(B).eval()

        print(a)
```


## tf.norm
[Tensorflow快餐教程(5) - 范数](https://blog.csdn.net/lusing/article/details/80082235)
```python
if __name__ == '__main__2':
    A = tf.constant([[1, 1], [2, 2], [3, 3]], dtype=tf.float32)
    # A = tf.constant([1, 2, 3, 4], dtype=tf.float32)

    with tf.Session() as sess:
        # axis=0时，按行向量计算
        # ord控制的是p范数
        norm = tf.norm(A, ord=1, axis=0).eval()
        norm = tf.norm(A, ord=2, axis=0).eval()
        norm = tf.norm(A, ord='euclidean', axis=0).eval()
        norm = tf.norm(A, ord=np.inf, axis=0).eval()

        # axis=1时，按列向量计算
        # ord控制的是p范数
        norm = tf.norm(A, ord=1, axis=1).eval()
        norm = tf.norm(A, ord=2, axis=1).eval()
        norm = tf.norm(A, ord='euclidean', axis=1).eval()
        norm = tf.norm(A, ord=np.inf, axis=1).eval()

        # axis=-1时，表示按最后一个维度计算，这里即是按列向量计算
        # ord控制的是p范数
        norm = tf.norm(A, ord=1, axis=-1).eval()
        norm = tf.norm(A, ord=2, axis=-1).eval()
        norm = tf.norm(A, ord='euclidean', axis=-1).eval()
        norm = tf.norm(A, ord=np.inf, axis=-1).eval()

        # ord控制的是p范数，当axis=None时，tf.norm表示在得到p范数后的向量的基础上再做一次求和，然后再开根号。
        # 而此时当ord=2或者'euclidean'时，tf.norm得到的数值也称作Frobenius范数
        norm = tf.norm(A, ord=1, axis=None).eval()
        norm = tf.norm(A, ord=2, axis=None).eval()
        norm = tf.norm(A, ord='euclidean', axis=None).eval()
        norm = tf.norm(A, ord=np.inf, axis=None).eval()

        print(norm)

```


## tf.nn.l2_normalize
[tensorflow l2_normalize函数](https://www.cnblogs.com/lovephysics/p/7222459.html)
[tf.nn.l2_loss和 tf.nn.l2_normalize](https://blog.csdn.net/m0_37561765/article/details/79645026)
```python
if __name__ == '__main__1':
    A = tf.constant([[1, 1], [2, 2], [3, 3]], dtype=tf.float32)

    with tf.Session() as sess:
        norm = tf.nn.l2_normalize(A, [0])  # 对列向量进行l2-norm
        # norm = tf.nn.l2_normalize(A, [1])  # 对行向量进行l2-norm

        arr = sess.run(norm)
        print(arr)
        sess.close()
```


## slim.l2_regularizer
参考[7,8]



[1] [Linear least squares, Lasso,ridge regression有何本质区别？](https://www.zhihu.com/question/38121173)
[2] [机器学习中的范数规则化之（一）L0、L1与L2范数](https://blog.csdn.net/zouxy09/article/details/24971995)
[3] [TensorFlow 深度学习笔记 TensorFlow实现与优化深度神经网络](https://www.cnblogs.com/hellocwh/p/5527141.html)
[4] [TensorFlow 中的正则化方法](https://www.bilibili.com/read/cv723177/)
[5] [l1-norm loss & l2-norm loss （l1范数和l2范数作为正则项的比较）](https://blog.csdn.net/edogawachia/article/details/80058340)
[6] [tensorflow的loss损失函数tf.nn.l2_loss](https://blog.csdn.net/zz2230633069/article/details/81413138)
[7] [『TensorFlow』正则化添加方法整理](https://www.cnblogs.com/hellcat/p/9474393.html)
[8] [Inside TF-Slim(4) regularizers & initializers](https://zhuanlan.zhihu.com/p/34202475)
[9] [范数和正则化](https://blog.csdn.net/iterate7/article/details/75443504)
[10] https://www.zhihu.com/question/20924039





