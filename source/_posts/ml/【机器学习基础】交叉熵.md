---
title: 【机器学习基础】交叉熵
date: 2017-06-25
tags:
categories: ["机器学习笔记"]
mathjax: true
---

## 概述 (me)
交叉熵是用来衡量两个分布之间相似程度的一种度量方法，假设$p(x)$为真实分布，$q(x)$为预测的分布，我们通过下面的交叉熵定义来衡量它们之间的相似度：
$H = -\sum \limits_{x \in X} p(x) \log q(x)$
<!-- more -->

## 信息量与熵
若有时间最好是看看文献[1]，时间紧迫可以先看参考文献[3]，另外可以再看看文献[2,6]。

熵和对数似然函数的关系，参考文献[4]


## 使用tensorflow基础api实现的交叉熵（不推荐使用）
```python
# 借用【Softmax函数】中介绍的softmax_NumStability
def softmax_NumStability(x):
    reduce_max = tf.reduce_max(x, 1, keepdims=True)
    prob = tf.nn.softmax(x - reduce_max)
    return prob
    
def tf_baseAPI_cross_entropy_loss(labels, prob):
    '''
    随着训练的进行，模型准确率越来越高，softmax的输出概率在相异类别上越来越趋近于0，对这些趋近于0的数取log将会得到很大的数，即nan了。
    所以我们这里使用tf.clip_by_value对prob数值进行裁剪，过滤掉太小的prob值
    '''
    clip_prob = tf.clip_by_value(prob, 1e-10, 1.0)
    cross_entropy = -tf.reduce_sum(labels * tf.log(clip_prob), 1)
    loss = tf.reduce_mean(cross_entropy)

    return loss

prob = softmax_NumStability(fc_layer)
loss = tf_baseAPI_cross_entropy_loss(labels_hold, prob)
```


## logit变换
参考文献[5]

## softmax、交叉熵、logit变换等在计算loss时的应用
参考文献[5,7,8]

> 注意：
> &emsp; 本文通过softmax\_NumStability和tf.clip\_by\_value等处理手段实现的交叉熵，虽然在数值稳定性方面有了不少改进，但还是推荐谷歌实现的tf.nn.softmax\_cross\_entropy\_with\_logits，相信谷歌牛人定会在内部有更好的优化（比如我还是没有弄明白logit变换是如何应用于交叉熵的，python程序如何实现）。
> &emsp; 另外，tf.nn.softmax_cross_entropy_with_logits函数已经过时 (deprecated)，它在TensorFlow未来的版本中将被去除。取而代之的是tf.nn.softmax_cross_entropy_with_logits_v2.


## 参考文献
[1] 信息论与编码 第三版. 曹雪虹，张宗橙. 清华大学出版社
[2] [如何通俗的解释交叉熵与相对熵?](https://www.zhihu.com/question/41252833) 主要看网友CyberRep的回答
[3] [交叉熵（Cross-Entropy）](https://blog.csdn.net/rtygbwwwerr/article/details/50778098)
[4] [Softmax函数与交叉熵](https://zhuanlan.zhihu.com/p/27223959)
[5] [softmax_cross_entropy_with_logits中“logits”是个什么意思？](https://zhuanlan.zhihu.com/p/51431626)
[6] [复习：常见的损失函数](https://zhuanlan.zhihu.com/p/33542937)	
[7] [TF里几种loss和注意事项](https://zhuanlan.zhihu.com/p/33560183)
[8] [Tensorflow基础知识---损失函数详解](https://sthsf.github.io/wiki/Algorithm/DeepLearning/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Tensorflow%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86---%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3.html)
