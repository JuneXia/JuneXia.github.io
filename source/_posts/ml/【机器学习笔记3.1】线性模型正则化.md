---
title: 【机器学习笔记3.1】线性模型正则化
date: 2017-06-10
tags:
categories: ["机器学习笔记"]
mathjax: true
---

## 过拟合问题描述
在机器学习应用时，会遇到过拟合(over-fitting)问题，本文将介绍一种被称为正则化(regularization)的技术，它可以有效改善或者减少过拟合问题[1]。
如果我们有非常多的特征，我们通过学习得到的假设可能能够非常好地适应训练集（代价函数可能几乎为0），但是可能会无法推广到新的数据，也就是泛化能力比较差。
<!-- more -->

下图是一个回归问题的例子：\
![这里写图片描述](https://img-blog.csdn.net/20180620202927320?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTEzNjIyOTc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
下图是分类问题的例子：\
![这里写图片描述](https://img-blog.csdn.net/20180620203050563?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTEzNjIyOTc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

就以多项式为例来理解，x的次数越高，则拟合的越好，但相应的预测能力就可能变差。
那么当我们发现过拟合问题时，应该如何处理呢？
1. 丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如PCA）
2. 正则化。保留所有特征，但是减少参数的大小。

## 代价函数
上面的回归问题中如果我们的模型是：
$h_{\theta}(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2^2 + \theta_3 x_3^3 + \theta_4 x_4^4$
我们可以从之前的事例中看出，正是那些高次项导致了过拟合的产生，所以如果我们能让这些高次项的系数近于0的话，我们就能很好的拟合了。
所以我们要做的就是在一定程度上减少这些参数θ的值，这就是正则化的基本方法。我们这里决定要减少$\theta_3$和$\theta_4$的大小，我们要做的便是修改代价函数，在其中$\theta_3$和$\theta_4$设置一点惩罚，修改后的代价函数如下：
\begin{equation}
	\mathop{\min}_{\theta} \dfrac{1}{2m}[\mathop{\sum}_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2 + 1000 \theta_3^2 + 10000 \theta_4^2] \tag{1}
\end{equation}
通过这样的代价函数选择出的 θ3和 θ4对预测结果的影响就比之前要小许多。假如我们有非常多的特征，我们并不知道其中哪些特征我们要惩罚，我们将对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设：
\begin{equation}
	\mathop{\min}_{\theta} \dfrac{1}{2m}[\mathop{\sum}_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2 + \lambda \mathop{\sum}_{j=1}^n {\theta_j}^2] \tag{2}
\end{equation}
其中 λ 又称为正则化参数（Regularization Parameter）。 注： 根据惯例， 我们不对 θ0 进行惩罚。经过正则化处理的模型与原模型的可能对比如下图所示：
![这里写图片描述](https://img-blog.csdn.net/20180620224049370?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTEzNjIyOTc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
如果选择的正则化参数 λ 过大，则会把所有的参数都最小化了，导致模型变成$h_{\theta}(x) = \theta_0$，也就是上图中红色直线所示的情况，造成欠拟合。
那为什么增加一项$\lambda = \mathop{\sum}_{j=1}^n {\theta}_j^2$就可以是θ的值减小呢？
因为如果我们令λ的值很大的话，为了使 Cost Function 尽可能的小，所有的 θ 的值（不包括 θ0） 都会在一定程度上减小。
但若λ的值太大了， 那么 θ（不包括 θ0） 都会趋近于 0， 这样我们所得到的只能是一条平行于 x 轴的直线。
所以对于正则化，我们要取一个合理的λ的值，这样才能更好的应用正则化。
下面介绍正则化在线性回归和逻辑回归中的应用。

## 正则化线性回归
正则化线性回归的代价函数为：
\begin{equation}
	J(\theta) = \dfrac{1}{2m}[\mathop{\sum}_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2 + \lambda \mathop{\sum}_{j=1}^n {\theta_j}^2],  \qquad  其中j \not= 0\tag{3}
\end{equation}
如果我们使用梯度下降法来使这个代价函数最小化，因为我们未对$\theta_0$进行正则化，所以梯度下降算法将分两种情形：
![这里写图片描述](https://img-blog.csdn.net/2018062022411966?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTEzNjIyOTc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
> j≠0时的求导过程如下：
$$\dfrac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m}[\mathop{\sum}_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)}) x_j^{(i)} + \lambda \theta_j]$$
(me)注：这里θ共有n项，当对第j项$\theta_j$求偏导时只有$\theta_j$项被保留下来，其他的都被当成常数求导等于0了。

也可以利用正规方程来求解正则化线性回归模型，参考[【机器学习笔记1.1】](https://blog.csdn.net/yahstudio/article/details/80679404)中的方法，我们可以把这里的代价函数式(3)写成矩阵的形式
$$J(\vec{\theta}) = (\vec{y} - \matrix{X}\vec{\theta})^T(\vec{y} - \matrix{X}\vec{\theta}) + \lambda \vec{\theta}^T \vec{\theta} \tag{4}$$
其中$\matrix{X}是一个m \times n$的矩阵，$\vec{y}是m \times 1$的向量，$\vec{\theta}是n \times 1$的向量。
将$J(\vec{\theta})$对$\vec{\theta}$求导得
$$
\frac{\partial J(\vec{\theta})}{\partial \vec{\theta}} = -2\matrix{X}^T(\vec{y} - \matrix{X}\vec{\theta}) + 2\lambda \vec{\theta} \tag{5}$$
令$$\dfrac{\partial J(\vec{\theta})}{\partial \vec{\theta}_j} = 0$$
解得
$$\vec{\theta} = \left(
\matrix{X}^T\matrix{X} + \lambda
 \left[
 \begin{matrix}
   1 \\
     & 1 \\
     & & \ddots \\
     & & & 1
  \end{matrix}
  \right]
  \right)^{-1} \matrix{X}^T \vec{y}
$$
考虑$\theta_0$时的求解结果
$$\vec{\theta} = \left(
\matrix{X}^T\matrix{X} + \lambda
 \left[
 \begin{matrix}
   0  & \\
       & 1\\
       & & 1 \\
       & & & \ddots \\
       & & & & 1
  \end{matrix}
  \right]
  \right)^{-1} \matrix{X}^T \vec{y}
$$
此时$\matrix{X}是一个m \times (n+1)$的矩阵，$\vec{y}是m \times 1$的向量，$\vec{\theta}是(n+1) \times 1$的向量。


## 正则化逻辑回归
逻辑回归参见[【机器学习笔记2.1】](https://blog.csdn.net/yahstudio/article/details/80615752)，我们这里为逻辑回归的代价函数增加一个正则化的表达式：
5！！！！！！！！！！！！！！！！！

## 参考文献
[1] 黄海广. MIT 机器学习教程
