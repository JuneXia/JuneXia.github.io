---
title: 【机器学习笔记1.1】线性回归之正规方程求解
date: 2017-05-19
tags:
categories: ["机器学习笔记"]
mathjax: true
---

## 线性回归概述
在这里：
我们先考虑最简单的一种情况，即输入属性的数目只有一个，线性回归试图学得[1]
$$f(x_i) = w x_i + b，使得f(x_i) \approx y_i \tag{1}$$
那么如何确定$\vec{w}$和b呢？关键在于如何衡量$f(\vec{x})$与y之间的差别。
均方误差是回归任务中最常用的性能度量，因此我们可以试图让均方误差最小化，即
<!-- more -->

$$(w^*, b^*) = \mathop{\arg\min}_{(w, b)} \mathop{\sum}_{i=1}^m (f(x_i) - y_i)^2 \tag{2}$$


基于均方误差最小化来进行模型求解的方法称为“最小二乘法”（least square method）。
为了方便，我们这里用向量的形式来表达。令 $\vec{w} = 
\left[ \begin{array}{ccc}
w \\
b \end{array} \right]$，$\vec{x}_i = 
\left[ \begin{array}{ccc}
x_i \\
1 \end{array} \right]$，则$f(\vec{x}_i) = \vec{w}^T\vec{x}_i$，于是(2)式可表示为
$$
(\vec{w}^*) = \mathop{\arg\min}_{(w, b)} \mathop{\sum}_{i=1}^m (\vec{w}^T\vec{x}_i - y_i)^2 \tag{3}
$$
令$E(\vec{w}) = \mathop{\sum}_{i=1}^m (\vec{w}^T\vec{x}_i - y_i)^2$，用矩阵的形式可将其写做[2]
$$E(\vec{w}) = (\vec{y} - \bf X\vec{w})^T(\vec{y} - \bf X\vec{w}) \tag{4}$$
其中
$\vec{y} =
\left[
\begin{matrix}
y_1 \\
y_2 \\
\vdots \\
y_m
\end{matrix}
\right],
\bf X =
\left[
\begin{matrix}
x_1 & 1\\
x_2 & 1\\
\vdots & \vdots \\
x_m & 1
\end{matrix}
\right]$

现在就是要求$E(\vec{w})$的最小值，将$E(\vec{w})$对$\vec{w}$求导得
$$\frac{\partial E(\vec{w})}{\partial \vec{w}} = -2\bf X^T(\vec{y} - \bf X\vec{w}) \tag{5}$$

求导过程如下：\
![E(w)对w求导过程)](https://github.com/JuneXia/JuneXia.github.io/raw/hexo/source/images/ml/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%925.jpg)

令
$$\frac{\partial E(\vec{w})}{\partial \vec{w}} = 0 \tag{6}$$
解得
$$\vec{\hat{w}} = (\bf X^T\bf X)^{-1}\bf X^T\vec{y} \tag{7}$$

式(6)中的$\vec{w}$是一个向量，因此式(6)实际上是一个方程组，若式(6)有解(即要求$\bf X^T\bf X$可逆)，则我们称这个方程组为正规方程(Normal Equation) [3][4]。
$\vec{w}$上方的小标记表示，这是当前可以估计出的最优解。从现有数据上估计出的$\vec{w}$可能并不是数据中的真实$\vec{w}$值，所以这里使用了一个“帽”符号来表示它仅仅是$\vec{w}$的一个最优估计。
上述公式中需要对矩阵求逆，因此这个式子只在逆矩阵存在的时候适用。然而并不是所有矩阵的逆都存在，当矩阵逆不存在的时候该怎么办呢？这一点在后面将会讲到。

## 代码示例
regression.py[2]
```python
'''
Created on Jan 8, 2011

@author: Peter
'''
from numpy import *

def loadDataSet(fileName):      #general function to parse tab -delimited floats
    numFeat = len(open(fileName).readline().split('\t')) - 1 #get number of fields 
    dataMat = []; labelMat = []
    fr = open(fileName)
    for line in fr.readlines():
        lineArr =[]
        curLine = line.strip().split('\t')
        for i in range(numFeat):
            lineArr.append(float(curLine[i]))
        dataMat.append(lineArr)
        labelMat.append(float(curLine[-1]))
    return dataMat,labelMat

def standRegres(xArr,yArr):
    xMat = mat(xArr); yMat = mat(yArr).T
    xTx = xMat.T*xMat
    if linalg.det(xTx) == 0.0:
        print("This matrix is singular, cannot do inverse")
        return
    ws = xTx.I * (xMat.T*yMat)
    return ws
```
main.py

```python
import regression
from numpy import *
xArr, yArr = regression.loadDataSet('ex0.txt')
# 用线性回归求得最佳拟合直线的参数
ws = regression.standRegres(xArr, yArr)

xMat = mat(xArr)
yMat = mat(yArr)
# 使用新的ws来计算yHat
yHat = xMat * ws

import matplotlib.pyplot as plt
fig = plt.figure()

#画出数据集散点图
ax = fig.add_subplot(111)
ax.scatter(xMat[:, 1].flatten().A[0], yMat.T[:, 0].flatten().A[0])

xCopy = xMat.copy()
xCopy.sort(0)

yHat = xCopy*ws
# 画出最佳拟合直线
ax.plot(xCopy[:, 1].A, yHat.A)
plt.show()
```

## 如何判断模型的好坏
几乎任一数据集都可以用上述方法建模，那么，如何判断这些模型的好坏呢？[2]比较一下下图中的两个子图，如果在两个数据集上分别作线性回归，将得到完全一样的模型（拟合直线）。显然两个数据是不一样的，那么模型分别在二者上的效果如何？我们当如何比较这些效果的好坏呢？有种方法可以计算预测值yHat序列和真实值y序列的匹配程度，那就是计算这两个序列的相关系数。\
![](https://github.com/JuneXia/JuneXia.github.io/raw/hexo/source/images/ml/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%924.jpg)

## 用正规方程求解时矩阵$X^TX$不可逆时的解决办法
关于不可逆矩阵，我们也称之为奇异或退化矩阵。矩阵不可逆的情况通常有以下几种[3-4.7]：

 1. 特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸这两个特征，例如在预测住房价格时，如果$x_1$是以英尺为尺寸规格计算的房子，$x_2$是以平方米为尺寸规格计算的房子，同时，你也知道1米等于3.28英尺，这样，你的这两个特征值将始终满足约束：$x_1 = x_2 * (3.28)^2$；
 2. 特征数量大于训练集的数量。例如，有m=10个训练样本，n=100个特征，这时候参数θ是一个101维的向量(其中一个是常数项)。尝试从10个训练样本中学得101个参数的值是比较困难的。
对于那些不可逆的矩阵，正规方程方法是不能用的。
那么当矩阵不可逆时该怎么办呢？首先，看特征值里是否有一些多余的特征，如果有多余的就删掉，直到他们不再是多余的为止。其次，我们还可以使用一种叫做正则化的线性代数方法。如此即使你有一个相对较小的训练集，也可使用很多的特征来找到很多合适的参数。

另外也可以采用梯度下降法来求解矩阵不可逆时的最优解(me: 用正规方程得到的是真实解，用梯度下降得到的最优解)。梯度下降与正规方程的比较如下表所示：[3-4.6]

| 梯度下降                       | 正规方程 |     
| -------                       | -----   |
| 需要选择学习率α                 | 不需要   
| 需要多次迭代                    | 一次运算得出
| 当特征数量n大时也能较好适用        | 需要计算$X^TX$，<br>如果特征数量n较大则运算代价大，<br>因为矩阵求逆的计算时间复杂度为$O(n^3)$，<br>通常来说当n<10000时还是可以接受的
| 适用各种类型的模型                | 只适用于线性模型，不适合逻辑回归模型等其他模型


## 参考文献
[1] 周志华. 机器学习
[2] Peter. 机器学习实战
[3] 黄海广. MIT 机器学习教程
[4] https://baike.baidu.com/item/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B/10001812?fr=aladdin

> 最小二乘法可以将误差方程转化为有确定解的代数方程组（其方程式数目正好等于未知数的个数），从而可求解出这些未知参数。这个有确定解的代数方程组称为最小二乘法估计的正规方程（或称为法方程）。
