---
title: 【机器学习笔记2.5】用Softmax回归做二分类（Tensorflow实现）
date: 2017-06-10
tags:
categories: ["机器学习笔记"]
mathjax: true
---

## Softmax回归和逻辑回归的区别
&emsp; 在本节中，我们介绍Softmax回归模型，该模型是logistic回归模型在多分类问题上的推广，在多分类问题中，类标签$y$可以取两个以上的值[1]。当类别数$k=2$时，softmax 回归退化为 logistic 回归。
<!-- more -->

## Softmax回归 vs. k个logistic回归
&emsp; 如果你在开发一个音乐分类的应用，需要对k种类型的音乐进行识别，那么是选择使用 softmax 分类器呢，还是使用 logistic 回归算法建立 k 个独立的二元分类器呢？

&emsp; 这一选择取决于你的类别之间是否互斥，例如，如果你有四个类别的音乐，分别为：古典音乐、乡村音乐、摇滚乐和爵士乐，那么你可以假设每个训练样本只会被打上一个标签（即：一首歌只能属于这四种音乐类型的其中一种），此时你应该使用类别数 k = 4 的softmax回归。（如果在你的数据集中，有的歌曲不属于以上四类的其中任何一类，那么你可以添加一个“其他类”，并将类别数 k 设为5。）

如果你的四个类别如下：人声音乐、舞曲、影视原声、流行歌曲，那么这些类别之间并不是互斥的。例如：一首歌曲可以来源于影视原声，同时也包含人声 。这种情况下，使用4个二分类的 logistic 回归分类器更为合适。这样，对于每个新的音乐作品 ，我们的算法可以分别判断它是否属于各个类别。

现在我们来看一个计算视觉领域的例子，你的任务是将图像分到三个不同类别中。(i) 假设这三个类别分别是：室内场景、户外城区场景、户外荒野场景。你会使用sofmax回归还是 3个logistic 回归分类器呢？ (ii) 现在假设这三个类别分别是室内场景、黑白图片、包含人物的图片，你又会选择 softmax 回归还是多个 logistic 回归分类器呢？

在第一个例子中，三个类别是互斥的，因此更适于选择softmax回归分类器 。而在第二个例子中，建立三个独立的 logistic回归分类器更加合适。 

## 代码示例
```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt


def loadDataSet(file_path):
    dataMat = []
    labelMat = []
    fr = open(file_path)
    for line in fr.readlines():
        lineArr = line.strip().split()
        dataMat.append([float(lineArr[0]), float(lineArr[1])])
        labelMat.append(int(lineArr[2]))
    return dataMat, labelMat

# 加载数据
dataMat, labelMat = loadDataSet('testSet.txt')  # 《机器学习实战》逻辑回归中用的数据集
dataMat = np.mat(dataMat).astype(np.float32)
labelMat = np.mat(labelMat).transpose()

# 制作one-hot格式的label
onehot_list = []
for i in range(len(labelMat)):
    onehot = [0, 0]
    onehot[labelMat[i].item()] = 1
    onehot_list.append(onehot)
labelMat = np.array(onehot_list).astype(np.float32)

class_num = 2
threshold = 1.0e-2

x_data = tf.placeholder("float32", [None, 2])
y_data = tf.placeholder("float32", [None, class_num])
weight = tf.Variable(tf.ones([2, class_num]))
bias = tf.Variable(tf.ones([class_num]))
y_model = tf.nn.softmax(tf.matmul(x_data, weight) + bias)

loss = tf.reduce_sum(tf.pow((y_model - y_data), 2))
train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss)
init = tf.initialize_all_variables()
sess = tf.Session()
sess.run(init)

step = 0
loss_buf = []
train_num = 70  # 一共就100个样本，拿出70个出来用于训练，剩下的30个用于测试
for _ in range(100):
    for data, label in zip(dataMat[0:train_num, :], labelMat[0:train_num, :]):
        label = label.reshape([1, 2])
        sess.run(train_step, feed_dict={x_data: data, y_data: label})

        step += 1
        '''
        if step % 10 == 0:
            print(step, sess.run(weight).flatten(), sess.run(bias).flatten())
        '''

    loss_val = sess.run(loss, feed_dict={x_data: data, y_data: label})
    print('loss_val = ', loss_val)
    loss_buf.append(loss_val)
    if loss_val <= threshold:
        flag = 0
    #print('weight = ', weight.eval(sess))

# 测试准确率
correct_prediction = tf.equal(tf.argmax(y_model, 1), tf.argmax(y_data, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
print(sess.run(accuracy, feed_dict={x_data: dataMat[train_num+1:100, :], y_data: labelMat[train_num+1:100, :]}))
sess.close()

# 画出loss曲线
loss_ndarray = np.array(loss_buf)
loss_size = np.arange(len(loss_ndarray))
plt.plot(loss_size, loss_ndarray, 'b+', label='loss')
plt.show()
print('end')
```

loss曲线：
![enter image description here](https://lh3.googleusercontent.com/-H6IdPXaqq9w/W1weFSXmnII/AAAAAAAAADc/1yO9gC-BkHkEZbo2eyg-Tb-pxevsnY1KgCLcBGAs/s0/%25E9%2580%25BB%25E8%25BE%2591%25E5%259B%259E%25E5%25BD%25923_softmax.png "逻辑回归3_softmax.png")

疑问：怎样画出Softmax回归得到的分类直线？
答：会提出这样的问题应该是Softmax回归和逻辑回归的概念还没弄清楚。
（me）在Softmax回归中，输出结果是one-hot形式的向量，向量的每一维的输出非0即1，根据Softmax回归的假设模型$h_{\theta}(x^{(i)})$可知，每一维的参数${\theta}_j$都不相同，所以也不能像逻辑回归中那样画出一条分类直线了。


## 参考文献
[1] [Softmax回归](http://deeplearning.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92)
