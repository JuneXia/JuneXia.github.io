---
title: 【机器学习笔记1.2】线性回归之梯度下降求解
date: 2017-05-19
tags:
categories: ["机器学习笔记"]
mathjax: true
---

> 原文参考文献[1]


## 线性模型
&emsp; 首先，我们把生物神经元(Neuron)的模型抽象为如图 2.2(a)所示的数学结构：神经元输入向量$\mathbf{x} = [x_1, x_2, x_3, ..., x_n ]^T$ ,经过函数映射:$f_θ:x→y$后得到输出y，其中θ为函数f自身的参数。考虑一种简化的情况，即线性变换:$f(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + b$，展开为标量形式:$f(\mathbf{x}) = w_1 x_1 + w_2 x_2 + w_3 x_3 + ⋯ + w_n x_n + b$.
<!-- more -->

上述计算逻辑可以通过图 2.2(b)直观地展现
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://github.com/JuneXia/JuneXia.github.io/raw/hexo/source/images/ml/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%921.jpg">
    <div style="color:orange; 
    display: inline-block;
    color: #999;
    padding: 2px;">图1. 神经元数学模型</div>
</center>


&emsp; 参数$\theta = \{w_1, w_2, w_3, ..., w_n, b\}$确定了神经元的状态,通过固定$θ$参数即可确定此神经元的处理逻辑。当神经元输入节点数n = 1(单输入)时,神经元数学模型可进一步简化为:
$$y = wx + b$$
此时我们可以绘制出神经元的输出y和输入x的变化趋势,如图 2.3 所示,随着输入信号x的增加,输出电平y也随之线性增加,其中w参数可以理解为直线的斜率(Slope)，b 参数为直线的偏置(Bias)。
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://github.com/JuneXia/JuneXia.github.io/raw/hexo/source/images/ml/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%922.jpg">
    <div style="color:orange; border-bottom: 
    display: inline-block;
    color: #999;
    padding: 2px;">图2. 单输入神经元线性模型</div>
</center>


&emsp; 对于某个神经元来说,x和y的映射关系$f_{w,b}$ 是未知但确定的。两点即可确定一条直
线,为了估计w和b的值,我们只需从图 2.3 中直线上采样任意 2 个数据点:
$(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)})$即可,其中上标表示数据点编号:
$$y^{(1)} = wx^{(1)} + b \\
y^{(2)} = wx^{(2)} + b$$


当$(x^{(1)}, y^{(1)}) ≠ (x^{(2)}, y^{(2)})$时,通过求解上式便可计算出w和b的值。考虑某个具体的例子:
$x^{(1)} = 1, y^{(1)} = 1.567 , x^{(2)} = 2, y^{(2)} = 3.043$, 代入上式中可得:
$$1.567 = w ∙ 1 + b \\
3.043= w∙2+b$$
这就是我们初中时代学习过的二元一次方程组,通过消元法可以轻松计算出w和b的解析
解:$w=1.477, b=0.089$

&emsp; 可以看到,只需要观测两个不同数据点,就可完美求解单输入线性神经元模型的参数,对于N输入的线性神经元模型,只需要采样N + 1组不同数据点即可,似乎线性神经元模型可以得到完美解决。那么上述方法存在什么问题呢?考虑对于任何采样点,都有可能存在观测误差,我们假设观测误差变量 $ε$ 属于均值为 $μ$，方差为 $σ^2$ 的正态分布(Normal Distribution,或高斯分布,Gaussian Distribution):$\mathcal{N}(μ, σ^2)$,则采样到的样本符合:
$$y = wx + b + ε, \quad ε \in \mathcal{N}(μ, σ^2)$$
一旦引入观测误差后,即使简单如线性模型,如果仅采样两个数据点,可能会带来较大估计偏差。如图 2.4 所示,图中的数据点均带有观测误差,如果基于蓝色矩形块的两个数据点进行估计,则计算出的蓝色虚线与真实橙色直线存在较大偏差。为了减少观测误差引入的估计偏差,可以通过采样多组数据样本集合$\mathbb{D} = \{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(n)}, y^{(n)})\}$,然后找出一条“最好”的直线,使得它尽可能地让所有采样点到该直线的误差(Error,或损失 Loss)之和最小。
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://github.com/JuneXia/JuneXia.github.io/raw/hexo/source/images/ml/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_%E5%B8%A6%E8%A7%82%E6%B5%8B%E8%AF%AF%E5%B7%AE%E7%9A%84%E4%BC%B0%E8%AE%A1%E6%A8%A1%E5%9E%8B.jpg">
    <div style="color:orange; border-bottom: 
    display: inline-block;
    color: #999;
    padding: 2px;">图3. 带观测误差的估计模型</div>
</center>


也就是说,由于观测误差ε的存在,当我们采集了多个数据点D时,可能不存在一条直线完美的穿过所有采样点。退而求其次,我们希望能找到一条比较“好”的位于采样点中间的直线。那么怎么衡量“好”与“不好”呢?一个很自然的想法就是,求出当前模型的所有采样点上的预测值$wx^{(i)} + b$与真实值 $y^{(i)}$ 之间的差的平方和作为总误差$L$:
$$L = \frac{1}{n} \sum_{i=1}^{n}(wx^{(i)} + b - y^{(i)})^2$$

然后搜索一组参数$w^∗, b^∗$ 使得L最小,对应的直线就是我们要寻找的最优直线:
$$w^*, b^* = argmin_{(w,b)} \frac{1}{n} \sum_{i=1}^{n}(wx^{(i)} + b - y^{(i)})^2$$

其中n表示采样点的个数。这种误差计算方法称为均方误差(Mean Squared Error,简称MSE)。


## 优化方法
&emsp; 现在来小结一下上述方案:我们需要找出最优参数(Optimal Parameter) $w^∗$ 和 $b^∗$，使得输入和输出满足线性关系$y^{(i)} = wx^{(i)} + b, i ∈ [1, n]$。但是由于观测误差ε的存在,需要通过采样足够多组的数据样本组成的数据集(Dataset):$\mathbb{D} = \{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(n)}, y^{(n)})\}$,找到一组最优的参数 $w^∗$ 和 $b^∗$ 使得均方误差 $L = \frac{1}{n} \sum_{i=1}^{n}(wx^{(i)} + b - y^{(i)})^2$ 最小。

&emsp; 对于单输入的神经元模型,只需要两个样本,就能通过消元法求出方程组的精确解, 这种通过严格的公式推导出的精确解称为解析解(Closed-form Solution)。但是对于多个数据点 $(n ≫ 2)$ 的情况,这时很有可能不存在解析解,我们只能借助数值方法去优化(Optimize)出一个近似的数值解(Numerical Solution)。为什么叫作优化?这是因为计算机的计算速度非常快,我们可以借助强大的计算能力去多次“搜索”和“试错”,从而一步步降低误差L。最简单的优化方法就是暴力搜索或随机试验,比如要找出最合适的$w^∗$ 和 $b^∗$ ,我们就可以从(部分)实数空间中随机采样任意的w和b,并计算出对应模型的误差值$L$,然后从测试过的${L}$中挑出最好的$L^∗$ ,它所对应的w和b就可以作为我们要找的最优$w^∗$ 和 $b^∗$ 。

&emsp; 这种算法固然简单直接,但是面对大规模、高维度数据的优化问题时计算效率极低,基本不可行。梯度下降算法(Gradient Descent)是神经网络训练中最常用的优化算法,配合强大的图形处理芯片 GPU(Graphics Processing Unit)的并行加速能力,非常适合优化海量数据的神经网络模型,自然也适合优化我们这里的神经元线性模型。这里先简单地应用梯度下降算法,用于解决神经元模型预测的问题。由于梯度下降算法是深度学习的核心算法,我们将在第 7 章非常详尽地推导梯度下降算法在神经网络中的应用,这里先给读者第一印
象。


&emsp; 我们在高中时代学过导数(Derivative)的概念,如果要求解一个函数的极大、极小值,可以简单地令导数函数为 $\theta$，求出对应的自变量点(称为驻点),再检验驻点类型即可。以函数 $f(x) = x^2 \centerdot sin(x)$ 为例，我们绘制出函数及其导数在 $x ∈ [−1 ,1]$ 区间曲线,其中蓝色实线为 $f(x)$, 黄色虚线为 $\frac{df(x)}{dx}$,如图 2.5 所示。可以看出,函数导数(虚线)为 0 的点即为 $f(x)$ 的驻点, 函数的极大值和极小值点均出现在驻点中。
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://github.com/JuneXia/JuneXia.github.io/raw/hexo/source/images/ml/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_%E5%87%BD%E6%95%B0%E5%8F%8A%E5%85%B6%E5%AF%BC%E6%95%B0.jpg">
    <div style="color:orange; border-bottom: 
    display: inline-block;
    color: #999;
    padding: 2px;">图4. 函数及其导数</div>
</center>

&emsp; 函数的梯度(Gradient)定义为函数对各个自变量的偏导数(Partial Derivative)组成的向量。考虑 3 维函数 $z = f(x, y)$, 函数对自变量x的偏导数记为 $\frac{\partial z}{\partial x}$,函数对自变量y的偏导数
记为 $\frac{\partial z}{\partial y}$,则梯度$∇f$为向量$(\frac{\partial z}{\partial x}$, $\frac{\partial z}{\partial y})$。我们通过一个具体的函数来感受梯度的性质,如图 2.6 所示,$f(x, y) = −(cos^2x + cos^2y)^2$,图中xy平面的红色箭头的长度表示梯度向量的模,箭头的方向表示梯度向量的方向。可以看到,箭头的方向总是指向当前位置函数值增速最大的方向,函数曲面越陡峭,箭头的长度也就越长,梯度的模也越大。
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://github.com/JuneXia/JuneXia.github.io/raw/hexo/source/images/ml/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_%E5%87%BD%E6%95%B0%E5%8F%8A%E5%85%B6%E6%A2%AF%E5%BA%A6%E5%90%91%E9%87%8F.jpg">
    <br>
    <div style="color:orange; border-bottom: 
    display: inline-block;
    color: #999;
    padding: 2px;">图5. 函数及其梯度向量</div>
</center>


![](chart/线性回归3.jpg)

下面主要讨论梯度 $(\frac{\partial z}{\partial x}$, $\frac{\partial z}{\partial y})$ 的计算方式：

![](https://github.com/JuneXia/JuneXia.github.io/raw/hexo/source/images/ml/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A61.jpg)
![](https://github.com/JuneXia/JuneXia.github.io/raw/hexo/source/images/ml/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92_%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A62.jpg)



# 参考文献
[1] TensorFlow深度学习 龙良曲

参见 [【机器学习笔记1.2】线性回归之梯度下降法求解](https://blog.csdn.net/yahstudio/article/details/80573592)
