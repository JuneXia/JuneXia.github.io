<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://yoursite.com').hostname,
    root: '/',
    scheme: 'Gemini',
    version: '7.7.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="其实，我是一个搬运工！">
<meta property="og:type" content="website">
<meta property="og:title" content="Paper搬运菌">
<meta property="og:url" content="http://yoursite.com/page/6/index.html">
<meta property="og:site_name" content="Paper搬运菌">
<meta property="og:description" content="其实，我是一个搬运工！">
<meta property="og:locale" content="en_US">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/page/6/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false
  };
</script>

  <title>Paper搬运菌</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Paper搬运菌</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/26/dl/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02.2.3%E3%80%91AlexNet%E8%AE%AD%E7%BB%8317flowers/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="其实，我是一个搬运工！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Paper搬运菌">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/01/26/dl/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02.2.3%E3%80%91AlexNet%E8%AE%AD%E7%BB%8317flowers/" class="post-title-link" itemprop="url">【深度学习笔记2.2.3】AlexNet训练17flowers</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-01-26 17:28:05" itemprop="dateCreated datePublished" datetime="2018-01-26T17:28:05+08:00">2018-01-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-22 14:06:45" itemprop="dateModified" datetime="2020-01-22T14:06:45+08:00">2020-01-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习笔记</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>本文介绍使用AlexNet做17flowers的分类任务，代码参考文献[1]，数据集17flowers来自文献[2]，预训练模型bvlc_alexnet.npy来自文献[4]。<br>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2018/01/26/dl/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02.2.3%E3%80%91AlexNet%E8%AE%AD%E7%BB%8317flowers/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/09/dl/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02.2.2%E3%80%91AlexNet%E8%AE%AD%E7%BB%83mnist%EF%BC%88LRN%E3%80%81BN%E7%AD%89%E5%8E%9F%E7%90%86%E5%BA%94%E5%BD%93%E6%80%BB%E7%BB%93%E5%87%BA%E6%9D%A5%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="其实，我是一个搬运工！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Paper搬运菌">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2018/01/09/dl/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02.2.2%E3%80%91AlexNet%E8%AE%AD%E7%BB%83mnist%EF%BC%88LRN%E3%80%81BN%E7%AD%89%E5%8E%9F%E7%90%86%E5%BA%94%E5%BD%93%E6%80%BB%E7%BB%93%E5%87%BA%E6%9D%A5%EF%BC%89/" class="post-title-link" itemprop="url">【深度学习笔记2.2.2】AlexNet训练mnist（LRN、BN等原理应当总结出来）</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-01-09 17:28:05" itemprop="dateCreated datePublished" datetime="2018-01-09T17:28:05+08:00">2018-01-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-22 13:25:27" itemprop="dateModified" datetime="2020-01-22T13:25:27+08:00">2020-01-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习笔记</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <a id="more"></a>
<h2 id="实验1：AlexNet-Tensorflow-实现"><a href="#实验1：AlexNet-Tensorflow-实现" class="headerlink" title="实验1：AlexNet Tensorflow 实现"></a>实验1：AlexNet Tensorflow 实现</h2><p>代码示例如下(详见文献[2]AlexNet1.py)：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import cv2</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">datapath &#x3D; &#39;&#x2F;home&#x2F;pathto&#x2F;res&#x2F;MNIST_data&#39;</span><br><span class="line">mnist_data_set &#x3D; input_data.read_data_sets(datapath, validation_size&#x3D;0, one_hot&#x3D;True)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def image_shape_scale(batch_xs):</span><br><span class="line">    images &#x3D; np.reshape(batch_xs, [batch_xs.shape[0], 28, 28])</span><br><span class="line">    imlist &#x3D; []</span><br><span class="line">    [imlist.append(cv2.resize(img, (227, 227))) for img in images]</span><br><span class="line">    images &#x3D; np.array(imlist)</span><br><span class="line">    # cv2.imwrite(&#39;scale1.jpg&#39;, images[0]*200)</span><br><span class="line">    # cv2.imwrite(&#39;scale2.jpg&#39;, images[1]*200)</span><br><span class="line">    # batch_xs &#x3D; np.reshape(images, [batch_xs.shape[0], 227 * 227 * input_image_channel])</span><br><span class="line">    batch_xs &#x3D; np.reshape(images, [batch_xs.shape[0], 227, 227, input_image_channel])</span><br><span class="line">    return batch_xs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">input_image_channel &#x3D; 1</span><br><span class="line">learning_rate &#x3D; 1e-4</span><br><span class="line">training_epoch &#x3D; 50</span><br><span class="line">batch_size &#x3D; 200</span><br><span class="line">n_classes &#x3D; 10</span><br><span class="line">n_fc1 &#x3D; 6*6*256</span><br><span class="line">n_fc2 &#x3D; 4096</span><br><span class="line">n_fc3 &#x3D; 4096</span><br><span class="line">dropout_rate &#x3D; 0.5</span><br><span class="line"></span><br><span class="line">X &#x3D; tf.placeholder(tf.float32, [None, 227, 227, input_image_channel])</span><br><span class="line">y &#x3D; tf.placeholder(tf.float32, [None, n_classes])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">W_conv &#x3D; &#123;</span><br><span class="line">    &#39;conv1&#39;: tf.Variable(tf.truncated_normal([11, 11, input_image_channel, 96])),</span><br><span class="line">    &#39;conv2&#39;: tf.Variable(tf.truncated_normal([5, 5, 96, 256])),</span><br><span class="line">    &#39;conv3&#39;: tf.Variable(tf.truncated_normal([3, 3, 256, 384])),</span><br><span class="line">    &#39;conv4&#39;: tf.Variable(tf.truncated_normal([3, 3, 384, 384])),</span><br><span class="line">    &#39;conv5&#39;: tf.Variable(tf.truncated_normal([3, 3, 384, 256])),</span><br><span class="line">    &#39;fc1&#39;: tf.Variable(tf.truncated_normal([n_fc1, n_fc2])),</span><br><span class="line">    &#39;fc2&#39;: tf.Variable(tf.truncated_normal([n_fc2, n_fc3])),</span><br><span class="line">    &#39;output&#39;: tf.Variable(tf.truncated_normal([n_fc3, n_classes]))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">b_conv &#x3D; &#123;</span><br><span class="line">    &#39;conv1&#39;: tf.Variable(tf.constant(0.1, dtype&#x3D;tf.float32, shape&#x3D;[96])),</span><br><span class="line">    &#39;conv2&#39;: tf.Variable(tf.constant(0.1, dtype&#x3D;tf.float32, shape&#x3D;[256])),</span><br><span class="line">    &#39;conv3&#39;: tf.Variable(tf.constant(0.1, dtype&#x3D;tf.float32, shape&#x3D;[384])),</span><br><span class="line">    &#39;conv4&#39;: tf.Variable(tf.constant(0.1, dtype&#x3D;tf.float32, shape&#x3D;[384])),</span><br><span class="line">    &#39;conv5&#39;: tf.Variable(tf.constant(0.1, dtype&#x3D;tf.float32, shape&#x3D;[256])),</span><br><span class="line">    &#39;fc1&#39;: tf.Variable(tf.constant(0.1, dtype&#x3D;tf.float32, shape&#x3D;[n_fc2])),</span><br><span class="line">    &#39;fc2&#39;: tf.Variable(tf.constant(0.1, dtype&#x3D;tf.float32, shape&#x3D;[n_fc3])),</span><br><span class="line">    &#39;output&#39;: tf.Variable(tf.constant(0.1, dtype&#x3D;tf.float32, shape&#x3D;[n_classes]))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">X_image &#x3D; tf.reshape(X, [-1, 227, 227, input_image_channel])</span><br><span class="line"></span><br><span class="line"># 卷积层1</span><br><span class="line">conv1 &#x3D; tf.nn.conv2d(X_image, W_conv[&#39;conv1&#39;], strides&#x3D;[1, 4, 4, 1], padding&#x3D;&#39;VALID&#39;)</span><br><span class="line">conv1 &#x3D; tf.nn.bias_add(conv1, b_conv[&#39;conv1&#39;])</span><br><span class="line">conv1 &#x3D; tf.nn.relu(conv1)</span><br><span class="line">conv1 &#x3D; tf.nn.local_response_normalization(conv1, depth_radius&#x3D;2, alpha&#x3D;2e-05, beta&#x3D;0.75, bias&#x3D;1.0)</span><br><span class="line"># 此时 conv1.shape &#x3D; [-1, 55, 55, 96]</span><br><span class="line"></span><br><span class="line"># 池化层1</span><br><span class="line">pool1 &#x3D; tf.nn.max_pool(conv1, ksize&#x3D;[1, 3, 3, 1], strides&#x3D;[1, 2, 2, 1], padding&#x3D;&#39;VALID&#39;)</span><br><span class="line"># pool1.shape &#x3D; [-1, 27, 27, 96]</span><br><span class="line"></span><br><span class="line"># 卷积层2</span><br><span class="line">conv2 &#x3D; tf.nn.conv2d(pool1, W_conv[&#39;conv2&#39;], strides&#x3D;[1, 1, 1, 1], padding&#x3D;&#39;SAME&#39;)</span><br><span class="line">conv2 &#x3D; tf.nn.bias_add(conv2, b_conv[&#39;conv2&#39;])</span><br><span class="line">conv2 &#x3D; tf.nn.relu(conv2)</span><br><span class="line">conv2 &#x3D; tf.nn.local_response_normalization(conv2, depth_radius&#x3D;2, alpha&#x3D;2e-05, beta&#x3D;0.75, bias&#x3D;1.0)</span><br><span class="line"># 此时 conv2.shape &#x3D; [-1, 27, 27, 256]</span><br><span class="line"></span><br><span class="line"># 池化层2</span><br><span class="line">pool2 &#x3D; tf.nn.max_pool(conv2, ksize&#x3D;[1, 3, 3, 1], strides&#x3D;[1, 2, 2, 1], padding&#x3D;&#39;VALID&#39;)</span><br><span class="line"># 此时 pool2.shape &#x3D; [-1, 13, 13, 256]</span><br><span class="line"></span><br><span class="line"># 卷积层3</span><br><span class="line">conv3 &#x3D; tf.nn.conv2d(pool2, W_conv[&#39;conv3&#39;], strides&#x3D;[1, 1, 1, 1], padding&#x3D;&#39;SAME&#39;)</span><br><span class="line">conv3 &#x3D; tf.nn.bias_add(conv3, b_conv[&#39;conv3&#39;])</span><br><span class="line">conv3 &#x3D; tf.nn.relu(conv3)</span><br><span class="line"># 此时 conv3.shape &#x3D; [-1, 13, 13, 384]</span><br><span class="line"></span><br><span class="line"># 卷积层4</span><br><span class="line">conv4 &#x3D; tf.nn.conv2d(conv3, W_conv[&#39;conv4&#39;], strides&#x3D;[1, 1, 1, 1], padding&#x3D;&#39;SAME&#39;)</span><br><span class="line">conv4 &#x3D; tf.nn.bias_add(conv4, b_conv[&#39;conv4&#39;])</span><br><span class="line">conv4 &#x3D; tf.nn.relu(conv4)</span><br><span class="line"># 此时 conv4.shape &#x3D; [-1, 13, 13, 384]</span><br><span class="line"></span><br><span class="line"># 卷积层5</span><br><span class="line">conv5 &#x3D; tf.nn.conv2d(conv4, W_conv[&#39;conv5&#39;], strides&#x3D;[1, 1, 1, 1], padding&#x3D;&#39;SAME&#39;)</span><br><span class="line">conv5 &#x3D; tf.nn.bias_add(conv5, b_conv[&#39;conv5&#39;])</span><br><span class="line">conv5 &#x3D; tf.nn.relu(conv5)</span><br><span class="line"># 此时 conv5.shape &#x3D; [-1, 13, 13, 256]</span><br><span class="line"></span><br><span class="line"># 池化层5</span><br><span class="line">pool5 &#x3D; tf.nn.max_pool(conv5, ksize&#x3D;[1, 3, 3, 1], strides&#x3D;[1, 2, 2, 1], padding&#x3D;&#39;VALID&#39;)</span><br><span class="line"># 此时pool5.shape &#x3D; [-1, 6, 6, 256]</span><br><span class="line"></span><br><span class="line"># 全连接层1</span><br><span class="line">reshape &#x3D; tf.reshape(pool5, [-1, n_fc1])</span><br><span class="line"># 此时reshape.shape &#x3D; [-1, 9216]</span><br><span class="line">fc1 &#x3D; tf.add(tf.matmul(reshape, W_conv[&#39;fc1&#39;]), b_conv[&#39;fc1&#39;])</span><br><span class="line">fc1 &#x3D; tf.nn.relu(fc1)</span><br><span class="line">fc1 &#x3D; tf.nn.dropout(fc1, dropout_rate)</span><br><span class="line"># 此时fc1.shape &#x3D; [-1, 4096]</span><br><span class="line"></span><br><span class="line"># 全连接层2</span><br><span class="line">fc2 &#x3D; tf.add(tf.matmul(fc1, W_conv[&#39;fc2&#39;]), b_conv[&#39;fc2&#39;])</span><br><span class="line">fc2 &#x3D; tf.nn.relu(fc2)</span><br><span class="line">fc2 &#x3D; tf.nn.dropout(fc2, dropout_rate)</span><br><span class="line"># 此时fc2.shape &#x3D; [-1, 4096]</span><br><span class="line"></span><br><span class="line"># 输出层</span><br><span class="line">output &#x3D; tf.add(tf.matmul(fc2, W_conv[&#39;output&#39;]), b_conv[&#39;output&#39;])</span><br><span class="line"># 此时output.shape &#x3D; [-1. 10]</span><br><span class="line"></span><br><span class="line"># 定义交叉熵损失函数（有两种方法）：</span><br><span class="line"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line"># 方法1： 自己实现交叉熵</span><br><span class="line">y_output &#x3D; tf.nn.softmax(output)  # 对网络最后一层的输出做softmax, 这通常是求取输出属于某一类的概率</span><br><span class="line">cross_entropy &#x3D; -tf.reduce_sum(y * tf.log(y_output))  # 用softmax的输出向量和样本的实际标签做一个交叉熵.</span><br><span class="line">loss &#x3D; tf.reduce_mean(cross_entropy)  # 对交叉熵求均值就是loss</span><br><span class="line"># loss &#x3D; -tf.reduce_mean(y * tf.log(y_output))  # 交叉熵本应是一个向量，但tf.reduce_mean可以直接求取tensor所有维度的和，所以这里可以用tf.reduce_mean一句代替上述三步。</span><br><span class="line"></span><br><span class="line"># 方法2：使用tensorflow自带的tf.nn.softmax_cross_entropy_with_logits函数实现交叉熵</span><br><span class="line"># loss &#x3D; tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits&#x3D;output, labels&#x3D;y))</span><br><span class="line"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_step &#x3D; tf.train.GradientDescentOptimizer(learning_rate&#x3D;learning_rate).minimize(loss)</span><br><span class="line"></span><br><span class="line"># 评估模型</span><br><span class="line">correct_pred &#x3D; tf.equal(tf.argmax(y_output, 1), tf.argmax(y, 1))</span><br><span class="line">accuracy &#x3D; tf.reduce_mean(tf.cast(correct_pred, tf.float32))</span><br><span class="line"></span><br><span class="line">init &#x3D; tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">loss_buf &#x3D; []</span><br><span class="line">accuracy_buf &#x3D; []</span><br><span class="line">with tf.device(&quot;&#x2F;gpu:0&quot;):</span><br><span class="line">    # with tf.Graph().as_default():</span><br><span class="line">    gpu_options &#x3D; tf.GPUOptions(per_process_gpu_memory_fraction&#x3D;1.0)</span><br><span class="line">    config &#x3D; tf.ConfigProto(gpu_options&#x3D;gpu_options, allow_soft_placement&#x3D;True, log_device_placement&#x3D;True)</span><br><span class="line">    # config &#x3D; tf.ConfigProto(allow_soft_placement&#x3D;True, log_device_placement&#x3D;False)</span><br><span class="line">    with tf.Session(config&#x3D;config) as sess:</span><br><span class="line">        sess.run(init)</span><br><span class="line"></span><br><span class="line">        total_batch &#x3D; mnist_data_set.train.num_examples &#x2F;&#x2F; batch_size</span><br><span class="line">        for i in range(training_epoch):</span><br><span class="line">            for iteration in range(total_batch):</span><br><span class="line">                batch_xs, batch_ys &#x3D; mnist_data_set.train.next_batch(batch_size)</span><br><span class="line">                batch_xs &#x3D; image_shape_scale(batch_xs)</span><br><span class="line"></span><br><span class="line">                sess.run(train_step, feed_dict&#x3D;&#123;X: batch_xs, y: batch_ys&#125;)</span><br><span class="line">                test_accuracy &#x3D; sess.run(accuracy, feed_dict&#x3D;&#123;X: batch_xs, y: batch_ys&#125;)</span><br><span class="line">                print(&quot;step &#123;&#125;, iteration &#123;&#125;, training accuracy &#123;&#125;&quot;.format(i, iteration, test_accuracy))</span><br><span class="line"></span><br><span class="line">            batch_xs, batch_ys &#x3D; mnist_data_set.test.images[0:1000, :], mnist_data_set.test.labels[0:1000, :]</span><br><span class="line">            batch_xs &#x3D; image_shape_scale(batch_xs)</span><br><span class="line"></span><br><span class="line">            loss_val &#x3D; sess.run(loss, feed_dict&#x3D;&#123;X: batch_xs, y: batch_ys&#125;)</span><br><span class="line">            test_accuracy &#x3D; sess.run(accuracy, feed_dict&#x3D;&#123;X: batch_xs, y: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">            loss_buf.append(loss_val)</span><br><span class="line">            accuracy_buf.append(test_accuracy)</span><br><span class="line">            print(&quot;step &#123;&#125;, loss &#123;&#125;, testing accuracy &#123;&#125;&quot;.format(i, loss_val, test_accuracy))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 画出准确率曲线</span><br><span class="line">accuracy_ndarray &#x3D; np.array(accuracy_buf)</span><br><span class="line">accuracy_size &#x3D; np.arange(len(accuracy_ndarray))</span><br><span class="line">plt.plot(accuracy_size, accuracy_ndarray, &#39;b+&#39;, label&#x3D;&#39;accuracy&#39;)</span><br><span class="line"></span><br><span class="line">loss_ndarray &#x3D; np.array(loss_buf)</span><br><span class="line">loss_size &#x3D; np.arange(len(loss_ndarray))</span><br><span class="line">plt.plot(loss_size, loss_ndarray, &#39;r*&#39;, label&#x3D;&#39;loss&#39;)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"># 保存loss和测试准确率到csv文件</span><br><span class="line">with open(&#39;AlexNet.csv&#39;, &#39;w&#39;) as fid:</span><br><span class="line">    for loss, acc in zip(loss_buf, accuracy_buf):</span><br><span class="line">        strText &#x3D; str(loss) + &#39;,&#39; + str(acc) + &#39;\n&#39;</span><br><span class="line">        fid.write(strText)</span><br><span class="line">fid.close()</span><br><span class="line"></span><br><span class="line">print(&#39;end&#39;)</span><br></pre></td></tr></table></figure></p>
<p>训练步骤打印结果如下：<br>step 0, loss nan, testing accuracy 0.08500000089406967<br>step 1, loss nan, testing accuracy 0.08500000089406967<br>step 2, loss nan, testing accuracy 0.08500000089406967<br>step 3, loss nan, testing accuracy 0.08500000089406967<br>… …<br>可以看到，模型无法收敛。</p>
<h2 id="实验2：通过改变权重初始化方法进行优化"><a href="#实验2：通过改变权重初始化方法进行优化" class="headerlink" title="实验2：通过改变权重初始化方法进行优化"></a>实验2：通过改变权重初始化方法进行优化</h2><p>后参考文献[1]代码，排查到模型无法收敛的原因可能是weights、biases的初始化不当。上述代码中，tf.truncated_normal默认使用均值mean为0、标准差stddev为1的截断正态分布来初始化W_conv和b_conv。<br>这里我们可以对上述代码AlexNet1.py做如下修改(其他不变，此部分完整代码详见文献[2]AlexNet2.py)：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">W_conv &#x3D; &#123;</span><br><span class="line">    &#39;conv1&#39;: tf.Variable(tf.truncated_normal([11, 11, input_image_channel, 96], mean&#x3D;0, stddev&#x3D;0.01)),</span><br><span class="line">    &#39;conv2&#39;: tf.Variable(tf.truncated_normal([5, 5, 96, 256], mean&#x3D;0, stddev&#x3D;0.01)),</span><br><span class="line">    &#39;conv3&#39;: tf.Variable(tf.truncated_normal([3, 3, 256, 384], mean&#x3D;0, stddev&#x3D;0.01)),</span><br><span class="line">    &#39;conv4&#39;: tf.Variable(tf.truncated_normal([3, 3, 384, 384], mean&#x3D;0, stddev&#x3D;0.01)),</span><br><span class="line">    &#39;conv5&#39;: tf.Variable(tf.truncated_normal([3, 3, 384, 256], mean&#x3D;0, stddev&#x3D;0.01)),</span><br><span class="line">    &#39;fc1&#39;: tf.Variable(tf.truncated_normal([n_fc1, n_fc2], mean&#x3D;0, stddev&#x3D;0.01)),</span><br><span class="line">    &#39;fc2&#39;: tf.Variable(tf.truncated_normal([n_fc2, n_fc3], mean&#x3D;0, stddev&#x3D;0.01)),</span><br><span class="line">    &#39;output&#39;: tf.Variable(tf.truncated_normal([n_fc3, n_classes], mean&#x3D;0, stddev&#x3D;0.01))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">b_conv &#x3D; &#123;</span><br><span class="line">    &#39;conv1&#39;: tf.Variable(tf.truncated_normal([96], mean&#x3D;0.005, stddev&#x3D;0.1)),</span><br><span class="line">    &#39;conv2&#39;: tf.Variable(tf.truncated_normal([256], mean&#x3D;0.005, stddev&#x3D;0.1)),</span><br><span class="line">    &#39;conv3&#39;: tf.Variable(tf.truncated_normal([384], mean&#x3D;0.005, stddev&#x3D;0.1)),</span><br><span class="line">    &#39;conv4&#39;: tf.Variable(tf.truncated_normal([384], mean&#x3D;0.005, stddev&#x3D;0.1)),</span><br><span class="line">    &#39;conv5&#39;: tf.Variable(tf.truncated_normal([256], mean&#x3D;0.005, stddev&#x3D;0.1)),</span><br><span class="line">    &#39;fc1&#39;: tf.Variable(tf.truncated_normal([n_fc2], mean&#x3D;0.005, stddev&#x3D;0.1)),</span><br><span class="line">    &#39;fc2&#39;: tf.Variable(tf.truncated_normal([n_fc3], mean&#x3D;0.005, stddev&#x3D;0.1)),</span><br><span class="line">    &#39;output&#39;: tf.Variable(tf.truncated_normal([n_classes], mean&#x3D;0.005, stddev&#x3D;0.1))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>训练步骤打印结果如下：<br>step 1, loss 2299.4792, testing accuracy 0.122<br>step 2, loss 2299.8113, testing accuracy 0.117<br>… …<br>step 11, loss 2293.2812, testing accuracy 0.125<br>step 12, loss 1955.1497, testing accuracy 0.391<br>step 13, loss 353.29596, testing accuracy 0.866<br>step 14, loss 146.65419, testing accuracy 0.954<br>… …<br>step 44, loss 36.49175, testing accuracy 0.99<br>step 45, loss 22.933325, testing accuracy 0.986<br>step 46, loss 35.3011, testing accuracy 0.99<br>step 47, loss nan, testing accuracy 0.085<br>step 48, loss nan, testing accuracy 0.085<br>step 49, loss nan, testing accuracy 0.085<br>step 50, loss nan, testing accuracy 0.085<br>可以看到，此时算法模型可以正常收敛，但在中途会突然梯度爆炸。</p>
<h2 id="实验3：中途调低学习率避免梯度爆炸"><a href="#实验3：中途调低学习率避免梯度爆炸" class="headerlink" title="实验3：中途调低学习率避免梯度爆炸"></a>实验3：中途调低学习率避免梯度爆炸</h2><p>为了解决梯度爆炸，我们可以在梯度下降接近低谷附近时调低学习率。这里需要继续在上述代码AlexNet2.py的基础上做些改进，主要是将学习率设为占位符变量，在训练的过程中动态设置学习率。主要改进如下(此部分完整代码详见文献[2]AlexNet3.py)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">learning_rate_holder &#x3D; tf.placeholder(tf.float32)</span><br><span class="line">train_step &#x3D; tf.train.GradientDescentOptimizer(learning_rate&#x3D;learning_rate_holder).minimize(loss)</span><br><span class="line"></span><br><span class="line">... ...</span><br><span class="line"></span><br><span class="line">with tf.Session(config&#x3D;config) as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    total_batch &#x3D; mnist_data_set.train.num_examples &#x2F;&#x2F; batch_size</span><br><span class="line">    for i in range(training_epoch):</span><br><span class="line">        for iteration in range(total_batch):</span><br><span class="line">            ... ...</span><br><span class="line"></span><br><span class="line">            if i &lt; 30:</span><br><span class="line">                sess.run(train_step, feed_dict&#x3D;&#123;X: batch_xs, y: batch_ys, learning_rate_holder: learning_rate&#125;)</span><br><span class="line">            elif i &lt; 50:</span><br><span class="line">                sess.run(train_step, feed_dict&#x3D;&#123;X: batch_xs, y: batch_ys, learning_rate_holder: learning_rate &#x2F; 10.0&#125;)</span><br><span class="line">            elif i &lt; 70:</span><br><span class="line">                sess.run(train_step, feed_dict&#x3D;&#123;X: batch_xs, y: batch_ys, learning_rate_holder: learning_rate &#x2F; 100.0&#125;)</span><br><span class="line">            else:</span><br><span class="line">                sess.run(train_step, feed_dict&#x3D;&#123;X: batch_xs, y: batch_ys, learning_rate_holder: learning_rate &#x2F; 1000.0&#125;)</span><br></pre></td></tr></table></figure>
<p>训练步骤打印结果如下：<br>step 1, loss 2300.436, testing accuracy 0.117<br>step 2, loss 2299.375, testing accuracy 0.13<br>… …<br>step 10, loss 2290.4155, testing accuracy 0.154<br>step 11, loss 678.09644, testing accuracy 0.788<br>step 12, loss 178.35306, testing accuracy 0.948<br>… …<br>step 49, loss 35.42096, testing accuracy 0.988<br>step 50, loss 34.458153, testing accuracy 0.988<br>梯度爆炸问题得以解决。</p>
<h2 id="实验4：使用tf-get-variable创建变量，使用tf-global-variables-initializer初始化"><a href="#实验4：使用tf-get-variable创建变量，使用tf-global-variables-initializer初始化" class="headerlink" title="实验4：使用tf.get_variable创建变量，使用tf.global_variables_initializer初始化"></a>实验4：使用tf.get_variable创建变量，使用tf.global_variables_initializer初始化</h2><p>上面代码都是使用 tf.Variable(tf.truncated_normal(…)) 来创建权重矩阵的，我们现在尝试使用 tf.get_variable() 来创建权重矩阵。tf.Variable 和 tf.get_variable的区别是前者每次调用都会创建新的对象；而对于后者来说，如果变量已经存在则直接将该变量返回，否则它才会创建一个新的变量 [3]。</p>
<p>在AlexNet1.py的基础上做如下改进，其他不变：(此部分完整代码参见文献[2]AlexNet4.py)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">W_conv &#x3D; &#123;</span><br><span class="line">    &#39;conv1&#39;: tf.get_variable(&#39;conv1&#x2F;weights&#39;, shape&#x3D;[11, 11, input_image_channel, 96]),</span><br><span class="line">    &#39;conv2&#39;: tf.get_variable(&#39;conv2&#x2F;weights&#39;, shape&#x3D;[5, 5, 96, 256]),</span><br><span class="line">    &#39;conv3&#39;: tf.get_variable(&#39;conv3&#x2F;weights&#39;, shape&#x3D;[3, 3, 256, 384]),</span><br><span class="line">    &#39;conv4&#39;: tf.get_variable(&#39;conv4&#x2F;weights&#39;, shape&#x3D;[3, 3, 384, 384]),</span><br><span class="line">    &#39;conv5&#39;: tf.get_variable(&#39;conv5&#x2F;weights&#39;, shape&#x3D;[3, 3, 384, 256]),</span><br><span class="line">    &#39;fc1&#39;: tf.get_variable(&#39;fc1&#x2F;weights&#39;, shape&#x3D;[n_fc1, n_fc2], trainable&#x3D;True),</span><br><span class="line">    &#39;fc2&#39;: tf.get_variable(&#39;fc2&#x2F;weights&#39;, shape&#x3D;[n_fc2, n_fc3], trainable&#x3D;True),</span><br><span class="line">    &#39;output&#39;: tf.get_variable(&#39;output&#x2F;weights&#39;, shape&#x3D;[n_fc3, n_classes], trainable&#x3D;True)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">b_conv &#x3D; &#123;</span><br><span class="line">    &#39;conv1&#39;: tf.get_variable(&#39;conv1&#x2F;biases&#39;, shape&#x3D;[96]),</span><br><span class="line">    &#39;conv2&#39;: tf.get_variable(&#39;conv2&#x2F;biases&#39;, shape&#x3D;[256]),</span><br><span class="line">    &#39;conv3&#39;: tf.get_variable(&#39;conv3&#x2F;biases&#39;, shape&#x3D;[384]),</span><br><span class="line">    &#39;conv4&#39;: tf.get_variable(&#39;conv4&#x2F;biases&#39;, shape&#x3D;[384]),</span><br><span class="line">    &#39;conv5&#39;: tf.get_variable(&#39;conv5&#x2F;biases&#39;, shape&#x3D;[256]),</span><br><span class="line">    &#39;fc1&#39;: tf.get_variable(&#39;fc1&#x2F;biases&#39;, shape&#x3D;[n_fc2], trainable&#x3D;True),</span><br><span class="line">    &#39;fc2&#39;: tf.get_variable(&#39;fc2&#x2F;biases&#39;, shape&#x3D;[n_fc3], trainable&#x3D;True),</span><br><span class="line">    &#39;output&#39;: tf.get_variable(&#39;output&#x2F;biases&#39;, shape&#x3D;[n_classes], trainable&#x3D;True)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>训练步骤打印结果如下：<br>step 1, loss 170.07048, testing accuracy 0.936<br>step 2, loss 86.70665, testing accuracy 0.972<br>… …<br>step 50, loss 26.004112, testing accuracy 0.99<br>可以看到，算法模型step1时测试准确率就达到93.6%了，且后面没有梯度爆炸。</p>
<p>&emsp; 在这里，由于我们并没有事先为权重创建变量，所以tf.get_variable会自己创建变量，然后使用tf.global_variables_initializer()初始化所有的变量，可见tf.get_variable和tf.global_variables_initializer()使用了一种较好的初始化策略。</p>
<p>本实验说明了良好的权重初始化对算法模型的训练是非常重要的。</p>
<h2 id="实验5：对卷积层分组"><a href="#实验5：对卷积层分组" class="headerlink" title="实验5：对卷积层分组"></a>实验5：对卷积层分组</h2><p>在AlexNet2.py的基础上进行改进，将 W_conv 中的conv4、conv5分成两组，具体改进如下：（代码详见文献[2]AlexNet5.py）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">W_conv &#x3D; &#123;</span><br><span class="line">    ... ...</span><br><span class="line">    &#39;conv4_1&#39;: tf.Variable(tf.truncated_normal([3, 3, 384&#x2F;&#x2F;2, 384&#x2F;&#x2F;2], mean&#x3D;0, stddev&#x3D;0.01)),</span><br><span class="line">    &#39;conv4_2&#39;: tf.Variable(tf.truncated_normal([3, 3, 384&#x2F;&#x2F;2, 384&#x2F;&#x2F;2], mean&#x3D;0, stddev&#x3D;0.01)),</span><br><span class="line">    &#39;conv5_1&#39;: tf.Variable(tf.truncated_normal([3, 3, 384&#x2F;&#x2F;2, 256&#x2F;&#x2F;2], mean&#x3D;0, stddev&#x3D;0.01)),</span><br><span class="line">    &#39;conv5_2&#39;: tf.Variable(tf.truncated_normal([3, 3, 384&#x2F;&#x2F;2, 256&#x2F;&#x2F;2], mean&#x3D;0, stddev&#x3D;0.01)),</span><br><span class="line">    ... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对卷积层4、卷积层5的具体改进如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 卷积层4</span><br><span class="line">conv3groups &#x3D; tf.split(axis&#x3D;3, num_or_size_splits&#x3D;2, value&#x3D;conv3)</span><br><span class="line">conv4_1 &#x3D; tf.nn.conv2d(conv3groups[0], W_conv[&#39;conv4_1&#39;], strides&#x3D;[1, 1, 1, 1], padding&#x3D;&#39;SAME&#39;)</span><br><span class="line">conv4_2 &#x3D; tf.nn.conv2d(conv3groups[1], W_conv[&#39;conv4_2&#39;], strides&#x3D;[1, 1, 1, 1], padding&#x3D;&#39;SAME&#39;)</span><br><span class="line">conv4 &#x3D; tf.concat(axis&#x3D;3, values&#x3D;[conv4_1, conv4_2])</span><br><span class="line">conv4 &#x3D; tf.nn.bias_add(conv4, b_conv[&#39;conv4&#39;])</span><br><span class="line">conv4 &#x3D; tf.nn.relu(conv4)</span><br><span class="line"></span><br><span class="line"># 卷积层5</span><br><span class="line">conv4groups &#x3D; tf.split(axis&#x3D;3, num_or_size_splits&#x3D;2, value&#x3D;conv4)</span><br><span class="line">conv5_1 &#x3D; tf.nn.conv2d(conv4groups[0], W_conv[&#39;conv5_1&#39;], strides&#x3D;[1, 1, 1, 1], padding&#x3D;&#39;SAME&#39;)</span><br><span class="line">conv5_2 &#x3D; tf.nn.conv2d(conv4groups[1], W_conv[&#39;conv5_2&#39;], strides&#x3D;[1, 1, 1, 1], padding&#x3D;&#39;SAME&#39;)</span><br><span class="line">conv5 &#x3D; tf.concat(axis&#x3D;3, values&#x3D;[conv5_1, conv5_2])</span><br><span class="line">conv5 &#x3D; tf.nn.bias_add(conv5, b_conv[&#39;conv5&#39;])</span><br><span class="line">conv5 &#x3D; tf.nn.relu(conv5)</span><br></pre></td></tr></table></figure><br>其他代码不变，训练测试结果打印如下：<br>step 1, loss 2297.4663, testing accuracy 0.104<br>step 2, loss 2299.2734, testing accuracy 0.124<br>… …<br>step 3, loss 2114.1978, testing accuracy 0.301<br>step 4, loss 366.07996, testing accuracy 0.872<br>… …<br>step 5, loss 44.795494, testing accuracy 0.992<br>step 6, loss 37.983482, testing accuracy 0.988<br>和AlexNet2.py相比，此时算法梯度下降速度更快，且不会出现梯度爆炸。</p>
<h2 id="实验6：使用Batch-Normalization优化"><a href="#实验6：使用Batch-Normalization优化" class="headerlink" title="实验6：使用Batch Normalization优化"></a>实验6：使用Batch Normalization优化</h2><p>在AlexNet2.py的基础上，使用Batch Normalization算法优化模型，并且去掉lrn，主要改进如下：（代码详见文献[2]AlexNet6.py）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">def batch_norm(inputs, is_training, is_conv_out&#x3D;True, decay&#x3D;0.999):</span><br><span class="line">    scale &#x3D; tf.Variable(tf.ones([inputs.get_shape()[-1]]))</span><br><span class="line">    beta &#x3D; tf.Variable(tf.zeros([inputs.get_shape()[-1]]))</span><br><span class="line">    pop_mean &#x3D; tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable&#x3D;False)</span><br><span class="line">    pop_var &#x3D; tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable&#x3D;False)</span><br><span class="line"></span><br><span class="line">    if is_training:</span><br><span class="line">        if is_conv_out:</span><br><span class="line">            batch_mean, batch_var &#x3D; tf.nn.moments(inputs, [0, 1, 2])</span><br><span class="line">        else:</span><br><span class="line">            batch_mean, batch_var &#x3D; tf.nn.moments(inputs, [0])</span><br><span class="line"></span><br><span class="line">        train_mean &#x3D; tf.assign(pop_mean, pop_mean*decay+batch_mean*(1-decay))</span><br><span class="line">        train_var &#x3D; tf.assign(pop_var, pop_var*decay+batch_var*(1-decay))</span><br><span class="line"></span><br><span class="line">        with tf.control_dependencies([train_mean, train_var]):</span><br><span class="line">            return tf.nn.batch_normalization(inputs, batch_mean, batch_var, beta, scale, 0.001)</span><br><span class="line">    else:</span><br><span class="line">        return tf.nn.batch_normalization(inputs, pop_mean, pop_var, beta, scale, 0.001)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">... ...</span><br><span class="line">... ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">conv1 &#x3D; tf.nn.bias_add(...)</span><br><span class="line">conv1 &#x3D; batch_norm(conv1, True)</span><br><span class="line">... ...</span><br><span class="line"></span><br><span class="line">conv2 &#x3D; tf.nn.bias_add(...)</span><br><span class="line">conv2 &#x3D; batch_norm(conv2, True)</span><br><span class="line">... ...</span><br><span class="line"></span><br><span class="line">conv3 &#x3D; tf.nn.bias_add(...)</span><br><span class="line">conv3 &#x3D; batch_norm(conv3, True)</span><br><span class="line">... ...</span><br><span class="line"></span><br><span class="line">conv4 &#x3D; tf.nn.bias_add(...)</span><br><span class="line">conv4 &#x3D; batch_norm(conv4, True)</span><br><span class="line">... ...</span><br><span class="line"></span><br><span class="line">conv5 &#x3D; tf.nn.bias_add(...)</span><br><span class="line">conv5 &#x3D; batch_norm(conv5, True)</span><br><span class="line">... ...</span><br><span class="line"></span><br><span class="line">fc1 &#x3D; tf.add(...)</span><br><span class="line">fc1 &#x3D; batch_norm(fc1, True, False)</span><br><span class="line">... ...</span><br><span class="line"></span><br><span class="line">fc2 &#x3D; tf.add(...)</span><br><span class="line">fc2 &#x3D; batch_norm(fc2, True, False)</span><br><span class="line">... ...</span><br></pre></td></tr></table></figure></p>
<p>训练测试结果打印如下：<br>step 1, loss 44.443394, testing accuracy 0.979<br>step 1, loss 34.873466, testing accuracy 0.988<br>… …<br>step 1, loss 13.521537, testing accuracy 0.992<br>step 1, loss 10.757448, testing accuracy 0.991<br>很明显，使用BN算法优化效果非常显著。</p>
<blockquote>
<p>AlexNet6.py优化中，我们使用了tf.truncated_normal并且指定mean、stddev来创建参数矩阵，后来我实验发现，如果使用AlexNet1.py中创建参数矩阵的方法的话(即使用tf.truncated_normal创建参数矩阵但mean和stddev却使用默认的0和1，详见文献[2]AlexNet7.py)，则模型仍然是无法收敛的，这说明即使是使用BN优化，但不恰当的参数初始化仍然无法使模型收敛。</p>
</blockquote>
<p>最后附上上述实验的loss和accuracy曲线图：<br><img src="https://lh3.googleusercontent.com/-98qN8utSMAg/XEWv_k-p4lI/AAAAAAAAALQ/HTcnPrenuwwPshaBiZ1aaFLFsNRVMilTQCLcBGAs/s0/AlexNet_loss.png" alt="enter image description here" title="AlexNet_loss.png"></p>
<p><img src="https://lh3.googleusercontent.com/-CxzJjCxrvqk/XEWwGoMl0XI/AAAAAAAAALc/bHJcAVHnR-AhuoYRmr_OsjRTezLCcumfQCLcBGAs/s0/AlexNet_accuracy.png" alt="enter image description here" title="AlexNet_accuracy.png"></p>
<h2 id="实验总结"><a href="#实验总结" class="headerlink" title="实验总结"></a>实验总结</h2><ol>
<li>合适的参数初始化是非常重要的；</li>
<li>动态调整学习率；</li>
<li>当我们不确定如何手动初始化参数矩阵时，可以使用 tf.get_variable + tf.global_variables_initializer 默认的初始化策略；</li>
<li>对卷积层分组是一个很好优化思路；</li>
<li>Batch Normalization算法优化效果非常显著。</li>
</ol>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] <a href="https://github.com/kratzert/finetune_alexnet_with_tensorflow" target="_blank" rel="noopener">finetune_alexnet_with_tensorflow</a><br>[2] <a href="https://github.com/JuneXia/handml" target="_blank" rel="noopener">我的handml仓库</a><br>[3] <a href="https://blog.csdn.net/u012436149/article/details/53696970" target="_blank" rel="noopener">tensorflow学习笔记（二十三）：variable与get_variable</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/22/dl/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01.4%E3%80%91%E6%9B%B4%E5%BF%AB%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="其实，我是一个搬运工！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Paper搬运菌">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/11/22/dl/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01.4%E3%80%91%E6%9B%B4%E5%BF%AB%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8/" class="post-title-link" itemprop="url">【深度学习笔记1.4】更快的优化器</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-11-22 17:28:05" itemprop="dateCreated datePublished" datetime="2017-11-22T17:28:05+08:00">2017-11-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-22 14:06:21" itemprop="dateModified" datetime="2020-01-22T14:06:21+08:00">2020-01-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习笔记</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>&emsp; 训练一个非常大的深度神经网络可能会非常缓慢。 到目前为止，我们已经看到了四种加速训练的方法：对连接权重应用良好的初始化策略，使用良好的激活函数，使用批量规范化以及重用预训练网络的部分。另一个巨大的速度提升来自使用比普通渐变下降优化器更快的优化器。 在本节中，我们将介绍最流行的：动量优化，Nesterov 加速梯度，AdaGrad，RMSProp，最后是 Adam 优化。<br>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2017/11/22/dl/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01.4%E3%80%91%E6%9B%B4%E5%BF%AB%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/22/dl/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02.1.1%E3%80%91LeNet-5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="其实，我是一个搬运工！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Paper搬运菌">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/11/22/dl/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02.1.1%E3%80%91LeNet-5/" class="post-title-link" itemprop="url">【深度学习笔记2.1.1】LeNet-5</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-11-22 17:28:05" itemprop="dateCreated datePublished" datetime="2017-11-22T17:28:05+08:00">2017-11-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-22 13:25:14" itemprop="dateModified" datetime="2020-01-22T13:25:14+08:00">2020-01-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习笔记</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <a id="more"></a>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>LeNet-5中的-5是个啥？</p>
<p><a href="http://www.dengfanxin.cn/wp-content/uploads/2016/03/1998Lecun.pdf" target="_blank" rel="noopener">Gradient-Based Learning Applied to Document Recognition</a></p>
<p><img src="https://lh3.googleusercontent.com/-2xYixX9xPnU/W2rYZSwne7I/AAAAAAAAAFM/cpvrI_zQ3h05HQWrxtA3K3sdP5ziStYpgCLcBGAs/s0/lenet5_1.png" alt="enter image description here" title="lenet5_1.png"></p>
<center>图1 [3]</center>
![enter image description here](https://lh3.googleusercontent.com/-KPfsR5nep9A/W2rbZF4xk-I/AAAAAAAAAFc/PtinL8z9rCA0Pzf_GiovJ7kS8zRkm7nrACLcBGAs/s0/lenet5_2.png "lenet5_2.png")
<center>图2 [2]</center>

<p>Input：shape=[-1, 28, 28, 1]<br>&emsp;  |&emsp;<br>&emsp;  |&emsp;<br>&emsp;  |&emsp;  filter.shape = [5, 5, 1, 6]<br>&emsp;  |&emsp;  C1 = tf.nn.conv2d(Input, filter, strides=[1,1,1,1], padding=’SAME’)<br>&emsp;  |&emsp;<br>&emsp;  |&emsp;  conv2d后C1层feature maps的shape为[-1, 28, 28, 6]<br>&emsp;  |&emsp;<br>&emsp;  |&emsp;  参数个数：6x(5x5+1)=156<br>&emsp;  |&emsp;  &emsp;  &emsp;  一个卷积核的大小为5x5，每个卷积核有5x5个参数，每个卷积核做完所有卷积后还要与一个bias相加，故每个卷积核对应有5x5+1个参数；从Input到C1一共有6个卷积核，所以从Input到C1共有6x(5x5+1)个参数需要训练；（疑问：能否一个卷积核做完一次卷积后就和一个bias相加？）<br>&emsp;  |&emsp;  连接个数：6x(5x5+1)x28x28=122304<br>&emsp;  |&emsp;  &emsp;  &emsp;  一个卷积核每做完一次卷积后都会在C1层生成一个像素，该像素对应着(5x5+1)个连接，又C1层每个通道有28x28个像素，故C1层每个通道有(5x5+1)x28x28个连接；又C1层有6个通道，故从Input到C1层一共有6x(5x5+1)x28x28个连接；<br>&emsp;  |&emsp;  &emsp;  &emsp;  疑问：根据图2可知(从文献[1]以及网上的很多示例代码也能看出)，这里的一个卷积核是和整个Input做完卷积后再和一个bias相加的，那么连接的个数不应该是6x(5x5x28x28+1)吗？<br>&emsp;  |&emsp;  &emsp;  &emsp;  答：一个卷积核是和整个Input做完卷积后得到的是一个28x28的feature map，该feature map加上一个数值bias可以等价于feature map的每个像素都加上一个bias，所以一个大小为28x28的feature map的每个像素都会和bias相加。<br>&emsp;  |&emsp;<br>C1 Layer：shape=[-1, 28, 28, 6]<br>&emsp;  |&emsp;<br>&emsp;  |&emsp;<br>&emsp;  |&emsp;  ksize=[1,2,2,1]<br>&emsp;  |&emsp;  bias1 = tf.Variable( tf.truncated_normal( [6] ) )<br>&emsp;  |&emsp;  S2 = tf.nn.max_pool(tf.nn.sigmoid(C1 + bias1), ksize, strides=[1, 2, 2, 1], padding=’SAME’)<br>&emsp;  |&emsp;<br>&emsp;  |&emsp;  参数个数：6x(1+1)=12; 1个训练参数w，一个偏置b<br>&emsp;  |&emsp;  &emsp;  &emsp;  C1层的2x2感受野的四个输入相加，然后乘以一个可训练参数，再加上一个可训练偏置。结果通过 sigmoid 函数计算。可训练系数和偏置控制着 sigmoid 函数的非线性程度。如果系数比较小，那么运算近似于线性运算，亚采样相当于模糊图像。如果系数比较大，根据偏置的大小亚采样可以被看成是有噪声的“或”运算或者有噪声的“与”运算。<br>&emsp;  |&emsp;  连接个数：6x(4+1)x14x14=5880<br>&emsp;  |&emsp;  &emsp;  &emsp;  从一个平面到下一个平面的映射可以看作是作卷积运算，S-层可看作是模糊滤波器，起到二次特征提取的作用。隐层与隐层之间空间分辨率递减，而每层所含的平面数递增，这样可用于检测更多的特征信息[2]。<br>&emsp;  |&emsp;  &emsp;  &emsp;  问：按照很多文章介绍说的，那么程序应该是下面这样的吧：<br>&emsp;  |&emsp;  &emsp;  &emsp;  &emsp;  c1 = conv2d( input, filter, … ) + bias;<br>&emsp;  |&emsp;  &emsp;  &emsp;  &emsp;  s2 = sigmoid( pooling( c1, pool_filter, … ) + bias );<br>&emsp;  |&emsp;  &emsp;  &emsp;  但是实际上在很多程序具体实现的过程中却是下面这样的：<br>&emsp;  |&emsp;  &emsp;  &emsp;  &emsp;  c1 = conv2d( input, filter, … );<br>&emsp;  |&emsp;  &emsp;  &emsp;  &emsp;  s2 = pooling( sigmoid( c1 + bias ) );<br>&emsp;  |&emsp;  &emsp;  &emsp;  这是为什么？<br>&emsp;  |&emsp;<br>S2 Layer：shape=[-1, 14, 14, 6]<br>&emsp;  |&emsp;<br>&emsp;  |&emsp;<br>&emsp;  |&emsp;  filter.shape = [5, 5, 6, 16]<br>&emsp;  |&emsp;  C3 = tf.nn.conv2d(S2, filter, strides=[1, 1, 1, 1], padding=’VALID’)<br>&emsp;  |&emsp;<br>&emsp;  |&emsp;  参数个数：6x(3x5x5+1)+6x(4x5x5+1)+3x(4x5x5+1)+1x(6x5x5+1)=1516<br>&emsp;  |&emsp;  连接个数：由于C3 Layer图像大小为10x10，所以共有151600个参数；<br>&emsp;  |&emsp;<br>C3 Layer：shape=[-1, 10, 10, 16]<br>&emsp;  |&emsp;<br>&emsp;  |&emsp;<br>&emsp;  |&emsp;  ksize=[1,2,2,1]<br>&emsp;  |&emsp;  bias2 = tf.Variable(tf.truncated_normal([16]))<br>&emsp;  |&emsp;  S4 = tf.nn.max_pool(tf.nn.sigmoid(C3 + bias2), ksize, strides=[1, 2, 2, 1], padding=’SAME’)<br>&emsp;  |&emsp;<br>S4 Layer：shape=[-1, 5, 5, 16]<br>&emsp;  |&emsp;<br>&emsp;  |&emsp;<br>&emsp;  |&emsp;  filter.shape=[5, 5, 16, 120]<br>&emsp;  |&emsp;  C5 = tf.nn.conv2d(S4, filter, strides=[1, 1, 1, 1], padding=’SAME’)<br>&emsp;  |&emsp;<br>C5 Layer：shape=[-1, 5, 5, 120]<br>&emsp;  |&emsp;<br>&emsp;  |&emsp;<br>&emsp;  |&emsp;  C5_flat = tf.reshape( C5, [-1, 5 <em> 5 </em> 120] )<br>&emsp;  |&emsp;  W_fc1 = tf.Variable( tf.truncated_normal( [5 <em> 5 </em> 120, 84]) )<br>&emsp;  |&emsp;  b_fc1 = tf.Variable( tf.truncated_normal( [84] ) )<br>&emsp;  |&emsp;  h_fc1 = tf.nn.sigmoid( tf.matmul( C5_flat, W_fc1 ) + b_fc1)<br>&emsp;  |&emsp;  参数个数：84x120+84=10164<br>&emsp;  |&emsp;<br>F6 Layer：全连接层<br>&emsp;  |&emsp;<br>&emsp;  |&emsp;<br>&emsp;  |&emsp;  W_fc2 = tf.Variable( tf.truncated_normal( [80, 10] ) )<br>&emsp;  |&emsp;  b_fc2 = tf.Variable( tf.truncated_normal( [10] ) )<br>&emsp;  |&emsp;  y_conv = tf.nn.softmax( tf.matmul( h_fc1, W_fc2 ) + b_fc2 )<br>&emsp;  |&emsp;<br>Output Layer<br>&emsp;  |&emsp;<br>&emsp;  |&emsp;<br>&emsp;  |&emsp;<br>&emsp;  |&emsp;  未完待续……</p>
<p>&emsp;  </p>
<h2 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h2><p>代码参考文献[1] 程序13.10<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">datapath = <span class="string">'/home/xiajun/res/MNIST_data'</span></span><br><span class="line">mnist_data_set = input_data.read_data_sets(datapath, validation_size=<span class="number">0</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(<span class="string">'float'</span>, [<span class="literal">None</span>, <span class="number">784</span>])</span><br><span class="line">y_ = tf.placeholder(<span class="string">'float'</span>, [<span class="literal">None</span>, <span class="number">10</span>])</span><br><span class="line">x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#第一层卷积层，初始化卷积核参数、偏置值，该卷积层5*5大小，1个通道，共有6个不同卷积核</span></span><br><span class="line">filter1 = tf.Variable(tf.truncated_normal([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">6</span>]))</span><br><span class="line">bias1 = tf.Variable(tf.truncated_normal([<span class="number">6</span>]))</span><br><span class="line">conv1 = tf.nn.conv2d(x_image, filter1, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"><span class="comment"># 此时conv1.shape = [-1, 28, 28, 6]</span></span><br><span class="line">h_conv1 = tf.nn.sigmoid(conv1 + bias1)</span><br><span class="line"><span class="comment"># h_conv1 = tf.nn.relu(conv1 + bias1)</span></span><br><span class="line"></span><br><span class="line">maxPool2 = tf.nn.max_pool(h_conv1, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"><span class="comment"># 此时maxPool2.shape = [-1, 14, 14, 6]</span></span><br><span class="line"></span><br><span class="line">filter2 = tf.Variable(tf.truncated_normal([<span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">16</span>]))</span><br><span class="line">bias2 = tf.Variable(tf.truncated_normal([<span class="number">16</span>]))</span><br><span class="line">conv2 = tf.nn.conv2d(maxPool2, filter2, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"><span class="comment"># 此时conv2.shape = [-1, 14, 14, 16]</span></span><br><span class="line">h_conv2 = tf.nn.sigmoid(conv2 + bias2)</span><br><span class="line"><span class="comment"># h_conv2 = tf.nn.relu(conv2 + bias2)</span></span><br><span class="line"></span><br><span class="line">maxPool3 = tf.nn.max_pool(h_conv2, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"><span class="comment"># 此时maxPool3.shape = [-1, 7, 7, 16]</span></span><br><span class="line"></span><br><span class="line">filter3 = tf.Variable(tf.truncated_normal([<span class="number">5</span>, <span class="number">5</span>, <span class="number">16</span>, <span class="number">120</span>]))</span><br><span class="line">bias3 = tf.Variable(tf.truncated_normal([<span class="number">120</span>]))</span><br><span class="line">conv3 = tf.nn.conv2d(maxPool3, filter3, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"><span class="comment"># 此时conv3.shape = [-1, 7, 7, 120]</span></span><br><span class="line">h_conv3 = tf.nn.sigmoid(conv3 + bias3)</span><br><span class="line"><span class="comment"># h_conv3 = tf.nn.relu(conv3 + bias3)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 全连接层</span></span><br><span class="line"><span class="comment"># 权值参数</span></span><br><span class="line">W_fc1 = tf.Variable(tf.truncated_normal([<span class="number">7</span> * <span class="number">7</span> * <span class="number">120</span>, <span class="number">80</span>]))</span><br><span class="line"><span class="comment"># 偏置值</span></span><br><span class="line">b_fc1 = tf.Variable(tf.truncated_normal([<span class="number">80</span>]))</span><br><span class="line"><span class="comment"># 将卷积的产出展开</span></span><br><span class="line">h_pool2_flat = tf.reshape(h_conv3, [<span class="number">-1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">120</span>])</span><br><span class="line"><span class="comment"># 神经网络计算，并添加sigmoid激活函数</span></span><br><span class="line">h_fc1 = tf.nn.sigmoid(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line"><span class="comment"># h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span></span><br><span class="line"><span class="comment"># 此时h_fc1.shape = [-1, 80]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出层，使用softmax进行多分类</span></span><br><span class="line">W_fc2 = tf.Variable(tf.truncated_normal([<span class="number">80</span>, <span class="number">10</span>]))</span><br><span class="line">b_fc2 = tf.Variable(tf.truncated_normal([<span class="number">10</span>]))</span><br><span class="line">y_output = tf.nn.softmax(tf.matmul(h_fc1, W_fc2) + b_fc2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">cross_entropy = -tf.reduce_sum(y_ * tf.log(y_output))</span><br><span class="line"><span class="comment"># 使用GD优化算法来调整参数</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.001</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试正确率</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y_output, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有变量进行初始化</span></span><br><span class="line">sess.run(tf.initialize_all_variables())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string"># Debug</span></span><br><span class="line"><span class="string">batch_xs, batch_ys = mnist_data_set.train.next_batch(5)</span></span><br><span class="line"><span class="string">x = tf.Variable(tf.truncated_normal([5, 5, 1, 6]))</span></span><br><span class="line"><span class="string">init = tf.global_variables_initializer()</span></span><br><span class="line"><span class="string">with tf.Session() as sess:</span></span><br><span class="line"><span class="string">    sess.run(init)</span></span><br><span class="line"><span class="string">    print(conv1.eval(feed_dict=&#123;x: batch_xs&#125;).shape)</span></span><br><span class="line"><span class="string">    print(h_conv1.eval(feed_dict=&#123;x: batch_xs&#125;).shape)</span></span><br><span class="line"><span class="string">    print(maxPool2.eval(feed_dict=&#123;x: batch_xs&#125;).shape)</span></span><br><span class="line"><span class="string">    print(conv2.eval(feed_dict=&#123;x: batch_xs&#125;).shape)</span></span><br><span class="line"><span class="string">    print(h_conv2.eval(feed_dict=&#123;x: batch_xs&#125;).shape)</span></span><br><span class="line"><span class="string">    print(maxPool3.eval(feed_dict=&#123;x: batch_xs&#125;).shape)</span></span><br><span class="line"><span class="string">    print(h_conv3.eval(feed_dict=&#123;x: batch_xs&#125;).shape)</span></span><br><span class="line"><span class="string">    print(h_fc1.eval(feed_dict=&#123;x: batch_xs&#125;).shape)</span></span><br><span class="line"><span class="string">    print('debug')</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行训练</span></span><br><span class="line">batch_size = <span class="number">200</span></span><br><span class="line">start_time = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> range(mnist_data_set.train.num_examples//batch_size):</span><br><span class="line">        <span class="comment"># 获取训练数据</span></span><br><span class="line">        batch_xs, batch_ys = mnist_data_set.train.next_batch(batch_size)</span><br><span class="line">        train_step.run(feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">    batch_xs, batch_ys = mnist_data_set.test.images, mnist_data_set.test.labels</span><br><span class="line">    train_accuracy = accuracy.eval(feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</span><br><span class="line">    print(<span class="string">"step %d, training accuracy %g"</span> % (i, train_accuracy))</span><br><span class="line"></span><br><span class="line">    end_time = time.time()</span><br><span class="line">    print(<span class="string">'time: '</span>, (end_time - start_time))</span><br><span class="line">    start_time = end_time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭会话</span></span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>注意事项：将sigmoid激活函数改为relu激活函数后，好像效果更差了（在我的笔记本上训练前3步后准确率都在0.09以下，我的笔记本速度太慢，不知继续训练下去会怎样，留待高级服务器上试试）。</p>
</blockquote>
<h2 id="各种优化后的loss和accuracy曲线图"><a href="#各种优化后的loss和accuracy曲线图" class="headerlink" title="各种优化后的loss和accuracy曲线图"></a>各种优化后的loss和accuracy曲线图</h2><p><img src="https://lh3.googleusercontent.com/-MLR3zMESskE/XEWvKjIDo_I/AAAAAAAAAK4/omsZPom_aAwAXWv_TaMGNAQGl1wuw38nwCLcBGAs/s0/LeNet_loss.png" alt="enter image description here" title="LeNet_loss.png"></p>
<p><img src="https://lh3.googleusercontent.com/-BJ1s0wWY_4o/XEWvdSL7hgI/AAAAAAAAALE/C571-MK_BNsTd-GOcn_6twLvk9LX1i-BQCLcBGAs/s0/LeNet_accuracy.png" alt="enter image description here" title="LeNet_accuracy.png"></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] 王晓华. TensorFlow深度学习应用实践<br>[2] <a href="https://blog.csdn.net/qiaofangjie/article/details/16826849" target="_blank" rel="noopener">Deep Learning（深度学习）学习笔记整理系列之LeNet-5卷积参数个人理解</a><br>[3] <a href="http://www.dengfanxin.cn/wp-content/uploads/2016/03/1998Lecun.pdf" target="_blank" rel="noopener">Gradient-Based Learning Applied to Document Recognition</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/19/TensorFlow%E7%AC%94%E8%AE%B0/%E3%80%90TensorFlow%E7%AC%94%E8%AE%B03.1%E3%80%91Tensorflow%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%94%A8%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="其实，我是一个搬运工！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Paper搬运菌">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/11/19/TensorFlow%E7%AC%94%E8%AE%B0/%E3%80%90TensorFlow%E7%AC%94%E8%AE%B03.1%E3%80%91Tensorflow%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%94%A8%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">TensorFlow笔记/【TensorFlow笔记3.1】Tensorflow模型复用总结</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-11-19 00:00:00" itemprop="dateCreated datePublished" datetime="2017-11-19T00:00:00+08:00">2017-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-22 13:22:45" itemprop="dateModified" datetime="2020-01-22T13:22:45+08:00">2020-01-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow笔记</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="方法1：导入整张Graph"><a href="#方法1：导入整张Graph" class="headerlink" title="方法1：导入整张Graph"></a>方法1：导入整张Graph</h2><p>当模型graph和预训练模型graph是一样的时候，我们通常可以import整张graph来恢复模型(即复用预训练模型)。</p>
<p>这种情况是：通常是预训练模型是我们自己训练的，再次微调时可以使用这种方法。文献[1]中对该方法也有所介绍。<br>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2017/11/19/TensorFlow%E7%AC%94%E8%AE%B0/%E3%80%90TensorFlow%E7%AC%94%E8%AE%B03.1%E3%80%91Tensorflow%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%94%A8%E6%80%BB%E7%BB%93/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/11/dl/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01.2%E3%80%91%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="其实，我是一个搬运工！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Paper搬运菌">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/11/11/dl/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01.2%E3%80%91%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/" class="post-title-link" itemprop="url">【深度学习笔记1.2】梯度消失与梯度爆炸</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-11-11 17:28:05" itemprop="dateCreated datePublished" datetime="2017-11-11T17:28:05+08:00">2017-11-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-02-16 09:22:53" itemprop="dateModified" datetime="2020-02-16T09:22:53+08:00">2020-02-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习笔记</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>&emsp; 梯度下降法（Gradient descent）是一种基于函数一阶性质的优化算法，其本质是在某个位置将目标函数一阶展开，利用其一阶性质持续向函数值下降最快的方向前进，以期找到函数的全局最小解。梯度下降属于梯度优化方法大类，此外还有最速下降法，共轭梯度法等等。还有其他方法基于目标函数的二阶性质，比如牛顿法、拟牛顿法等[1]。<br>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2017/11/11/dl/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01.2%E3%80%91%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/10/26/TensorFlow%E7%AC%94%E8%AE%B0/%E3%80%90TensorFlow%E7%AC%94%E8%AE%B02.1%E3%80%91tf.nn.conv2d%EF%BC%8Ctf.nn.max_pool/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="其实，我是一个搬运工！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Paper搬运菌">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/10/26/TensorFlow%E7%AC%94%E8%AE%B0/%E3%80%90TensorFlow%E7%AC%94%E8%AE%B02.1%E3%80%91tf.nn.conv2d%EF%BC%8Ctf.nn.max_pool/" class="post-title-link" itemprop="url">TensorFlow笔记/【TensorFlow笔记2.1】tf.nn.conv2d，tf.nn.max_pool</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-10-26 00:00:00" itemprop="dateCreated datePublished" datetime="2017-10-26T00:00:00+08:00">2017-10-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-22 14:08:11" itemprop="dateModified" datetime="2020-01-22T14:08:11+08:00">2020-01-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow笔记</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="tf-nn-conv2d函数解析"><a href="#tf-nn-conv2d函数解析" class="headerlink" title="tf.nn.conv2d函数解析"></a>tf.nn.conv2d函数解析</h2><p><strong>tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)</strong></p>
<p>除去name参数用以指定该操作的name，与方法有关的一共五个参数：<br>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2017/10/26/TensorFlow%E7%AC%94%E8%AE%B0/%E3%80%90TensorFlow%E7%AC%94%E8%AE%B02.1%E3%80%91tf.nn.conv2d%EF%BC%8Ctf.nn.max_pool/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/10/23/TensorFlow%E7%AC%94%E8%AE%B0/%E3%80%90TensorFlow%E7%AC%94%E8%AE%B01.3%E3%80%91TFRecords%E6%96%87%E4%BB%B6%E8%AF%BB%E5%86%99/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="其实，我是一个搬运工！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Paper搬运菌">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/10/23/TensorFlow%E7%AC%94%E8%AE%B0/%E3%80%90TensorFlow%E7%AC%94%E8%AE%B01.3%E3%80%91TFRecords%E6%96%87%E4%BB%B6%E8%AF%BB%E5%86%99/" class="post-title-link" itemprop="url">TensorFlow笔记/【TensorFlow笔记1.3】TFRecords文件读写</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-10-23 00:00:00" itemprop="dateCreated datePublished" datetime="2017-10-23T00:00:00+08:00">2017-10-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-22 14:08:05" itemprop="dateModified" datetime="2020-01-22T14:08:05+08:00">2020-01-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow笔记</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>&emsp; 除了典型的CSV文件存储方式外，TensorFlow还有专门的文件存储格式：TFRecords文件。<br>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2017/10/23/TensorFlow%E7%AC%94%E8%AE%B0/%E3%80%90TensorFlow%E7%AC%94%E8%AE%B01.3%E3%80%91TFRecords%E6%96%87%E4%BB%B6%E8%AF%BB%E5%86%99/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/10/22/TensorFlow%E7%AC%94%E8%AE%B0/%E3%80%90TensorFlow%E7%AC%94%E8%AE%B01.2%E3%80%91TensorFlow%E9%98%9F%E5%88%97%E5%92%8CCSV%E6%96%87%E4%BB%B6%E8%AF%BB%E5%86%99/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="其实，我是一个搬运工！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Paper搬运菌">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/10/22/TensorFlow%E7%AC%94%E8%AE%B0/%E3%80%90TensorFlow%E7%AC%94%E8%AE%B01.2%E3%80%91TensorFlow%E9%98%9F%E5%88%97%E5%92%8CCSV%E6%96%87%E4%BB%B6%E8%AF%BB%E5%86%99/" class="post-title-link" itemprop="url">TensorFlow笔记/【TensorFlow笔记1.2】TensorFlow队列和CSV文件读写</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-10-22 00:00:00" itemprop="dateCreated datePublished" datetime="2017-10-22T00:00:00+08:00">2017-10-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-22 13:21:58" itemprop="dateModified" datetime="2020-01-22T13:21:58+08:00">2020-01-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow笔记</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <a id="more"></a>
<h2 id="tensorflow队列"><a href="#tensorflow队列" class="headerlink" title="tensorflow队列"></a>tensorflow队列</h2><p>&emsp; 在tensorflow中可以使用FIFOQueue、RandomShuffleQueue等方式创建一个队列[1]。</p>
<p>代码示例1：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    q = tf.FIFOQueue(<span class="number">3</span>, <span class="string">"float"</span>) <span class="comment"># 创建长度为3，元素数据类型是float的队列。</span></span><br><span class="line">    init = q.enqueue_many(([<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>],)) <span class="comment"># 向队列中填充数据（注意这只是预备操作，真正的数据填充是要到sess.run(init)操作时才会完成）</span></span><br><span class="line">    init2 = q.dequeue() <span class="comment"># 出队</span></span><br><span class="line">    init3 = q.enqueue(<span class="number">1.</span>) <span class="comment"># 入队</span></span><br><span class="line"></span><br><span class="line">    sess.run(init)</span><br><span class="line">    sess.run(init2)</span><br><span class="line">    sess.run(init3)</span><br><span class="line"></span><br><span class="line">    quelen =  sess.run(q.size())</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(quelen):</span><br><span class="line">        print(sess.run(q.dequeue()))</span><br></pre></td></tr></table></figure></p>
<h2 id="tensorflow中队列如何实现入队与出队同时进行"><a href="#tensorflow中队列如何实现入队与出队同时进行" class="headerlink" title="tensorflow中队列如何实现入队与出队同时进行"></a>tensorflow中队列如何实现入队与出队同时进行</h2><p>&emsp; 上述代码是现将所有数据都存入队列，然后再依次从队列中取出，这并没有发挥出队列的价值。队列是为了实现入队与出队操作可以同时进行而设计的，tensorflow中可以通过QueueRunner和Coordinator协作来实现这项工作。<br>下面先简要说下tf.train.Coordinator和tf.train.QueueRunner的用法和意义。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建线程协调器，用于协调主线程和各个子线程之间的交互操作。</span></span><br><span class="line">coord = tf.train.Coordinator()</span><br><span class="line">    </span><br><span class="line">queue_runner = tf.train.QueueRunner(q, enqueue_ops=[add_op, enqueue_op] * <span class="number">2</span>) <span class="comment"># 定义用2个线程去完成这项任务</span></span><br><span class="line"><span class="comment"># 先用QueueRunner定义队列的入队操作，然后用queue_runner创建子线程去处理该入队操作。</span></span><br><span class="line"><span class="comment"># queue_runner在创建线程的时候需要传入Coordinator协调器，用于和主线程协调操作。</span></span><br><span class="line">enqueue_threads = queue_runner.create_threads(sess, coord=coord, start=<span class="literal">True</span>)  <span class="comment"># 启动入队线程</span></span><br><span class="line"><span class="comment"># queue_runner在创建线程的时候如果不传入Coordinator协调器的话，则程序运行结束前会报错。</span></span><br><span class="line"><span class="comment"># 这是因为当主线程运行完毕后就接直接结束了，而没有发出终止其他线程的请求。</span></span><br></pre></td></tr></table></figure></p>
<p>完整代码如下，代码示例2：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    q = tf.FIFOQueue(<span class="number">10</span>, <span class="string">"float32"</span>) <span class="comment"># 创建一个队列，该队列有10个数据，数据类型是float32</span></span><br><span class="line">    counter = tf.Variable(<span class="number">0.0</span>)</span><br><span class="line">    add_op = tf.assign_add(counter, tf.constant(<span class="number">1.0</span>))</span><br><span class="line">    enqueue_op = q.enqueue(counter)</span><br><span class="line"></span><br><span class="line">    sess.run(tf.initialize_all_variables())</span><br><span class="line">    coord = tf.train.Coordinator()</span><br><span class="line">    queue_runner = tf.train.QueueRunner(q, enqueue_ops=[add_op, enqueue_op] * <span class="number">2</span>)</span><br><span class="line">    enqueue_threads = queue_runner.create_threads(sess, coord=coord, start=<span class="literal">True</span>)  <span class="comment"># 启动入队线程</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        print(sess.run(q.dequeue()))</span><br><span class="line">    coord.request_stop()</span><br><span class="line">    coord.join(enqueue_threads)</span><br><span class="line">    print(<span class="string">'sess end'</span>)</span><br><span class="line">print(<span class="string">'program end'</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="CSV文件读写"><a href="#CSV文件读写" class="headerlink" title="CSV文件读写"></a>CSV文件读写</h2><p>&emsp; 逗号分隔值（Comma-Separated Values，CSV，有时也称为字符分隔值，因为分隔字符也可以不是逗号），其文件以纯文本形式存储表格数据（数字和文本）[1]。<br>在介绍tensorflow读写csv文件之前，先说下python读写csv文件</p>
<h3 id="Python读写CSV文件"><a href="#Python读写CSV文件" class="headerlink" title="Python读写CSV文件"></a>Python读写CSV文件</h3><h4 id="Python写CSV文件"><a href="#Python写CSV文件" class="headerlink" title="Python写CSV文件"></a>Python写CSV文件</h4><p>新建img文件夹，并放入若干张图片，如下图所示：<br>img<br>├── cat.0.jpg<br>├── cat.1.jpg<br>├── cat.2.jpg<br>├── cat.3.jpg<br>├── cat.4.jpg<br>├── cat.5.jpg<br>├── cat.6.jpg<br>├── cat.7.jpg<br>├── cat.8.jpg<br>└── cat.9.jpg<br>下面介绍python写csv文件。</p>
<p>代码示例:3：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">path = <span class="string">'img'</span></span><br><span class="line">filenames=os.listdir(path)</span><br><span class="line">strText = <span class="string">""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"train_list.csv"</span>, <span class="string">"w"</span>) <span class="keyword">as</span> fid:</span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> range(len(filenames)):</span><br><span class="line">        strText = path+os.sep+filenames[a]  + <span class="string">","</span> + filenames[a].split(<span class="string">'.'</span>)[<span class="number">1</span>]  + <span class="string">"\n"</span></span><br><span class="line">        fid.write(strText)</span><br><span class="line">fid.close()</span><br></pre></td></tr></table></figure><br>生成的csv文件内容如下：<br>img/cat.0.jpg,0<br>img/cat.8.jpg,8<br>img/cat.3.jpg,3<br>img/cat.2.jpg,2<br>img/cat.1.jpg,1<br>img/cat.5.jpg,5<br>img/cat.4.jpg,4<br>img/cat.7.jpg,7<br>img/cat.6.jpg,6<br>img/cat.9.jpg,9</p>
<h4 id="Python读取CSV文件"><a href="#Python读取CSV文件" class="headerlink" title="Python读取CSV文件"></a>Python读取CSV文件</h4><p>代码示例4：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">image_add_list = []</span><br><span class="line">image_label_list = []</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"train_list.csv"</span>) <span class="keyword">as</span> fid:</span><br><span class="line">    <span class="keyword">for</span> image <span class="keyword">in</span> fid.readlines():</span><br><span class="line">        image_add_list.append(image.strip().split(<span class="string">","</span>)[<span class="number">0</span>])</span><br><span class="line">        image_label_list.append(image.strip().split(<span class="string">","</span>)[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 上面这段代码就是csv文件的读取，</span></span><br><span class="line"><span class="comment"># 下面介绍一下如何将图片文件转换成tensorflow所需要的张量形式。</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_image</span><span class="params">(image_path)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.image.convert_image_dtype(</span><br><span class="line">        tf.image.decode_jpeg(</span><br><span class="line">            tf.read_file(image_path), channels=<span class="number">1</span>),</span><br><span class="line">        dtype=tf.uint8)</span><br><span class="line"><span class="comment"># tf.read_file, 读取图片文件</span></span><br><span class="line"><span class="comment"># tf.image.decode_jpeg, 将读取进来的图片文件解码成jpg格式</span></span><br><span class="line"><span class="comment">#                       channels=1表示读取灰度图</span></span><br><span class="line"><span class="comment"># tf.image.convert_image_dtype，将图像转化成TensorFlow需要的张量形式</span></span><br><span class="line"></span><br><span class="line">img = get_image(image_add_list[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    cv2Img = sess.run(img)</span><br><span class="line">    img2 = cv2.resize(cv2Img, (<span class="number">200</span>,<span class="number">200</span>))</span><br><span class="line">    cv2.imshow(<span class="string">'image'</span>, img2)</span><br><span class="line">    cv2.waitKey()</span><br></pre></td></tr></table></figure></p>
<h3 id="tensorflow读写CSV文件"><a href="#tensorflow读写CSV文件" class="headerlink" title="tensorflow读写CSV文件"></a>tensorflow读写CSV文件</h3><p>&emsp; 关于CSV文件的读写，文献[1]中介绍的是用Python写CSV，用Python读CSV；文献[2]中介绍的是用tensorflow读取CSV。所以，有用tensorflow写CSV吗？好吧，遇到时再说吧。</p>
<p>&emsp; 新建文件file0.csv、file1.csv，其内容分别如下：<br>file0.csv<br>21,31,41,44,0<br>22,32,42,44,0<br>23,33,53,44,0<br>24,34,44,44,0<br>25,35,45,44,0</p>
<p>file1.csv<br>11,31,41,50,1<br>12,42,42,55,1<br>13,23,53,55,1<br>14,34,44,45,1<br>15,35,45,55,1</p>
<h4 id="tensorflow读取CSV文件"><a href="#tensorflow读取CSV文件" class="headerlink" title="tensorflow读取CSV文件"></a>tensorflow读取CSV文件</h4><p>使用 tf.TextLineReader与 tf.decode_csv操作，主要代码讲解如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">filename_queue = tf.train.string_input_producer([<span class="string">"file0.csv"</span>, <span class="string">"file1.csv"</span>])</span><br><span class="line"><span class="comment"># 将文件名列表传递给tf.train.string_input_producer函数。string_input_producer创建一个用于保存文件名的FIFO队列。</span></span><br></pre></td></tr></table></figure><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">record_defaults &#x3D; [[1], [1], [1], [1], [1]]</span><br><span class="line">col1, col2, col3, col4, col5 &#x3D; tf.decode_csv(value, record_defaults&#x3D;record_defaults)</span><br><span class="line"># decode_csv操作将value解析成张量列表，record_defaults参数决定了所得张量的类型。</span><br><span class="line"># 注意，如果要读取的每个记录是固定数量字节的二进制文件（这个一般是TFRecords文件而不是csv文件了吧），请使用 tf.FixedLengthRecordReader 读取该文件，并使用 tf.decode_raw 解码文件内容。decode_raw 操作进行从字符串到UINT8张量转换。</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># Start populating the filename queue.</span></span><br><span class="line">    coord = tf.train.Coordinator() <span class="comment"># 创建线程协调器</span></span><br><span class="line">    threads = tf.train.start_queue_runners(coord=coord) <span class="comment"># 启动线程，用于往队列中输入数据</span></span><br><span class="line">    <span class="comment"># 对比“代码示例2”中的tf.train.QueueRunner和queue_runner.create_threads，这里用tf.train.start_queue_runners包含了这两步操作</span></span><br><span class="line">    <span class="comment"># 疑问：为什么这里的tf.train.start_queue_runners没有传入sess参数？</span></span><br><span class="line">    <span class="comment"># 这可能是因为tf.train.start_queue_runners是被包含在“with tf.Session() as sess”里的吧</span></span><br></pre></td></tr></table></figure>
<p>完整代码如下[2]，代码示例5：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">filename_queue = tf.train.string_input_producer([<span class="string">"file0.csv"</span>, <span class="string">"file1.csv"</span>])</span><br><span class="line"></span><br><span class="line">reader = tf.TextLineReader()</span><br><span class="line">key, value = reader.read(filename_queue) <span class="comment"># read操作每次从文件中读取一行</span></span><br><span class="line"><span class="comment"># key是文件名，value是该文件中某一行内容，这些可以在后面通过sess.run查看</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Default values, in case of empty columns. Also specifies the type of the</span></span><br><span class="line"><span class="comment"># decoded result.</span></span><br><span class="line">record_defaults = [[<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">1</span>]]</span><br><span class="line">col1, col2, col3, col4, col5 = tf.decode_csv(value, record_defaults=record_defaults)</span><br><span class="line">features = tf.stack([col1, col2, col3, col4])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># Start populating the filename queue.</span></span><br><span class="line">    coord = tf.train.Coordinator() <span class="comment"># 创建线程协调器</span></span><br><span class="line">    threads = tf.train.start_queue_runners(coord=coord) <span class="comment"># 启动线程，用于往队列中输入数据</span></span><br><span class="line">    <span class="comment"># 注意：如果不启动该线程，则不会有往队列输入数据的操作，则下面的sess.run(...)会一直被阻塞</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1200</span>):</span><br><span class="line">        <span class="comment"># Retrieve a single instance:</span></span><br><span class="line">        example, label = sess.run([features, col5])</span><br><span class="line">        print(example, label)</span><br><span class="line"></span><br><span class="line">    coord.request_stop()</span><br><span class="line">    coord.join(threads)</span><br></pre></td></tr></table></figure></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] 王晓华. TensorFlow深度学习应用实践<br>[2] <a href="http://cwiki.apachecn.org/pages/viewpage.action?pageId=10029497" target="_blank" rel="noopener">ApacheCN &gt;&gt; Tensorflow &gt;&gt; 编程指南 &gt;&gt; 阅读数据</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/10/20/TensorFlow%E7%AC%94%E8%AE%B0/%E3%80%90TensorFlow%E7%AC%94%E8%AE%B01.1%E3%80%91%E5%90%84%E7%A7%8D%E5%87%BD%E6%95%B0%E7%9A%84%E4%BD%BF%E7%94%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="其实，我是一个搬运工！">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Paper搬运菌">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/10/20/TensorFlow%E7%AC%94%E8%AE%B0/%E3%80%90TensorFlow%E7%AC%94%E8%AE%B01.1%E3%80%91%E5%90%84%E7%A7%8D%E5%87%BD%E6%95%B0%E7%9A%84%E4%BD%BF%E7%94%A8/" class="post-title-link" itemprop="url">TensorFlow笔记/【TensorFlow笔记1.1】各种函数的使用</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-10-20 00:00:00" itemprop="dateCreated datePublished" datetime="2017-10-20T00:00:00+08:00">2017-10-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-22 13:21:11" itemprop="dateModified" datetime="2020-01-22T13:21:11+08:00">2020-01-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow笔记</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <a id="more"></a>
<h3 id="numpy随机数"><a href="#numpy随机数" class="headerlink" title="numpy随机数"></a>numpy随机数</h3><p>参考资料[2]<br>np.random.randn(d0, d1, …, dn)，从标准正太分布中产生shape为(d0, d1, …, dn)的随机数组<br>np.random.rand(d0, d1, …, dn)，从区间为[0, 1)的均匀分布中产生shape为(d0, d1, …, dn)的随机数组</p>
<h3 id="8-2-tensorflow常量、变量、数据类型、常用函数"><a href="#8-2-tensorflow常量、变量、数据类型、常用函数" class="headerlink" title="8.2 tensorflow常量、变量、数据类型、常用函数"></a>8.2 tensorflow常量、变量、数据类型、常用函数</h3><p>tf.int8、tf.float32等tensorflow常用数据类型<br>tf.add、tf.mul等tensorflow常用函数</p>
<h3 id="8-3-tensorflow矩阵计算"><a href="#8-3-tensorflow矩阵计算" class="headerlink" title="8.3 tensorflow矩阵计算"></a>8.3 tensorflow矩阵计算</h3><p>tf.random_normal、tf.truncated_normal、tf.random_uniform等随机生成矩阵张量（[1]中170页也有提及）</p>
<p>tf.diag、tf.diag_part、tf.trace、tf.transpose、tf.matmul、tf.matrix_determinant、tf.matrix_inverse、tf.cholesky、tf.matrix_solve</p>
<h3 id="11-2-2-数据的矩阵化"><a href="#11-2-2-数据的矩阵化" class="headerlink" title="11.2.2 数据的矩阵化"></a>11.2.2 数据的矩阵化</h3><p>tf.mul(matrix1, matrix2) #点乘，要求shape相同，对应相乘，现改用tf.multiply代替。<br>tf.matmul(matrix1, matrix2) # 叉乘，要求matrix1的列数等于matrix2的行数。类似numpy.dot(matrix1, matrix2)</p>
<h3 id="tf-argmax"><a href="#tf-argmax" class="headerlink" title="tf.argmax()"></a>tf.argmax()</h3><p>首先，明确一点，tf.argmax可以认为就是np.argmax。tensorflow使用numpy实现的这个API。<br>tf.argmax(input, axis=None, name=None, dimension=None, output_type=dtypes.int64):<br><strong>函数功能：</strong>简单的说，就是返回最大值所在的下标[3]；<br><strong>参数解析：</strong><br>    axis：用于多维度计算<br>举例：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test &#x3D; np.array([[1, 2, 3], [2, 3, 4], [5, 4, 3], [8, 7, 2]])</span><br><span class="line">np.argmax(test, 0)　　　＃输出：array([3, 3, 1]</span><br><span class="line">np.argmax(test, 1)　　　＃输出：array([2, 2, 0, 0]</span><br></pre></td></tr></table></figure><br>axis=0 : 按列比较<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">test[0] &#x3D; array([1, 2, 3])</span><br><span class="line">test[1] &#x3D; array([2, 3, 4])</span><br><span class="line">test[2] &#x3D; array([5, 4, 3])</span><br><span class="line">test[3] &#x3D; array([8, 7, 2])</span><br><span class="line"># output   :    [3, 3, 1]</span><br></pre></td></tr></table></figure><br>axis=1 : 按行比较<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">test[0] &#x3D; array([1, 2, 3])  #2</span><br><span class="line">test[1] &#x3D; array([2, 3, 4])  #2</span><br><span class="line">test[2] &#x3D; array([5, 4, 3])  #0</span><br><span class="line">test[3] &#x3D; array([8, 7, 2])  #0</span><br></pre></td></tr></table></figure><br>这是里面都是数组长度一致的情况，如果不一致，axis最大值为最小的数组长度-1，超过则报错。<br>当不一致的时候，axis=0的比较也就变成了每个数组的和的比较。</p>
<h3 id="tf-reduce-max和tf-reduce-mean"><a href="#tf-reduce-max和tf-reduce-mean" class="headerlink" title="tf.reduce_max和tf.reduce_mean"></a>tf.reduce_max和tf.reduce_mean</h3><p>求最大值tf.reduce_max(input_tensor, reduction_indices=None, keep_dims=False, name=None)<br>求平均值tf.reduce_mean(input_tensor, reduction_indices=None, keep_dims=False, name=None)<br><strong>参数解析：</strong><br>    input_tensor : 待求值的tensor；<br>    reduction_indices : 在哪一维上求解；<br>    keep_dims : 表示是否保留原始数据的维度，False相当于执行完后原始数据就会少一个维度。<br>例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># &#39;x&#39; is [[1., 2.]</span><br><span class="line">#         [3., 4.]]</span><br></pre></td></tr></table></figure><br>以reduce_mean为例[4]：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.reduce_mean(x) &#x3D;&#x3D;&gt; 2.5 #如果不指定第二个参数，那么就在所有的元素中取平均值</span><br><span class="line">tf.reduce_mean(x, 0) &#x3D;&#x3D;&gt; [2.,  3.] #指定第二个参数为0，则第一维的元素取平均值，即每一列求平均值</span><br><span class="line">tf.reduce_mean(x, 1) &#x3D;&#x3D;&gt; [1.5,  3.5] #指定第二个参数为1，则第二维的元素取平均值，即每一行求平均值</span><br><span class="line">tf.reduce_mean(x, 1, keep_dims&#x3D;True) &#x3D;&#x3D;&gt; [[1.5],  [3.5]]</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>tf.reduce_mean中的keep_dims有什么作用？<br>设有矩阵A和向量b如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A &#x3D; [[1., 2.],     b &#x3D; [1., 1.]</span><br><span class="line">     [3., 4.]]</span><br></pre></td></tr></table></figure><br>那么A-b是非法的，但若：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b &#x3D; [[1.], [1.]]，也即b &#x3D; [[1.],</span><br><span class="line">                          [1.]]</span><br></pre></td></tr></table></figure><br>那么此时A-b就是合法的了，此时：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A-b&#x3D;[[0., 1.],</span><br><span class="line">     [2., 3.]]</span><br></pre></td></tr></table></figure></p>
</blockquote>
<p>上述内容是以tf.reduce_mean()为例，同理，还可用tf.reduce_max()求最大值等。</p>
<h3 id="tf-equal"><a href="#tf-equal" class="headerlink" title="tf.equal"></a>tf.equal</h3><p>tf.equal(x, y, name=None)<br>例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"> </span><br><span class="line">A &#x3D; [[1,3,4,5,6]]</span><br><span class="line">B &#x3D; [[1,3,4,3,2]]</span><br><span class="line"> </span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    print(sess.run(tf.equal(A, B)))</span><br></pre></td></tr></table></figure><br>输出：<br>[[ True  True  True False False]]</p>
<h3 id="tf-cast"><a href="#tf-cast" class="headerlink" title="tf.cast"></a>tf.cast</h3><p>tf.cast(x, dtype, name=None)<br><strong>函数功能：</strong>张量类型转换函数，将张量x的类型转换为dtype<br>例1：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># tensor &#96;a&#96; is [1.8, 2.2], dtype&#x3D;tf.float</span><br><span class="line">tf.cast(a, tf.int32) &#x3D;&#x3D;&gt; [1, 2]  # dtype&#x3D;tf.int32</span><br></pre></td></tr></table></figure><br>例2：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; tf.Variable([1,0,0,1,1])</span><br><span class="line">b &#x3D; tf.cast(a,dtype&#x3D;tf.bool)</span><br><span class="line">sess &#x3D; tf.Session()</span><br><span class="line">sess.run(tf.initialize_all_variables())</span><br><span class="line">print(sess.run(b))</span><br><span class="line">#[ True False False  True  True]</span><br></pre></td></tr></table></figure></p>
<h3 id="tf-tile"><a href="#tf-tile" class="headerlink" title="tf.tile"></a>tf.tile</h3><p>tf.tile(input, multiples, name=None)<br><strong>函数功能：</strong><br>tile有平铺之意，用于在同一维度上的复制，用来对张量(Tensor)进行扩展，最终的输出张量的维度不变，但张量的shape会发生改变。<br><strong>参数解析：</strong><br>input, 输入的待扩展的张量；<br>multiples, 扩展方法，假如input是一个2维的张量。那么mutiples就必须是一个1x2的1维张量，这个张量的两个值依次表示input的第1、第2维数据扩展几倍。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># &#39;x&#39; is [[1., 2.]</span><br><span class="line">#         [3., 4.]]</span><br><span class="line"># tile &#x3D; tf.tile(arr, [2])  # failed, multiples列表中的元素个数必须要和arr的维度相等</span><br><span class="line">tile &#x3D; tf.tile(arr, [2, 3])  &#x3D;&#x3D;&gt;  [[1. 2. 1. 2. 1. 2.]</span><br><span class="line">                                   [3. 4. 3. 4. 3. 4.]</span><br><span class="line">                                   [1. 2. 1. 2. 1. 2.]</span><br><span class="line">                                   [3. 4. 3. 4. 3. 4.]]</span><br></pre></td></tr></table></figure></p>
<p>[1] 王晓华. TensorFlow深度学习应用实践<br>[2] numpy随机数介绍<a href="https://blog.csdn.net/u013920434/article/details/52507173" target="_blank" rel="noopener">https://blog.csdn.net/u013920434/article/details/52507173</a><br>[3] <a href="https://blog.csdn.net/qq575379110/article/details/70538051/" target="_blank" rel="noopener">tf.argmax()以及axis解析</a><br>[4] <a href="https://blog.csdn.net/qq_32166627/article/details/52734387" target="_blank" rel="noopener">tf.reduce_mean和tf.reduce_max</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/5/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/7/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name"></p>
  <div class="site-description" itemprop="description">其实，我是一个搬运工！</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">91</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder"></span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.7.0
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      
<script type="text/x-mathjax-config">
    MathJax.Ajax.config.path['mhchem'] = '//cdn.jsdelivr.net/npm/mathjax-mhchem@3';

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        extensions: ['[mhchem]/mhchem.js'],
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

</body>
</html>
